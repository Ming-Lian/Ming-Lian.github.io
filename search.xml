<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[免疫组库测序：从入门到进阶]]></title>
    <url>%2F2019%2F04%2F28%2FLearning-ImmuSeq%2F</url>
    <content type="text"><![CDATA[背景介绍TCR与BCR的结构： TCR与APC的互作： (a)(b) TCR与APC的互作; (c)T细胞中的VDJ基因重排 CDR3区域： 互补决定区域（complementarity determining region 3 (CDR3) domain），一般长度为45nt，有VJ junction形式（TCR-α）和VDJ junction形式（TCR-β） 材料选择：用αβ还是γδ？ TCR有 αβ（外周血中90%~95%）和 γδ（外周血中5%~10%）二聚体形式，一般对TCR的研究都是针对占主体的αβ二聚体形式，且α的CDR3区域是VJ junction，β的CDR3区域是VDJ junction，所以β链与α链相比具有更多的组合形式和连接的多样性，因此TCR-seq一般都是针对β链的CDR3区域 材料选择：用DNA还是RNA？ DNA 优点：丰富，容易提取且能保持长时间的稳定，且对于每一个TCR subunit，一个细胞只有两个位置有，或者说只有固定的两份拷贝，因此DNA模板分子的数量能反映T细胞的数量 缺点：必须进行PCR扩增来达到足够的测序量，而为了得到进尽可能全的TCR库的组成，使用了多套PCR引物多重PCR方法，使得很容易在PCR过程中引入PCR bias RNA 使用5’ RACE方法进行cDNA的扩增，因此只需要使用一套PCR引物即可 优点：只使用一套PCR引物，极大地降低了PCR bias 缺点：TCR表达水平的变异很大，不能准确地反映T细胞的数量 详尽彻底的测序不是T细胞库分析的目标（也不现实，因为目前免疫组库基本都是从外周血取样，而要想实现彻底的测序，即意味着将这个个体外周血中的免疫细胞克隆型几乎全部取样到，除非将这个人的血几乎抽干，才可能实现），对于旨在阐明样本间差异的比较研究，极端深的TCR-seq也是没有必要的。对应TCR-seq来说，人们主要关心的是：当取样不完整，测序深度差异比较大时，怎么鉴定一个给定的样本它的TCR组成与其他的样本不同？ 可以计算一个置信度（confidence)，来表示一个克隆在给定样本中差异于另一个样本是偶然产生的概率，概率越低则说明越不可能是由于偶然因素导致的，也就是说是真实的差异的可能性比较大 为了能够将两个或多个样本进行比较，需要先将它们的input data进行标准化处理： 以多个样本中的数据量最少的那个样本为基准，对其他样本的reads进行无偏好的随机抽样，将它们的input data砍到同一水平 —— 这是在生态学研究中常用的方法 这是目前免疫组库测序领域常用的标准化方法，但是该标准化方法是否合理？是否还有其他可选的方法？ 比较样本间差异或相似度的几个指标： Simpson diversity index：样本间的多样性的比较 Morisita-Horn similarity index：样本间相似度的比较 一些描述样本免疫组库的指标 个体免疫多样性 (immunological diversity) 采用信息论中的香农指数 $$H=-\sum_i^{|S|}p(c_i)\log p(c_i)$$ 其中，$S$表示该样本total unique clone的集合$S={c_1,c_2,…,c_m}$ 个体免疫组库采样的饱和度 采用了生态学中常用的 Chao1 指数，它常被用作种群丰富度的一个描述指标 想象一下这样一个场景： 在一个放了各种各样玩具模型的水池中（水池很大，其中玩具有相同的，有不同的，且各种类型及数目不限），随机来捞玩具。这时捞起来一个，发现之前有个玩具和这个捞起的玩具一模一样，这时有两个这种玩具在手上，这个玩具模型就是doubletons；当然也可能捞起一个玩具发现手里没有相同的，那这个就叫singletons 那么经典的chao1指数的计算公式是这样的： $$S_{chao1}=S_{obs}+\frac{F_1^2}{2F_2}$$ $S_{obs}$表示样本中观察到的物种数目。$F_1$和$F_2$分别表示singletons和doubletons的数目 由经典公式可以看到，当doubletons为0（即$F_2$为0）时计算的结果没有意义，因此又提出了另外一种修正偏差的公式 $$\hat S_{chao1}=S_{obs}+\frac{F_1(F_1-1)}{2(F_2+1)}$$ 可以这样理解这个修正公式（虽然不太严格）：它从singletons中拿出1条来（严格来说与经典公式相比还不到1条），当作doubletons，这样分母一定会大于0 理解chao1指数的含义： chao1指数是用来反映物种丰富度的指标 它通过观测到的结果推算出一个理论的丰富度，这个丰富度更接近真实的丰富度——一般来讲能观测到的物种丰富度肯定会比实际少，那么两者之间的差距有多大呢？ chao1指数给出的答案是 $(F_1^2)/(2F_2)$，它通过singletons和doubletons进行了合理的推算，那么差距为 $(F_1^2)/(2F_2)$ 的合理性在哪里？ 分析 $(F_1^2)/(2F_2)$ 我们不难发现它对singletons的权重要高于doubletons (即 $F_1^2$ 比 $2F_2$ 变化的速度更快)，这和我们的一个直观理解是相符的： 在一个群体中随机抽样，当稀有的物种 (singletons) 依然不断的被发现时，则表明还有一些稀有的物种没有被发现；直到所有物种至少被抽到两次 (doubletons) 时，则表明不会再有新的物种被发现 可以通过比较chao1指数和实际检测到的unique克隆数进行比较，来评估当前样本的测序饱和度 PCR与测序错误的校正测序错误的影响及处理方法： TCR-seq对测序错误十分敏感，因为只要有一个碱基不同，一条TCR β链就能区别于其他的克隆，一个碱基的测序错误可能在后续的分析中会被错误地鉴定出一个低丰度的新克隆，因此 （1） 在进入后续分析之前需要执行严格的质控，但是 （2） 对于深度的TCR-seq则没有这个必要，因为错误的TCR序列总是表现出低丰度的特征，因此通过一个丰度的阈值筛选就可以比较轻松且准确地将这些错误的TCR克隆过滤掉；还有另外一种解决方法 （3） 假设每一种低丰度的克隆都是由测序错误产生的，将它们分别与高丰度的克隆依据序列相似性进行聚类，将高丰度的克隆的序列作为它的正确的序列 Wei Zhang等提出了一种进行错误校正的方法 可分为三步进行，前两步进行测序错误的校正，最后一步进行PCR错误校正： （1）将reads根据测序质量分成三组： 高质量序列：每个碱基的质量都大于Q20； 丢弃序列：超过5个碱基的质量低于Q20，将这样的reads直接丢弃； 低质量序列：减去前两组，剩下的那些序列； （2）将低质量的序列比对到高质量的序列上，若某条低质量序列能比对到这样一条高质量序列：mismatch数不超过5个碱基，且都落在低质量位点上，则依据高质量序列对mismatch位点进行修正，否则丢弃这条低质量序列； （3）最后，为了消除PCR过程中引入的错误，将低丰度的reads比对高丰度reads，对于某一个低丰度reads，若能找到一条高丰度reads使得它们之间的mismatch低于3个碱基，则将它合并到对应高丰度reads中； 缩小多重PCR引入的PCR bias一般PCR仅应用一对引物，通过PCR扩增产生一个核酸片段，而多重PCR (multiplex PCR)，又称多重引物PCR或复合PCR，它是在同一PCR反应体系里加上二对以上引物，同时扩增出多个核酸片段的PCR反应，其反应原理，反应试剂和操作过程与一般PCR相同· 在免疫组库建库的过程中一般都采用针对V和J基因的多套引物进行PCR扩增，即使用的是多重PCR方法，与普通PCT相比，多重PCR明显会带来更大程度的PCR bias，所以为了保证下游分析的可靠性，进行PCR bias的修正是非常有必要的 Wei Zhang等提出了一种进行PCR bias修正的方法： 该方法基于这样一个前提假设：multiplex PCR过程中，克隆的扩增效率仅受到以下两个因素的影响——模板的浓度和多重引物的效率 健康个体的免疫组库 TCR的多样性/克隆种类 TCR β的CDR3序列，长度为45bp，其最大可能的容量为445，考虑一些已知的限制因素，它理论上的多样性也能达到1011，然而实际上T细胞在胸腺成熟的过程中要经历两个选择过程： 阳性选择：留下那些能与自身MHC结合的T细胞克隆 阴性选择：消除那些与自身MHC结合能力过强的T细胞克隆 只有经过这两步筛选，才能产生成熟的且具有功能的T细胞，并进入外周血然后分散到各个组织器官中，因此实际上产生的成熟的T细胞克隆的种类要远远少于理论值 在1999年的Science文章中，有人基于Vβ18 和 Jβ1.4 subset抽样推断，认为TCR β的克隆种类大概为106 2009年，基于深度的TCR-seq和unseen species model，推断TCR β的克隆种类大概为3-4 million，目前对个体进行全面深度的TCR-seq的研究得出结论，一个健康个体大概有1.3 million的distinct TCR β chain sequences 个体内不同的TCR克隆，其丰度有数量级上的差异 样本间共享的TCR克隆 Public T cells：个体间共有的相同的T细胞克隆型，由于不同个体偶然产生相同的TCR的可能性极低，一段时间以来，它们一直是一种稀奇的事物。 若按照随机事件来看，两个个体之间出现相同克隆是一个小概率事件，但是实际检测出来的共享克隆的发生概率比随机期望高了上千倍 TCR-seq研究表明，公共T细胞实际上是常见的，这是由于这些跨个体共享的TCR特异性的产生概率增加，以及由于遗传密码的简并，不同的TCR核苷酸序列可以编码相同的TCR氨基酸序列。一个人共有的TCR库所占比例已被证明高达14%，而共有TCR库的真实程度可能还要高得多 简单来说，就是任意两个健康个体，它们之间共享的TCR克隆种类大约占到各自总克隆种类的14%，两个个体间的共享克隆常见但不多，但是两个以上个体间的共享克隆则少之又少，甚至根本没有 因此想要通过比较两组样本间某个克隆的丰度是否存在显著差异基本是不可能的：若control组和case组各有3个样本，要比较的克隆只在其中的一个或两个样本中有检测到（绝大多数克隆都是这种情况），此时根本无法进行比较！ 有一种解释是，其实不同个体之间的共享克隆很高，只是你检测不到而已 其实在每个个体的 naive T 库中都随时在产生着丰富多样的T细胞克隆类型（T细胞克隆类型由TCR决定），因为其足够丰富，丰富到 naive T 库的容量几乎达到甚至已经超过它可能产生的克隆类型的总量，那么此时大部分的TCRβ在不同个体间，不论什么时刻什么生理状态下，都是共享的，但是此时每种T细胞克隆几乎都是微量的，或者说是单克隆。 当个体被暴露在某种特定的抗原环境下，针对这种特定抗原的TCR识别MHC-抗原复合物（antigen–MHC complex），使得带有这种TCR的T细胞克隆增殖发生克隆扩张（clonal expansion），那么它在整个T细胞库中的比例就会显著增加，而从外周血取样也只是对T细胞库进行抽样测序，比例高的T细胞克隆类型相对于其他克隆类型当然更容易被检测到。 因此，若两个个体同时都暴露在一种抗原环境下，针对这种抗原的T细胞克隆有很大可能性会在这两个个体中被检测到，而被鉴定为共享克隆，而如果两个个体没有接触或没有同时接触到这种抗原，则从他们中都检测到对应抗体克隆类型的可能性就偏低，从而有很大可能性被鉴定为非共享克隆，但实际上这种克隆类型有很大可能性在两者体内都有 对低丰度的T细胞克隆具有极高的灵敏度在相同的T细胞克隆的混合背景（1 million) 中添加不同量的已知的T细胞克隆作为spike-in 其中D克隆在Mix3和G克隆在Mix1中的量最少，都只有10个，但都在后续的分析中成功检测到，说明：免疫组库测序对低丰度的T细胞克隆具有极高的灵敏 实际检测到频率与期望的频率基本都十分相近 参考资料： (1) Zhang W , Du Y , Su Z , et al. IMonitor: A Robust Pipeline for TCR and BCR Repertoire Analysis[J]. Genetics, 2015, 201. (2) 卢锐《Alpha多样性指数之Chao1指数 》 (3) Chao, A. 1984. Non-parametric estimation of the number of classes in a population. Scandinavian Journal of Statistics 11, 265-270. (4) Harlan Robins, Cindy Desmarais, Jessica Matthis, et al. Ultra-sensitive detection of rare T cell clones[J]. Journal of Immunological Methods, 2012, 375(1-2):14-19. (5) Woodsworth DJ, Castellarin M, Holt RA. Sequence analysis of T-cell repertoires in health and disease. Genome Med. 2013;5(10):98. Published 2013 Oct 30. doi:10.1186/gm502 (6) Robins, H.S. et al. Overlap and effective size of the human CD8 + T cell receptor repertoire. Sci. Transl. Med. 2, 47ra64 (2010). (7) Emerson R O , Dewitt W S , Vignali M , et al. Immunosequencing identifies signatures of cytomegalovirus exposure history and HLA-mediated effects on the T cell repertoire[J]. Nature Genetics, 2017, 49(5):659-665.]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>Bioinformatics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Biopython学习笔记]]></title>
    <url>%2F2019%2F04%2F22%2FBiopython-Note%2F</url>
    <content type="text"><![CDATA[1. 序列读入与输出序列的读入与输出通过Bio.SeqIO模块实现，它旨在提供一个简单的接口，实现对各种不同格式序列文件进行统一的处理 主要是基于对SeqRecord对象的操作，该对象包含一个 Seq 对象）和注释信息（如序列ID和描述信息） SeqRecord对象本质上就是一个字典： Seq键：保存序列组成 ID键：保存序列ID description键：保存序列的描述信息 …… 1.1. 解析/读取序列 （1）基本用法 1SeqIO.parse( [FILE|HANDLE] , filetype) 第一个参数是一个文件名或者一个句柄（ handle ，推荐） 第二个参数是一个小写字母字符串，用于指定序列格式（我们并不推测文件格式！），支持的文件格式 Bio.SeqIO.parse() 函数返回一个 SeqRecord 对象迭代器（ iterator ），迭代器通常用在循环中 12345from Bio import SeqIOfor seq_record in SeqIO.parse("ls_orchid.gbk", "genbank"): print seq_record.id print seq_record.seq print len(seq_record) （2）使用with和句柄的方法操作 由于我们建议养成一个良好的编程习惯——在文件句柄用完后要及时把它关闭，所以推荐使用with和句柄的方法操作Bio.SeqIO.parse() 函数，如下： 1234from Bio import SeqIOwith open("ls_orchid.gbk") as handle: for r in SeqIO.parse(handle, "gb"): ... 有时你需要处理只包含一个序列条目的文件，此时请使用函数 Bio.SeqIO.read() 。它使用与函数 Bio.SeqIO.parse() 相同的参数，因为Bio.SeqIO.parse() 函数返回一个 SeqRecord 对象迭代器，所以也可以使用Bio.SeqIO.parse().next() （3）保存成列表形式——方便后续操作 如果想要把整个文件中的所有序列的SeqRecord 对象保持则一个列表，则可以这样： 1records = [seq_record for seq_record in SeqIO.parse(&quot;ls_orchid.gbk&quot;, &quot;genbank&quot;)] 将序列保持成一个列表，使我们能方便地从里面挑取里面的任意一条序列的信息 （4）读取压缩文件里的序列 可以使用Python的 gzip 模块打开压缩文档以读取数据 12345import gzipfrom Bio import SeqIOwith gzip.open("ls_orchid.gbk.gz", "r") as handle: for record in SeqIO.parse(handle,'gb'): ... 相同地，如果我们有一个bzip2压缩文件，可以使用bz2模块 12import bz2handle = bz2.BZ2File(&quot;ls_orchid.gbk.bz2&quot;, &quot;r&quot;) 1.2. 转换成字典形式之所以要将序列文件的解析结果转换为字典形式，是为了后续能随意地读取任意一条序列的信息 Bio.SeqIO 模块中3个相关函数，用于随机读取多序列文件 Bio.SeqIO.to_dict()：最灵活但内存占用最大，因为它将整个字典都载人到内存中； Bio.SeqIO.index()：工作原理上与Bio.SeqIO.to_dict()略有不同——它是对这个文件进行索引。尽管仍然是返回一个类似于字典的对象，它并不将所有的信息存储在内存中。相反，它仅仅记录每条序列条目在文件中的位置——当你需要读取某条特定序列条目时，它才进行解析； Bio.SeqIO.to_dict() 1orchid_dict = SeqIO.to_dict(SeqIO.parse(&quot;ls_orchid.gbk&quot;, &quot;genbank&quot;)) 使用 Bio.SeqIO.to_dict() 函数将明确检查重复键，如果发现任何重复键将引发异常并退出 默认会使用每条序列条目的ID（i.e. .id 属性）作为键 如果你需要别的作为键，如登录号（Accession Number），可使用 SeqIO.to_dict() 的可选参数 key_function ，它允许你根据你的序列条目特点，自定义字典键，例如： 123456789101112# 定义key_functiondef get_accession(record): """"Given a SeqRecord, return the accession number as a string. e.g. "gi|2765613|emb|Z78488.1|PTZ78488" -&gt; "Z78488.1" """ parts = record.id.split("|") assert len(parts) == 5 and parts[0] == "gi" and parts[2] == "emb" return parts[3]# 将SeqRecord对象转换为字典，并用前面定义的key_function定义字典的键orchid_dict = SeqIO.to_dict(SeqIO.parse("ls_orchid.fasta", "fasta"), key_function=get_accession) 也可以利用python的lambda表达式定义一个简易的一次性的函数来充当key_function： 1seguid_dict = SeqIO.to_dict(SeqIO.parse(&quot;ls_orchid.gbk&quot;, &quot;genbank&quot;), lambda rec : seguid(rec.seq)) Bio.SeqIO.index() 1orchid_dict = SeqIO.index(&quot;ls_orchid.gbk&quot;, &quot;genbank&quot;) 注意： Bio.SeqIO.index() 不接受句柄参数，仅仅接受文件名 它默认也会使用每条序列条目的ID（i.e. .id 属性）作为键，如果想要自定义，也需要定义一个key_function 1.3. 写入序列文件Bio.SeqIO.write() 用于输出序列（写入文件） 1Bio.SeqIO.write(SeqRecord, [FILE | HANDLE], file-format) 可以在写出时实现文件格式的转换： 在之前的例子中我们使用 SeqRecord 对象列表作为 Bio.SeqIO.write() 函数的输入，但是它也接受如来自于 Bio.SeqIO.parse() 的 SeqRecord 迭代器 - 这允许我们通过结合使用这两个函数实现文件转换。 1234from Bio import SeqIOrecords = SeqIO.parse("ls_orchid.gbk", "genbank")count = SeqIO.write(records, "my_example.fasta", "fasta")print "Converted %i records" % count 这仍然有点复杂，有一个辅助函数可以替代上述代码： 1count = SeqIO.convert(&quot;ls_orchid.gbk&quot;, &quot;genbank&quot;, &quot;my_example.fasta&quot;, &quot;fasta&quot;) 2. 序列比对2.1. 读取多序列比对数据在Biopython中，有两种方法读取多序列比对数据： Bio.AlignIO.read() 只能读取一个多序列比对； Bio.AlignIO.parse() 可以依次读取多个序列比对数据。 使用 Bio.AlignIO.parse() 将会返回一个 MultipleSeqAlignment 的 迭代器（iterator） Stockholm格式的一个多序列比对文件： 123456789101112131415161718192021222324252627# STOCKHOLM 1.0#=GS COATB_BPIKE/30-81 AC P03620.1#=GS COATB_BPIKE/30-81 DR PDB; 1ifl ; 1-52;#=GS Q9T0Q8_BPIKE/1-52 AC Q9T0Q8.1#=GS COATB_BPI22/32-83 AC P15416.1#=GS COATB_BPM13/24-72 AC P69541.1#=GS COATB_BPM13/24-72 DR PDB; 2cpb ; 1-49;#=GS COATB_BPM13/24-72 DR PDB; 2cps ; 1-49;#=GS COATB_BPZJ2/1-49 AC P03618.1#=GS Q9T0Q9_BPFD/1-49 AC Q9T0Q9.1#=GS Q9T0Q9_BPFD/1-49 DR PDB; 1nh4 A; 1-49;#=GS COATB_BPIF1/22-73 AC P03619.2#=GS COATB_BPIF1/22-73 DR PDB; 1ifk ; 1-50;COATB_BPIKE/30-81 AEPNAATNYATEAMDSLKTQAIDLISQTWPVVTTVVVAGLVIRLFKKFSSKA#=GR COATB_BPIKE/30-81 SS -HHHHHHHHHHHHHH--HHHHHHHH--HHHHHHHHHHHHHHHHHHHHH----Q9T0Q8_BPIKE/1-52 AEPNAATNYATEAMDSLKTQAIDLISQTWPVVTTVVVAGLVIKLFKKFVSRACOATB_BPI22/32-83 DGTSTATSYATEAMNSLKTQATDLIDQTWPVVTSVAVAGLAIRLFKKFSSKACOATB_BPM13/24-72 AEGDDP...AKAAFNSLQASATEYIGYAWAMVVVIVGATIGIKLFKKFTSKA#=GR COATB_BPM13/24-72 SS ---S-T...CHCHHHHCCCCTCCCTTCHHHHHHHHHHHHHHHHHHHHCTT--COATB_BPZJ2/1-49 AEGDDP...AKAAFDSLQASATEYIGYAWAMVVVIVGATIGIKLFKKFASKAQ9T0Q9_BPFD/1-49 AEGDDP...AKAAFDSLQASATEYIGYAWAMVVVIVGATIGIKLFKKFTSKA#=GR Q9T0Q9_BPFD/1-49 SS ------...-HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH--COATB_BPIF1/22-73 FAADDATSQAKAAFDSLTAQATEMSGYAWALVVLVVGATVGIKLFKKFVSRA#=GR COATB_BPIF1/22-73 SS XX-HHHH--HHHHHH--HHHHHHH--HHHHHHHHHHHHHHHHHHHHHHH---#=GC SS_cons XHHHHHHHHHHHHHHHCHHHHHHHHCHHHHHHHHHHHHHHHHHHHHHHHC--#=GC seq_cons AEssss...AptAhDSLpspAT-hIu.sWshVsslVsAsluIKLFKKFsSKA// phylip格式： 12345678910111213141516171819202122232425 5 6Alpha AAACCABeta AAACCCGamma ACCCCADelta CCCAACEpsilon CCCAAA 5 6Alpha AAACAABeta AAACCCGamma ACCCAADelta CCCACCEpsilon CCCAAA 5 6Alpha AAAAACBeta AAACCCGamma AACAACDelta CCCCCAEpsilon CCCAAC... 5 6Alpha AAAACCBeta ACCCCCGamma AAAACCDelta CCCCAAEpsilon CAAACC 这些格式的比对文件都可以非常明确地储存多个序列比对 然而，例如FASTA一类的普通序列文件格式并没有很直接的分隔符来分开多个序列比对： 123456789101112&gt;AlphaACTACGACTAGCTCAG--G&gt;BetaACTACCGCTAGCTCAGAAG&gt;GammaACTACGGCTAGCACAGAAG&gt;AlphaACTACGACTAGCTCAGG--&gt;BetaACTACCGCTAGCTCAGAAG&gt;GammaACTACGGCTAGCACAGAAG 以上FASTA格式文件可以认为是一个包含有6条序列的序列比对（有重复序列名）。或者从文件名来看，这很可能是两个序列比对，每一个包含有三个序列，只是这两个序列比对恰好具有相同的长度 很明显，将多个序列比对以FASTA格式储存并不方便 为了处理这样的FASTA格式的数据，我们可以指定 Bio.AlignIO.parse() 的第三个可选参数 seq_count ，这一参数将告诉Biopython你所期望的每个序列比对中序列的个数。例如： 12345for alignment in AlignIO.parse(handle, "fasta", seq_count=2): print "Alignment length %i" % alignment.get_alignment_length() for record in alignment: print "%s - %s" % (record.seq, record.id) print 2.2. 序列比对的写出使用 Bio.AlignIO.write() 写出序列比对文件 1Bio.AlignIO.write(MultipleSeqAlignment, [FILE | HANDLE], file-format) Bio.AlignIO 模块中的序列比对格式转换功能与 Bio.SeqIO 模块的格式转换是一样的 1count = AlignIO.convert("PF05371_seed.sth", "stockholm", "PF05371_seed.aln", "clustal") 当你写出的文件格式是PHYLIP时，需要注意： PHYLIP格式它严格地要求每一条序列的ID是都为10个字符（ID中多出的字符将被截短），这会带来序列ID的改变，甚至使得原先ID不同的两条序列，被截短后ID变成一样的了 为了解决这个问题，我们提供了另一种解决方案 —— 利用自定义的序列ID来代替原本的序列ID（建立一个字典对象实现自定义的ID和原始ID的映射）： 12345678910from Bio import AlignIOalignment = AlignIO.read("PF05371_seed.sth", "stockholm")name_mapping = &#123;&#125;for i, record in enumerate(alignment): # 建立一个字典对象实现自定义的ID和原始ID的映射 name_mapping[i] = record.id record.id = "seq%i" % iprint name_mappingAlignIO.write([alignment], "PF05371_seed.phy", "phylip") 2.3. 序列比对的操纵使用切片操作来获得其中某些序列比对 1alignment[3:7] # 获取一个多序列比对的第4到第8条序列 获得特定的列，使用双切片： 123456789&gt;&gt;&gt; print alignment[2,6]T# 其实是以下操作的简化&gt;&gt;&gt; print alignment[2].seq[6]T# 获取整列&gt;&gt;&gt; print alignment[:,6]TTT---T 2.4. 构建序列比对的工具使用Python来执行序列比对任务的实现思路： 创造一个命令行对象来指定各种参数（例如：输入文件名，输出文件名等），然后通过Python的系统命令模块来运行这一程序 大多数的打包程序都在 Bio.Align.Applications 中定义： 12345&gt;&gt;&gt; import Bio.Align.Applications&gt;&gt;&gt; dir(Bio.Align.Applications)...['ClustalwCommandline', 'DialignCommandline', 'MafftCommandline', 'MuscleCommandline','PrankCommandline', 'ProbconsCommandline', 'TCoffeeCommandline' ...] 注意：在python调用这些序列比对工具之前，需要保证这些工具已经安装好，且被添加到PATH下，或者也可以提供这些工具的绝对路径，下面以ClustalW为例进行说明： 12345678import os# Python中 '\n' 和 '\t' 会被解析为一个新行和制表空白（tab），# 如果你将一个小写的“r”放在字符串的前面，这一字符串就将保留原始状态，而不被解析# 这种方式对于指定Windows风格的文件名来说是一种良好的习惯clustalw_exe = r"C:\Program Files\new clustal\clustalw2.exe"# 检测指定的路径是否存在assert os.path.isfile(clustalw_exe), "Clustal W executable missing"clustalw_cline = ClustalwCommandline(clustalw_exe, infile="opuntia.fasta") ClustalW 1234&gt;&gt;&gt; from Bio.Align.Applications import ClustalwCommandline&gt;&gt;&gt; cline = ClustalwCommandline(&quot;clustalw2&quot;, infile=&quot;opuntia.fasta&quot;)&gt;&gt;&gt; print clineclustalw2 -infile=opuntia.fasta 参考资料： (1) BioPython中文官方文档]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>Bioinformatics</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[西瓜书带学训练营·实战任务]]></title>
    <url>%2F2019%2F04%2F15%2FInAction-ML-XGS%2F</url>
    <content type="text"><![CDATA[1. 线性回归1.1. 目标函数与损失函数$$\hat y = \theta_0+\theta_1x_1+…+\theta_nx_n$$ 写成向量化的形式为： $$h_{\theta}=\theta^TX$$ 优化目标为最小化均方差 (MSE)： $$\min_{\theta} MSE(\theta)=\frac 12 \sum_{i=1}^m (\theta^TX^{(i)}-y^{(i)})^2$$ 1.2. 标准方程求解用标准方程法得到线性回归问题的闭式解，表达式为 $$\theta^*=(X^TX)^{-1}X^Ty$$ 可以使用Numpy中的线性代数模块 np.linalg 进行求解 1234567891011121314151617181920212223import numpy as npimport matplotlib.pyplot as plt# 构成原始表达式为 y=4+3x 直线，加上高斯噪声X = 2*np.random.rand(100,1)y = 4 + 3*X + np.random.rand(100,1)# 用标准方程法进行求解X_b = np.c_[np.ones(100,1),X]theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)&gt;&gt;&gt; theta_bestarray([4.2151], [2.7701])# 画出拟合直线X_new = np.array([[0], [2]])X_new_b = np.c_[np.ones((2,1)),X_new]y_predict = X_new_b.T.dot(theta_best)plt.plot(X, y, &apos;b.&apos;)plt.plot(X_new, y_predic, &apos;r-&apos;)plt.axis([0, 2, 0, 15])plt.show() 也可以用Scikit-Learn实现 1234from sklearn.linear_model import LinearRegressionlin_reg = LinearRegression()lin_reg.fit(X,y)lin_reg.predict(X_new) 1.3. 梯度下降求解应用梯度下降时，需要保证所有特征值的大小比例差不多（比如使用Scikit-Learn的StandardScaler类），否则收敛时间会长很多 批量梯度下降 (Batch Gradient Descent, BGD) 成本函数的偏导数 $$\frac{\partial}{\partial \theta_j} MSE(\theta)=\frac{2}{m}\sum_{i=1}^m (\theta^TX^{(i)}-y^{(i)})X_j^{(i)}$$ 向量化表示为 $$\nabla_{\theta}MSE(\theta)=\frac 2m X^T(X\theta-y)$$ 则每一步迭代后得到新的$\theta$为 $$\theta = \theta - \eta \nabla_{\theta}MSE(\theta)$$ 我们来看看这个算法的快速实现： 123456789101112import numpy as npeta = 0.1 # 学习率n_iterations = 1000m = 100# 随机初始化theta = np.random.rand(2,1)for iteration in range(n_iterations): gradients = 2/m * X_b.T.dot(X_b.dot(theta)-y) theta = theta - eta * gradients 随机梯度下降 (Stochastic Gradient Descent, SGD) 批量梯度下降的主要问题是每一次迭代要用到整个训练集，所以如果训练集很大的话，算法会很慢 随机梯度下降是另外一个极端：每一步迭代只从训练集中随机选择一个样本，并且仅依据这一个样本来计算梯度，由于它的随机性，使得它与梯度下降相比要不规则得多，存在比较明显的震荡，但从整体上来看，还是在慢慢下降的 当成本函数不规则时，随机梯度下降其实可以帮助算法跳出局部最优，所以相比梯度下降，它对找到全局最优更有优势 随机性的好处在于可以逃离局部最优，但缺点是永远定位不出最优解，如果要解决这个问题，有一个办法是逐步降低学习率：开始的步长比较大（这有助于快速进展和逃离局部最优），然后越来越小，让算法尽量靠近全局最小值——这个过程叫作模拟退火 确定每个迭代学习率的函数叫作学习计划 (learning schedule) 用自定义的脚本来执行SGD： 1234567891011121314151617n_epochs = 50t0, t1 = 5, 50 # 学习计划的超参数def learning_schedule(t): return t0 / (t + t1)# 随机初始化theta = np.random.rand(2,1)for epoch in range(n_epochs): for i in range(m): random_index = np.random.randint(m) # 随机选择一个样本 xi = X_b[random_index:random_index+1] yi = y[random_index:random_index+1] gradients = 2/m * xi.T.dot(xi.dot(theta) - yi) eta = learning_schedule(epoch * m + i) theta = theta - eta * gradients 在Scikit-Learn里，用SGD执行线性回归可以使用SGDRegression类，其默认优化的成本函数为平方误差 1234from sklearn.linear_model import SGDRegressionsgd_reg = SGDRegression(n_iter=50, penalty=None, eta0=o.1)sgd_reg.fit(X, y.ravel()) 小批量梯度下降 （Mini-Batch Gradient Descent) 优点： 可以从矩阵运算的硬件优化中获得显著的性能提升； 在参数空间的搜索进程不像SGD那样不稳定，所以它会比SGD跟接近最小值； 缺点： 更难从局部最小值中逃脱 2. logistic回归：“达观杯”文本智能处理挑战赛2.1. 任务描述具体任务可以到 ‘达观杯’文本智能处理挑战赛官网 查看 任务：建立模型通过长文本数据正文(article)，预测文本对应的类别(class) 数据： 数据包含2个csv文件： train_set.csv 此数据集用于训练模型，每一行对应一篇文章。文章分别在“字”和“词”的级别上做了脱敏处理。共有四列： 第一列是文章的索引(id)，第二列是文章正文在“字”级别上的表示，即字符相隔正文(article)；第三列是在“词”级别上的表示，即词语相隔正文(word_seg)；第四列是这篇文章的标注(class)。 注：每一个数字对应一个“字”，或“词”，或“标点符号”。“字”的编号与“词”的编号是独立的！ test_set.csv 此数据用于测试。数据格式同train_set.csv，但不包含class 注：test_set与train_test中文章id的编号是独立的。 2.2. 编程实现使用logistic回归来实现这个多元分类任务 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import pandas as pdfrom sklearn.feature_extract.txt import TfidfVectorizerfrom sklearn.decomposition import TruncatedSVDfrom sklearn.linear_model import LogisticRegressionfrom sklearn.model_select import train_test_splitfrom sklearn.externals import joblibimport timet_start = time.time()# ===========================================================# 1. 载人数据与数据预处理df_train = pd.read_csv(&apos;data/train_set.csv&apos;)df_train = df_train.drop(columns=&apos;article&apos;,inplace=True)df_test = pd.read_csv(&apos;data/test_set.csv&apos;)df_test = df_test.drop(columns=&apos;article&apos;,inplace=True)y_train = (df_train[&apos;class&apos;] - 1).values# ===========================================================# 2. 特征工程，这里先使用经典的文本特征提取方法TFIDF，提取的TFIDF特征vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.9, sublinear_tf=True)vectorizer.fit(df_train[&apos;word_seg&apos;])X_train = vectorizer.transform(df_train[&apos;word_seg&apos;])X_test = vectorizer.transform(df_test[&apos;word_seg&apos;])# 可以将得到的TFIDF特征保存至本地&apos;&apos;&apos;data = (X_train,y_train,X_test)f = open(&apos;data/data_tfidf.pkl&apos;,&apos;wb&apos;)pickle.dump(data,f)f.close()&apos;&apos;&apos;# ===========================================================# 3. 特征降维，将上一步提取的TFIDF特征使用lsa方法进行特征降维lsa = TruncatedSVD(n_components=200)X_train = lsa.transform(X_train)X_test = lsa.transform(X_test)# ===========================================================# 4. 训练分类器# 划分训练集和验证集X_train,X_vali,y_train,y_vali = train_test_split(X_train,y_train,test_size=0.1,random_state=0)## multi_class:分类方式选择参数，有&quot;ovr(默认)&quot;和&quot;multinomial&quot;两个值可选择，在二元逻辑回归中无区别## solver:优化算法选择参数，当penalty为&quot;l1&quot;时，参数只能是&quot;liblinear(坐标轴下降法)&quot;；&quot;lbfgs&quot;和&quot;cg&quot;都是关于目标函数的二阶泰勒展开## 当penalty为&quot;l2&quot;时，参数可以是&quot;lbfgs(拟牛顿法)&quot;,&quot;newton_cg(牛顿法变种)&quot;,&quot;seg(minibactch随机平均梯度下降)&quot;lr = LogisticRegression(multi_class=&quot;ovr&quot;,penalty=&quot;l2&quot;,solver=&quot;lbfgs&quot;)lr.fit(X_train,y_train)# 模型的保存与持久化joblib.dump(lr,&quot;logistic_lr.model&quot;)joblib.load(&quot;logistic_lr.model&quot;) #加载模型,会保存该model文件# ===========================================================# 5. 在验证集上评估模型pre_vali = lr.predict(X_vali)pre_score = f1_score(y_true=y_vali,y_pred=pre_vali,average=&apos;macro&apos;)print(&quot;验证集分数：&#123;&#125;&quot;.format(score_vali))# ===========================================================# 6. 对测试集进行预测y_test = lr.predict(X_test) + 1 2.3. sklearn包中logistic算法的使用 3. 决策树：鸢尾花数据分类3.1. DecisionTreeClassifier实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263from itertools import productimport numpy as npimport matplotlib.pyplot as pltfrom sklearn import datasetsfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2from sklearn.decomposition import PCA# 使用自带的iris数据iris = datasets.load_iris()X = iris.data[:, np.arange(0,4)]y = iris.target# 划分训练集与测试集x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=14)print(&quot;训练数据集样本总数:%d;测试数据集样本总数:%d&quot; %(x_train.shape[0],x_test.shape[0]))# 对数据集进行标准化ss = MinMaxScaler()x_train = ss.fit_transform(x_train,y_train)x_test = ss.transform(x_test)# 特征选择：从已有的特征属性中选择出影响目标最大的特征属性# 常用方法：# 离散属性：F统计量、卡方系数、互信息mutual_info_classif# 连续属性：皮尔逊相关系数、F统计量、互信息mutual_info_classif&#125;# 这里使用离散属性的卡方系数，实现函数为SelectKBest，用SelectKBest方法从四个原始特征属性中选择出最能影响目标的3个特征属性ch2 = SelectKBest(chi2,k=3) # k默认为10，指定后会返回想要的特征个数ch2.fit(x_train,y_train)x_train = ch2.transform(x_train)x_test = ch2.transform(x_test)# 特征降维，这里使用PCA方法pca = PCA(n_components=2) # 构建一个PCA对象，设置最终维度为2维。这里为了后边画图方便，将数据维度设置为 2，一般用默认不设置就可以x_train = pca.fit_transform(x_train) # 训练与转换，也可以拆分成两步x_test = pca.transform(x_test)# 训练模型# criterion：指定特征选择标准，可以使用&quot;gini&quot;或者&quot;entropy&quot;，前者代表基尼系数，后者代表信息增益# max_depth：限制树的最大深度4clf = DecisionTreeClassifier(criterion=&quot;entropy&quot;,max_depth=4)clf.fit(X, y) # 拟合模型# 画图x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1# 生成网格采样点xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])Z = Z.reshape(xx.shape)plt.contourf(xx, yy, Z, alpha=0.4)plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)plt.show() 3.2. 可视化决策树scikit-learn中决策树的可视化一般需要安装graphviz，主要包括graphviz的安装和python的graphviz插件的安装 1) 安装graphviz。下载地址在：http://www.graphviz.org/。如果你是linux，可以用apt-get或者yum的方法安装。如果是windows，就在官网下载msi文件安装。无论是linux还是windows，装完后都要设置环境变量，将graphviz的bin目录加到PATH，比如我是windows，将C:/Program Files (x86)/Graphviz2.38/bin/加入了PATH 2) 安装python插件graphviz： pip install graphviz 3) 安装python插件pydotplus: pip install pydotplus 这样环境就搭好了，有时候python会很笨，仍然找不到graphviz，这时，可以在代码里面加入这一行： 1os.environ[&quot;PATH&quot;] += os.pathsep + &apos;C:/Program Files (x86)/Graphviz2.38/bin/&apos; 可视化决策树的代码： 12345678910from IPython.display import Image from sklearn import treeimport pydotplus dot_data = tree.export_graphviz(clf, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True, special_characters=True) graph = pydotplus.graph_from_dot_data(dot_data) Image(graph.create_png()) 4. SVM：sklearn中测试数据4.1. SVM相关知识点回顾4.1.1. SVM与SVR SVM分类算法 其原始形式是： 其中m为样本个数，我们的样本为(x1,y1),(x2,y2),…,(xm,ym)。w,b是我们的分离超平面的w∙ϕ(xi)+b=0系数, ξi为第i个样本的松弛系数， C为惩罚系数。ϕ(xi)为低维到高维的映射函数 通过拉格朗日函数以及对偶化后的形式为： SVR回归算法 其中m为样本个数，我们的样本为(x1,y1),(x2,y2),…,(xm,ym)。w,b是我们的回归超平面的w∙xi+b=0系数, ξ∨i，ξ∧i为第i个样本的松弛系数， C为惩罚系数，ϵ为损失边界，到超平面距离小于ϵ的训练集的点没有损失。ϕ(xi)为低维到高维的映射函数。 4.1.2. 核函数在scikit-learn中，内置的核函数一共有4种： 线性核函数（Linear Kernel）表达式为：K(x,z)=x∙z，就是普通的内积 多项式核函数（Polynomial Kernel）是线性不可分SVM常用的核函数之一，表达式为：K(x,z)=（γx∙z+r)d ，其中，γ,r,d都需要自己调参定义 高斯核函数（Gaussian Kernel），在SVM中也称为径向基核函数（Radial Basis Function,RBF），它是 libsvm 默认的核函数，当然也是 scikit-learn 默认的核函数。表达式为：K(x,z)=exp(−γ||x−z||2)， 其中，γ大于0，需要自己调参定义 Sigmoid核函数（Sigmoid Kernel）也是线性不可分SVM常用的核函数之一，表达式为：K(x,z)=tanh（γx∙z+r)， 其中，γ，r都需要自己调参定义 一般情况下，对非线性数据使用默认的高斯核函数会有比较好的效果，如果你不是SVM调参高手的话，建议使用高斯核来做数据分析。 4.2. sklearn中SVM相关库的简介scikit-learn SVM算法库封装了libsvm 和 liblinear 的实现，仅仅重写了算法了接口部分 4.2.1. 分类库与回归库 分类算法库 包括SVC， NuSVC，和LinearSVC 3个类 对于SVC， NuSVC，和LinearSVC 3个分类的类，SVC和 NuSVC差不多，区别仅仅在于对损失的度量方式不同，而LinearSVC从名字就可以看出，他是线性分类，也就是不支持各种低维到高维的核函数，仅仅支持线性核函数，对线性不可分的数据不能使用 回归算法库 包括SVR， NuSVR，和LinearSVR 3个类 同样的，对于SVR， NuSVR，和LinearSVR 3个回归的类， SVR和NuSVR差不多，区别也仅仅在于对损失的度量方式不同。LinearSVR是线性回归，只能使用线性核函数 4.2.2. 高斯核调参4.2.2.1. 需要调节的参数 SVM分类模型 如果是SVM分类模型，这两个超参数分别是惩罚系数C和RBF核函数的系数γ 惩罚系数C 它在优化函数里主要是平衡支持向量的复杂度和误分类率这两者之间的关系，可以理解为正则化系数 当C比较大时，我们的损失函数也会越大，这意味着我们不愿意放弃比较远的离群点。这样我们会有更加多的支持向量，也就是说支持向量和超平面的模型也会变得越复杂，也容易过拟合 当C比较小时，意味我们不想理那些离群点，会选择较少的样本来做支持向量，最终的支持向量和超平面的模型也会简单 scikit-learn中默认值是1 C越大，泛化能力越差，易出现过拟合现象；C越小，泛化能力越好，易出现过欠拟合现象 BF核函数的参数γ RBF 核函数K(x,z)=exp(−γ||x−z||2) γ&gt;0 γ主要定义了单个样本对整个分类超平面的影响 当γ比较小时，单个样本对整个分类超平面的影响比较小，不容易被选择为支持向量 当γ比较大时，单个样本对整个分类超平面的影响比较大，更容易被选择为支持向量，或者说整个模型的支持向量也会多 scikit-learn中默认值是 1/样本特征数 γ越大，训练集拟合越好，泛化能力越差，易出现过拟合现象 如果把惩罚系数C和RBF核函数的系数γ一起看，当C比较大， γ比较大时，我们会有更多的支持向量，我们的模型会比较复杂，容易过拟合一些。如果C比较小 ， γ比较小时，模型会变得简单，支持向量的个数会少 SVM回归模型 SVM回归模型的RBF核比分类模型要复杂一点，因为此时我们除了惩罚系数C和RBF核函数的系数γ之外，还多了一个损失距离度量ϵ 对于损失距离度量ϵ，它决定了样本点到超平面的距离损失 当 ϵ 比较大时，损失较小，更多的点在损失距离范围之内，而没有损失,模型较简单 当 ϵ 比较小时，损失函数会较大，模型也会变得复杂 scikit-learn中默认值是0.1 如果把惩罚系数C，RBF核函数的系数γ和损失距离度量ϵ一起看，当C比较大， γ比较大，ϵ比较小时，我们会有更多的支持向量，我们的模型会比较复杂，容易过拟合一些。如果C比较小 ， γ比较小，ϵ比较大时，模型会变得简单，支持向量的个数会少 4.2.2.2. 调参方法：网格搜索对于SVM的RBF核，我们主要的调参方法都是交叉验证。具体在scikit-learn中，主要是使用网格搜索，即GridSearchCV类 123from sklearn.model_selection import GridSearchCVgrid = GridSearchCV(SVC(), param_grid=&#123;&quot;C&quot;:[0.1, 1, 10], &quot;gamma&quot;: [1, 0.1, 0.01]&#125;, cv=4)grid.fit(X, y) 将GridSearchCV类用于SVM RBF调参时要注意的参数有： 1) estimator：即我们的模型，此处我们就是带高斯核的SVC或者SVR 2) param_grid：即我们要调参的参数列表。 比如我们用SVC分类模型的话，那么param_grid可以定义为{“C”:[0.1, 1, 10], “gamma”: [0.1, 0.2, 0.3]}，这样我们就会有9种超参数的组合来进行网格搜索，选择一个拟合分数最好的超平面系数 3) cv：S折交叉验证的折数，即将训练集分成多少份来进行交叉验证。默认是3。如果样本较多的话，可以适度增大cv的值 4.3. 编程实现 生成测试数据 123456from sklearn.datasets import make_circlesfrom sklearn.preprocessing import StandardScaler# 生成一些随机数据用于后续分类X, y = make_circles(noise=0.2, factor=0.5, random_state=1) # 生成时加入了一些噪声X = StandardScaler().fit_transform(X) # 把数据归一化 生成的随机数据可视化结果如下： 调参 接着采用网格搜索的策略进行RBF核函数参数搜索 12345678from sklearn.model_selection import GridSearchCVgrid = GridSearchCV(SVC(), param_grid=&#123;&quot;C&quot;:[0.1, 1, 10], &quot;gamma&quot;: [1, 0.1, 0.01]&#125;, cv=4) # 总共有9种参数组合的搜索空间grid.fit(X, y)print(&quot;The best parameters are %s with a score of %0.2f&quot; % (grid.best_params_, grid.best_score_))输出为：The best parameters are &#123;&apos;C&apos;: 10, &apos;gamma&apos;: 0.1&#125; with a score of 0.91 可以对9种参数组合训练的结果进行可视化，观察分类的效果： 12345678910111213141516171819202122232425x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1xx, yy = np.meshgrid(np.arange(x_min, x_max,0.02), np.arange(y_min, y_max, 0.02))for i, C in enumerate((0.1, 1, 10)): for j, gamma in enumerate((1, 0.1, 0.01)): plt.subplot() clf = SVC(C=C, gamma=gamma) clf.fit(X,y) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) # Put the result into a color plot Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8) # Plot also the training points plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm) plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) plt.xticks(()) plt.yticks(()) plt.xlabel(&quot; gamma=&quot; + str(gamma) + &quot; C=&quot; + str(C)) plt.show() C \ gamma 1 0.1 0.001 0.1 1 10 5.朴素贝叶斯5.1. 朴素贝叶斯相关知识点回顾5.1.1. 什么是朴素贝叶斯分类器判别式模型（discriminative models)：像决策树、BP神经网络、支持向量机等，都可以归入判别式模型，它们都是直接学习出输出Y与特征X的关系，如： 决策函数 Y=f(X) 条件概率 P(Y|X) 生成式模型 (gernerative models)：先对联合概率分布 P(X,Y) 进行建模，然后再由此获得P(Y|X) = P(X,Y)/P(X) 贝叶斯学派的思想： 贝叶斯学派的思想可以概括为先验概率+数据=后验概率。也就是说我们在实际问题中需要得到的后验概率，可以通过先验概率和数据一起综合得到。数据大家好理解，被频率学派攻击的是先验概率，一般来说先验概率就是我们对于数据所在领域的历史经验，但是这个经验常常难以量化或者模型化，于是贝叶斯学派大胆的假设先验分布的模型，比如正态分布，beta分布等。这个假设一般没有特定的依据，因此一直被频率学派认为很荒谬。虽然难以从严密的数学逻辑里推出贝叶斯学派的逻辑，但是在很多实际应用中，贝叶斯理论很好用，比如垃圾邮件分类，文本分类 如何对P(X,Y)进行建模？ 假如我们的分类模型样本是： (x(1)1,x(1)2,…x(1)n,y1), (x(2)1,x(2)2,…x(2)n,y2),…(x(m)1,x(m)2, …, x(m)n,ym) 则 P(X, Y) = P(Y) * P( X = (x1, x2, …, xn) | Y ) 其中 P( X = (x1, x2, …, xn) | Y ) = P( x1 | Y) * P( x2 | Y, x1) * … P( xn | Y, x1, … , xn-1) 这是一个超级复杂的有n个维度的条件分布，很难求出 朴素贝叶斯模型在这里做了一个大胆的假设，即X的n个维度之间相互独立，这样就可以得出： P( X = (x1, x2, …, xn) | Y ) = P( x1 | Y) * P( x2 | Y) * … * P( xn | Y) 5.1.2. 朴素贝叶斯推断 5.1.3. 朴素贝叶斯学习需要从训练样本中学习到以下两个参数： 先验概率 P(c) P(c)表示了样本空间中各类样本所占的比例 根据大数定律，当训练集中包含充足的独立同分布样本时，P(c)可根据各类样本出现的频率来估计 类条件概率（又称为似然） P(xi | c) （1）如果 xi 是离散的，可以假设 xi符合多项式分布，这样得到 P(xi | c) 是在样本类别 c 中，特征 xi 出现的频率 （2）如果 xi 是连续属性，可以假设 P(xi | c) ~ N( μc,i , σ2c,i ) 5.2. sklearn中朴素贝叶斯类库的简介在scikit-learn中，一共有3个朴素贝叶斯的分类算法类 GaussianNB：先验为高斯分布的朴素贝叶斯 MultinomialNB：先验为多项式分布的朴素贝叶斯 BernoulliNB：先验为伯努利分布的朴素贝叶斯 这三个类适用的分类场景各不相同 一般来说，如果样本特征的分布大部分是连续值，使用GaussianNB会比较好 如果如果样本特征的分大部分是多元离散值，使用MultinomialNB比较合适 如果样本特征是二元离散值或者很稀疏的多元离散值，应该使用BernoulliNB。 5.2.1. GaussianNB类GaussianNB类的主要参数仅有一个，即先验概率priors 这个值默认不给出，如果不给出此时P(Y=c)= mc / m，如果给出的话就以priors 为准 在使用GaussianNB的fit方法拟合数据后，我们可以进行预测。此时预测有三种方法 predict方法：就是我们最常用的预测方法，直接给出测试集的预测类别输出； predict_proba方法：给出测试集样本在各个类别上预测的概率； predict_log_proba方法：和predict_proba类似，它会给出测试集样本在各个类别上预测的概率的一个对数转化； 12345678from sklearn.naive_bayes import GaussianNBclf = GaussianNB()#拟合数据clf.fit(X, Y)#进行预测clf.predict([[-0.8, -1]]) 5.2.2. MultinomialNB类MultinomialNB假设特征的先验概率为多项式分布，即如下式： MultinomialNB参数比GaussianNB多，但是一共也只有仅仅3个 参数alpha：为上面的常数λ。如果你没有特别的需要，用默认的1即可。如果发现拟合的不好，需要调优时，可以选择稍大于1或者稍小于1的数 参数fit_prior：是否要考虑先验概率，如果是false，则所有的样本类别输出都有相同的类别先验概率；否则可以自己用第三个参数class_prior输入先验概率，或者不输入第三个参数class_prior让MultinomialNB自己从训练集样本来计算先验概率 参数class_prior：输入先验概率，若不输入第三个参数class_prior让MultinomialNB自己从训练集样本来计算先验概率 5.2.3. BernoulliNB类 其中，x i 只能取0或1 BernoulliNB一共有4个参数，其中3个参数的名字和意义和MultinomialNB完全相同 唯一增加的一个参数是binarize，这个参数主要是用来帮BernoulliNB处理二项分布的。如果不输入，则BernoulliNB认为每个数据特征都已经是二元的。否则的话，小于binarize的会归为一类，大于binarize的会归为另外一类 6. 神经网络（sklearn）：手写数字识别5.1. 神经网络相关知识点回顾6.1.1. 感知机：神经网络的最小单元神经网络中最基本的成分是神经元（neuron）模型，最常见的模型是M-P神经元模型，也可以称为感知机（Perceptron） 若对生物学有一点基础的了解，就可以知道上面构造的简化的M-P神经元模型几乎完全吻合生物学上的定义： 当前神经元通过与上游神经元之间的突触结构，接受来自上游的神经元1到n传递过来的信号x1,x2…xn，由于不同的上游信号传递过来它们的影响程度是不一样的，因此对它们进行加权求和，即∑wixi，则得到总的上游信号 另外当前神经元还有一个激活阈值θ，若上游信号的汇总信号强度大于θ，即则∑wixi-θ &gt; 0，则当前神经元被激活，处于兴奋状态；若若上游信号的汇总信号强度小于θ，即则∑wixi-θ &lt; 0，则当前神经元未被激活，处于抑制状态 对于生物学上的神经元来说，它只有两个状态： “1”：对应神经元兴奋； “0”：对应神经元抑制； 则它接受的信号为上游神经元的输出为 xi∈{0,1}，它的输出信号为 y∈{0,1} 则它的理想激活函数是阶跃函数，即f(x)=sgn(x) 然而，阶跃函数具有不连续、不光滑等不太好的性质，因此实际上常用sigmoid函数作为激活函数 则此时激活函数为f(x)=sigmoid(∑wixi-θ），很明显若选择sigmod函数作为感知机的激活函数，则此时感知机等价于一个logistic回归分类器 类比logistic回归分类器 对于线性可分的分类任务，感知机完全等价于logistic回归分类器 对于线性不可分问题，logistic回归可以构造额外的高阶多项式： 本质上是将原始特征空间中线性不可分的样本向高维特征空间进行映射，使得它们在新的高维特征空间中线性可分 而感知器无法进行高维空间的映射，只能基于原始特征空间寻找线性判别边界，因此感知机在面对简单的非线性可分问题是，往往无法得到理想的判别面 感知机的学习规则： 6.1.2. 神经网络的学习：BackPropagation算法神经网络的学习算法本质上是数学上常用的梯度下降（Gradient Descend）算法 依据在一轮神经网络的训练过程中所用到的训练样本的数量的不同，可分为以下两类： 标准BP算法：每次网络训练只针对单个训练样本，一次训练就输入一个训练样本，更新一次网络参数； 累积BP算法：每次网络训练只针对所有训练样本，一次将所有训练样本输入，更新一次网络参数； 6.1.2.1. 标准BP算法 以wh,j为例进行推导 在标准BP算法中，由于只对输入的一个训练样本进行网络参数的训练，因此它的目标函数为当前样本k的均方误差Ek 此时关键在于怎么推出△whj的表达式？ 根据链式求导法则，Whj的影响链条为： 因此△whj可以写成： 中间推导过程省略，最后得到的△whj的表达式为： 6.1.2.2. 累积BP算法累积BP算法的目标是最小化训练集的累积误差： 这是累积BP算法与标准BP算法的最大的也是最本质的差别 标准BP算法存在的问题： 每次更新只针对单个训练样本，参数更新得非常频繁，而且不同样本训练的效果可能出现相互“抵消”的现象 因此为了达到与累积误差相同的极小点，标准BP算法往往需要进行更多次的迭代 累积BP算法的优缺点： 优点：直接对累积误差最小化，它在读取整个训练样本集D后才对参数进行一次更新，其参数更新的频率低得多 缺点：累积误差下降到一点程度之后，进一步下降会非常缓慢 此时标准BP算法往往会更快得到较好的解，因为其解的震荡性给它带来了一个好处——容易跳出局部最优 6.2. sklearn中神经网络库的简介sklearn中神经网络的实现不适用于大规模数据应用。特别是 scikit-learn 不支持 GPU 6.2.1. MLPClassifier神经网络又称为多层感知机（Multi-layer Perceptron，MLP） sklearn中用MLP进行分类的类为MLPClassifier 主要参数说明： 参数 说明 hidden_​​layer_sizes tuple，length = n_layers - 2，默认值（100，）第i个元素表示第i个隐藏层中的神经元数量。 activation {‘identity’，‘logistic’，‘tanh’，‘relu’}，默认’relu’ 隐藏层的激活函数：‘identity’，无操作激活，对实现线性瓶颈很有用，返回f（x）= x；‘logistic’，logistic sigmoid函数，返回f（x）= 1 /（1 + exp（-x））；‘tanh’，双曲tan函数，返回f（x）= tanh（x）；‘relu’，整流后的线性单位函数，返回f（x）= max（0，x） slover {‘lbfgs’，‘sgd’，‘adam’}，默认’adam’。权重优化的求解器：’lbfgs’是准牛顿方法族的优化器；’sgd’指的是随机梯度下降。’adam’是指由Kingma，Diederik和Jimmy Ba提出的基于随机梯度的优化器。注意：默认解算器“adam”在相对较大的数据集（包含数千个训练样本或更多）方面在训练时间和验证分数方面都能很好地工作。但是，对于小型数据集，“lbfgs”可以更快地收敛并且表现更好。 alpha float，可选，默认为0.0001。L2惩罚（正则化项）参数。 batch_size int，optional，默认’auto’。用于随机优化器的minibatch的大小。如果slover是’lbfgs’，则分类器将不使用minibatch。设置为“auto”时，batch_size = min（200，n_samples） learning_rate {‘常数’，‘invscaling’，‘自适应’}，默认’常数”。 用于权重更新。仅在solver =’sgd’时使用。’constant’是’learning_rate_init’给出的恒定学习率；’invscaling’使用’power_t’的逆缩放指数在每个时间步’t’逐渐降低学习速率learning_rate_， effective_learning_rate = learning_rate_init / pow（t，power_t）；只要训练损失不断减少，“adaptive”将学习速率保持为“learning_rate_init”。每当两个连续的时期未能将训练损失减少至少tol，或者如果’early_stopping’开启则未能将验证分数增加至少tol，则将当前学习速率除以5。 MLPClassifier的训练使用BP算法，其使用交叉熵损失函数（Cross-Entropy loss function） MLP的训练需要准备 Xm*n：m个训练样本的n维特征向量构成的特征矩阵 Ym：m训练样本的目标值组成的向量 1234567891011from sklearn.neural_network import MLPClassifierX = [[0., 0.], [1., 1.]]y = [0, 1]# 模型训练clf = MLPClassifier(solver=&apos;lbfgs&apos;, alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)clf.fit(X, y)# 模型预测clf.predict([[2., 2.], [-1., -2.]]) 以上训练好的MLP模型保存在clf对象中，该对象有以下两个子变量： clf.coefs_：模型的一系列权重矩阵 1234&gt; # 用以下命令查看每一层的权重矩阵的维度&gt; &gt;&gt;[coef.shape for coef in clf.coefs_]&gt; [(2, 5), (5, 2), (2, 1)]&gt; 其中下标为 i 的权重矩阵表示第 i 层和第 i+1 层之间的权重 intercepts_：一系列偏置向量 其中的下标为 i 的向量表示添加到第 i+1 层的偏置值 多分类任务 MLPClassifier 通过应用 Softmax 作为输出函数来支持多分类 softmax回归本质上是将原先常见的二元分类任务神经网络的输出层采用的sigmoid激活函数换成softmax回归 例如要利用神经网络进行k类的分类，则神经网络结构如下： 那么最后一层即网络的输出层所采用的激活函数——softmax回归到底长什么样呢？ 因此该网络的输出层又被称为softmax layer 因此在提供训练集时，若总共有k的类别，当某一个训练样本的类别为第i类时，它的目标值应该为[0, 0, ..., 1, ..., 0]，只在向量的第i个位置标为1，其他位置都为0 多标签分类任务 一个样本可能属于多个类别。 对于每个类，原始输出经过 logistic 函数变换后，大于或等于 0.5 的值将进为 1，否则为 0。 对于样本的预测输出，值为 1 的索引位置表示该样本的分类类别 1234567891011&gt;&gt;&gt; X = [[0., 0.], [1., 1.]]&gt;&gt;&gt; y = [[0, 1], [1, 1]]&gt;&gt;&gt; clf = MLPClassifier(solver=&apos;lbfgs&apos;, alpha=1e-5,... hidden_layer_sizes=(15,), random_state=1)...&gt;&gt;&gt; clf.fit(X, y) &gt;&gt;&gt; clf.predict([[1., 2.]])array([[1, 1]])&gt;&gt;&gt; clf.predict([[0., 0.]])array([[0, 1]]) 6.3. 编程实现对于神经网络，要说它的Hello world，莫过于识别手写数字了 首先要获取实验数据，下载地址：http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz 数组中的每一行都表示一个灰度图像。每个元素都表示像素所对应的灰度值。 数据集中将数据分成3类：训练集，验证集，测试集。一般情况下会将训练数据分为训练集和测试集 为什么要将训练数据分为训练集和测试集？ 将原始数据分开，保证用于测试的数据是训练模型时从未遇到的数据可以使测试更客观。否则就像学习教课书的知识，又只考教课书的知识，就算不理解记下了就能得高分但遇到新问题就傻眼了。 好一点的做法就是用训练集当课本给他上课，先找出把课本知识掌握好的人，再参加由新题组成的月考即测试集，若是还是得分高，那就是真懂不是死记硬背了。 但这样选出来的模型实际是还是用训练集和测试集共同得到的，再进一步，用训练集和验证集反复训练和检测，得到最好的模型，再用测试集来一局定输赢即期末考试，这样选出来的就更好了。 本实验中为了方便将训练集与验证集合并 先载入数据，并简单查看一下数据 1234567891011121314151617181920212223242526from sklearn.neural_network import MLPClassifierimport numpy as npimport pickleimport gzipimport matplotlib.pyplot as plt# 加载数据with gzip.open(r&quot;mnist.pkl.gz&quot;) as fp: training_data, valid_data, test_data = pickle.load(fp)X_training_data, y_training_data = training_dataX_valid_data, y_valid_data = valid_dataX_test_data, y_test_data = test_data# 合并训练集,验证集X_training = np.vstack((X_training_data, X_valid_data)) y_training = np.append(y_training_data, y_valid_data)def show_data_struct(): print X_training_data.shape, y_training_data.shape print X_valid_data.shape, y_valid_data.shape print X_test_data.shape, y_test_data.shape print X_training_data[0] print y_training_data[0]show_data_struct() 随便看几张训练图片 1234567891011121314def show_image(): plt.figure(1) for i in range(10): image = X_training[i] # 得到包含第i张图的像素向量，为1*768 pixels = image.reshape((28, 28)) # 将原始像素向量转换为28*28的像素矩阵 plt.subplot(5,2,i+1) plt.imshow(pixels, cmap=&apos;gray&apos;) plt.title(y_training[i]) plt.axis(&apos;off&apos;) plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.45, wspace=0.85) plt.show()show_image() 训练模型 为了获得比默认参数更佳的模型，我们采用网格搜索法搜索更优的训练超参数，使用gridSearchCV实现，上文4.2.2.2. 调参方法：网格搜索中有较为详细的说明 12345678910from sklearn.model_selection import GridSearchCVmlp = MLPClassifier()mlp_clf__tuned_parameters = &#123;&quot;hidden_layer_sizes&quot;: [(100,), (100, 30)], &quot;solver&quot;: [&apos;adam&apos;, &apos;sgd&apos;, &apos;lbfgs&apos;], &quot;max_iter&quot;: [20], &quot;verbose&quot;: [True] &#125;estimator = GridSearchCV(mlp, mlp_clf__tuned_parameters, n_jobs=6)estimator.fit(X_training, y_training) 7. 特征选择/特征降维：sklearn包中feature_selection模块的使用6.1. 相关知识回顾7.1.1. 特征降维7.1.2. 特征选择7.1.2.1. 特征选择的一般解决思路目标：从初始特征集合中选取一个包含所有重要信息的特征子集 可用策略: （1）找到该领域懂业务的专家，让他们给一些建议。比如我们需要解决一个药品疗效的分类问题，那么先找到领域专家，向他们咨询哪些因素（特征）会对该药品的疗效产生影响，较大影响的和较小影响的都要。这些特征就是我们的特征的第一候选集 （2）若没有任何领域知识作为先验假设 方差筛选：最简单的方法，方差越大的特征，那么我们可以认为它是比较有用的。如果方差较小，比如小于1，那么这个特征可能对我们的算法作用没有那么大。最极端的，如果某个特征方差为0，即所有的样本该特征的取值都是一样的，那么它对我们的模型训练没有任何作用，可以直接舍弃； 遍历所有可能的子集——计算上不可行； 产生一个“候选子集”，评价出它的好坏，基于评价结果产生下一个候选子集，再对其进行评价…这个过程持续下去，直到无法再找到更好的候选子集为止——这就是我们所说的一般的特征选择问题的处理策略； 那么这就产生了两个问题： 如何根据评价结果获取下一个候选特征子集？——子集搜索 如何评价候选特征子集的好坏？——子集评价 子集搜索 （1）给定特征结合${a_1,a_2,…,a_d}$，可将每个特征看作一个候选子集，对这d个候选单特征子集进行评价，假定${a_2}$最优，将${a_2}$作为第一轮的选定子集； （2）在上一轮的选定子集中加入一个特征，构成包含两个特征的候选子集 假定在这d-1个候选两特征子集中${a_2,a_4}$最优，且由于${a_2}$，于是将${a_2,a_4}$作为本轮的选定子集； （3）假定在第k+1轮时，最优的候选特征子集不如上一轮的选定集，则停止生产候选子集，并将上一轮的k特征子集作为特征选择结果 以上逐渐增加相关特征的策略称为 “前向”搜索 ，与它相对于的还有 “反向”搜索 和 “双向”搜索 注意：上述策略都是贪心的，因为它们都仅考虑了使本轮选定集最优 子集评价 给定数据集D，假定D中第i类样本所占的比例为$p_i(i=1,2,…,|y|)$，为了后面方便进行讨论，假定样本属性均为离散型 对于属性子集A，假定根据其取值将D分成V个子集${D^1,D^2,…,D^V}$，每个子集中的样本在A上取值相同 属性子集A的信息增益： $$Gain(A)=Ent(D)-\sum_{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v)$$ 其中信息熵定义为： $$Ent(D)=-\sum_{k=1}^{|y|}p_klog_2p_k$$ 信息熵增益越大，意味着特征子集A包含的有助于分类的信息越多 7.1.2.2. 具体的几种特征选择类型特征选择方法有很多，一般分为三类： 过滤式选择 过滤式选择：先对数据集进行特征选择，它按照特征的发散性或者相关性指标对各个特征进行评分，设定评分阈值或者待选择阈值的个数，选择合适特征然后再训练学习器——特征选择过程与后续学习器无关 相当于：先用特征选择过程对初始特征进行“过滤”，再用过滤后的特征来训练模型 方差筛选就是过滤法的一种 一个经典的过滤式特征选择方法为Relief（Relevant Features）方法： 该方法设计了一个“相关性统计量”来度量特征的重要性，该统计量是一个向量，其每一个分量对于一个初始特征，而特征子集的重要性则由子集中每个特征所对应的相关统计量之和来决定 Relief的关键是如何确定相关统计量的： 给定训练集${(x_1,y_1),(x_2,y_2),…,(x_m,y_m)}$，对每个示例$x_i$，Relief先在$x_i$的同类样本中寻找其最近邻$x_{i,nh}$，称为“猜中近邻”(near-hit)，再从$x_i$的异类样本中寻找其最近邻$x_{i,nm}$，称为“猜错近邻”(near-miss) 相关统计量对应于属性j的分量为： $$\delta^j=\sum_i[-diff(x_i^j,x_{i,nh}^j)^2+diff(x_i^j,x_{i,nm}^j)^2]$$ 其中$diff(x_a^j,x_b^j)$的取值取决于属性j的类型： 离散型：$x_a^j=x_b^j$时，$diff(x_a^j,x_b^j)=0$，否则为1； 连续型：$diff(x_a^j,x_b^j)=|x_a^j-x_b^j|$ 理解Relief相关统计量的意义： 若$x_i$与其猜中近邻$x_{i,nh}$在属性j上的距离小于$x_i$与其猜错近邻$x_{i,nm}$的距离，则说明该属性对于区分同类样本与异类样本是有益的，于是增大j的统计量； 反之，说明属性j起负面作用，生与死减小j对于的统计量 以上说的是Relief对于二分类问题的情况，其扩展变体Relief-F能处理多分类问题，由于篇幅有限，此处略去 其实不同过滤式选择方法主要的差别在于“相关统计量”的选择，其他常用的统计量和它们的适用场景： 相关系数：主要用于输出连续值的监督学习算法中。分别计算所有训练集中各个特征与输出值之间的相关系数，设定一个阈值，选择相关系数较大的部分特征； 假设检验：比如卡方检验。卡方检验可以检验某个特征分布和输出值分布之间的相关性，它比粗暴的方差法好用。 在sklearn中，使用chi2这个类来做卡方检验得到所有特征的卡方值与显著性水平P临界值，我们可以给定卡方值阈值， 选择卡方值较大的部分特征。 除了卡方检验，我们还可以使用F检验和t检验，它们都是使用假设检验的方法，只是使用的统计分布不是卡方分布，而是F分布和t分布而已。在sklearn中，有F检验的函数f_classif和f_regression，分别在分类和回归特征选择时使用。 互信息：从信息熵的角度分析各个特征和输出值之间的关系评分 互信息值越大，说明该特征和输出值之间的相关性越大，越需要保留。在sklearn中，可以使用mutual_info_classif(分类)和mutual_info_regression(回归)来计算各个输入特征和输出值之间的互信息。 包裹式选择 包裹式选择：直接把最终将要使用的学习器的性能作为特征子集的评价准则 即，为给定的学习器选择最有利于其性能、“量身定做”的特征子集 其特点： 由于包裹式特征选择方法直接针对给定学习器进行优化，因此从最终性能来看，它比过滤式特征选择要好； 特征选择过程中需多次训练学习器，因此其计算开销通常比过滤式大得多； 最常用的包装法是递归消除特征法(recursive feature elimination,以下简称RFE)。递归消除特征法使用一个机器学习模型来进行多轮训练，每轮训练后，消除若干权值系数的对应的特征，再基于新的特征集进行下一轮训练。在sklearn中，可以使用RFE函数来选择特征。 以经典的SVM-RFE算法来讨论这个特征选择的思路： 这个算法以支持向量机来做RFE的机器学习模型选择特征 它在第一轮训练的时候，会选择所有的特征来训练，得到了分类的超平面$wx+b=0$后，如果有n个特征，那么RFE-SVM会选择出$w$中分量的平方值$w^2_i$最小的那个序号i对应的特征，将其排除 在第二轮的时候，特征数就剩下n-1个了，我们继续用这n-1个特征和输出值来训练SVM，同样的，去掉$w^2_i$最小的那个序号i对应的特征。以此类推，直到剩下的特征数满足我们的需求为止。 嵌入式选择——基于$L_1$正则化 嵌入式特征选择：将特征选择与学习器训练过程融合为一体，两者在同一个优化过程中完成，即在学习训练过程中自动地进行了特征选择 对于简单的线性回归问题，它的优化目标一般为最小化平方误差： $$\min_\omega \, \sum_{i=1}^m(y_i-\omega^Tx_i)$$ 当样本特征多而样本数又相对较少时，很容易陷入过拟合，为了缓解过拟合，需要对上面的优化目标引入最终项 若使用L2范数正则化则有 $$\min_\omega \, \sum_{i=1}^m(y_i-\omega^Tx_i)-\lambda||\omega||_2^2\quad(岭回归）$$ 若使用L1范数则有 $$\min_\omega \, \sum_{i=1}^m(y_i-\omega^Tx_i)-\lambda||\omega||_1\quad(LASSO）$$ L1和L2范数都能降低过拟合的风险，而L1范数还会带来一个额外的好处：跟容易获得“稀疏”（sparse）解，即它求得的$\omega$会有更少的非零分量 7.1.3. 构造高级特征在我们拿到已有的特征后，我们还可以根据需要寻找到更多的高级特征 比如有车的路程特征和时间间隔特征，我们就可以得到车的平均速度这个二级特征。根据车的速度特征，我们就可以得到车的加速度这个三级特征，根据车的加速度特征，我们就可以得到车的加加速度这个四级特征……也就是说，高级特征可以一直寻找下去 在Kaggle之类的算法竞赛中，高分团队主要使用的方法除了集成学习算法，剩下的主要就是在高级特征上面做文章。所以寻找高级特征是模型优化的必要步骤之一 寻找高级特征最常用的方法有： 若干项特征加和： 我们假设你希望根据每日销售额得到一周销售额的特征。你可以将最近的7天的销售额相加得到 若干项特征之差： 假设你已经拥有每周销售额以及每月销售额两项特征，可以求一周前一月内的销售额 若干项特征乘积： 假设你有商品价格和商品销量的特征，那么就可以得到销售额的特征 若干项特征除商： 假设你有每个用户的销售额和购买的商品件数，那么就是得到该用户平均每件商品的销售额 寻找高级特征的方法远不止于此，它需要你根据你的业务和模型需要而得，而不是随便的两两组合形成高级特征，这样容易导致特征爆炸，反而没有办法得到较好的模型 7.2. 功能实现在 sklearn.feature_selection 模块中的类可以用来对样本集进行 feature selection（特征选择）和 dimensionality reduction（降维） 7.2.1. 特征降维7.2.2. 特征选择7.2.2.1. 移除低方差特征VarianceThreshold 是特征选择的一个简单基本方法，它会移除所有那些方差不满足一些阈值的特征。默认情况下，它将会移除所有的零方差特征，即那些在所有的样本上的取值均不变的特征。 例如，假设我们有一个特征是布尔值的数据集，我们想要移除那些在整个数据集中特征值为0或者为1的比例超过80%的特征。布尔特征是伯努利（ Bernoulli ）随机变量，变量的方差为 $$Var[X] = p(1 - p)$$ 因此，我们可以使用阈值 .8 * (1 - .8) 进行选择: 12345678910111213from sklearn.feature_selection import VarianceThresholdX = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]sel = VarianceThreshold(threshold=(.8 * (1 - .8)))sel.fit_transform(X)特征选择结果如下：array([[0, 1], [1, 0], [0, 0], [1, 1], [1, 0], [1, 1]]) 7.2.2.2. 单变量特征选择单变量的特征选择是通过基于单变量的统计测试来选择最好的特征，因此它实际采用的特征选择策略是过滤式特征选择方法中的Relief方法 SelectKBest 移除那些除了评分最高的 K 个特征之外的所有特征 SelectPercentile 移除除了用户指定的最高得分百分比之外的所有特征 对每个特征应用常见的单变量统计测试: 假阳性率（false positive rate） SelectFpr, 伪发现率（false discovery rate） SelectFdr , 或者族系误差（family wise error） SelectFwe 。 GenericUnivariateSelect 允许使用可配置方法来进行单变量特征选择。它允许超参数搜索评估器来选择最好的单变量特征。 例如下面的实例，我们可以使用 x2 检验样本集来选择最好的两个特征： 12345678910from sklearn.datasets import load_irisfrom sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2iris = load_iris()X, y = iris.data, iris.targetX.shape# 输出为：(150, 4)X_new = SelectKBest(chi2, k=2).fit_transform(X, y)X_new.shape# 输出为：(150, 2) 对于回归问题和分类问题，适用的统计量是不一样的： 对于回归: f_regression , mutual_info_regression 对于分类: chi2 , f_classif , mutual_info_classif 注意：不要使用一个回归评分函数来处理分类问题，你会得到无用的结果 7.2.2.3. 递归式特征消除给定一个外部的估计器，可以对特征赋予一定的权重（比如，线性模型的相关系数），recursive feature elimination ( RFE ) 通过考虑越来越小的特征集合来递归的选择特征。 首先，评估器在初始的特征集合上面训练并且每一个特征的重要程度是通过一个 coef_ 属性 或者 feature_importances_ 属性来获得。 然后，从当前的特征集合中移除最不重要的特征。在特征集合上不断的重复递归这个步骤，直到最终达到所需要的特征数量为止。 RFECV 在一个交叉验证的循环中执行 RFE 来找到最优的特征数量 参考资料： (1) 机械工业出版社《机器学习实战：基于Scikit-Learn和TensorFlow》 (2) 【GitHub】MLjian/TextClassificationImplement (3) 刘建平Pinard《scikit-learn 逻辑回归类库使用小结》 (4) loveliuzz《机器学习sklearn19.0——Logistic回归算法》 (5) 刘建平Pinard《scikit-learn决策树算法类库使用小结》 (6) loveliuzz《机器学习sklearn19.0——决策树算法》 (7) 刘建平Pinard《scikit-learn 支持向量机算法库使用小结》 (8) 刘建平Pinard《支持向量机高斯核调参小结》 (9) loveliuzz《机器学习sklearn19.0——SVM算法》 (10) 周志华《机器学习：第7章 贝叶斯分类器》 (11) 刘建平Pinard《朴素贝叶斯算法原理小结》 (12) 刘建平Pinard《scikit-learn 朴素贝叶斯类库使用小结》 (13) 周志华《机器学习：第5章 神经网络》 (14) 吴恩达《deeplearning.ai：改善深层神经网络》 (15) scikit-learn中文网《神经网络模型（有监督）》 (16) 啊噗不是阿婆主《sklearn 神经网络MLPclassifier参数详解》 (17) 多问Why.简书《SkLearn之MLP》 (18) 周志华《机器学习：第11章 特征选择与稀疏学习》 (19) 刘建平Pinard《特征工程之特征选择》 (20) sklearn中文文档：特征选择 (21) sklearn英文文档：1.13. Feature selection]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>Machine-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《数学之美》阅读笔记]]></title>
    <url>%2F2019%2F04%2F06%2FBeauty-Mathmatics-Note%2F</url>
    <content type="text"><![CDATA[1. 奇异值分解 (SVD) 及其应用1.1. SVD算法矩阵A可以如下分解成三个矩阵的乘积： $$A_{MN}=X_{MM} \times B_{MN} \times Y_{NN}$$ 其中X是一个酉矩阵 (Unitary Matrix)，Y则是一个酉矩阵的共轭矩阵 与其共轭矩阵转置相乘等于单位阵的矩阵是酉矩阵，因此酉矩阵及其共轭矩阵都是方阵 B是一个对角阵，即只有对角线上是非0值 维基百科上给出了下面的例子： $$A_{4 \times 5}=\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0 &amp; 2 \\0 &amp; 0 &amp; 3 &amp; 0 &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\0 &amp; 4 &amp; 0 &amp; 0 &amp; 0\end{bmatrix}\quadX_{4 \times 5}=\begin{bmatrix}0 &amp; 0 &amp; 1 &amp; 0 \\0 &amp; 1 &amp; 0 &amp; 0 \\0 &amp; 0 &amp; 0 &amp; -1 \\1 &amp; 0 &amp; 0 &amp; 0\end{bmatrix}\quadB_{4 \times 5}=\begin{bmatrix}4 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\0 &amp; 3 &amp; 0 &amp; 0 &amp; 0 \\0 &amp; 0 &amp; \sqrt{5} &amp; 0 &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\end{bmatrix}\quadY_{5 \times 5}=\begin{bmatrix}0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\sqrt{0.2} &amp; 0 &amp; 0 &amp; 0 &amp; \sqrt{0.8} \\-\sqrt{0.8} &amp; 0 &amp; 0 &amp; 0 &amp; \sqrt{0.2}\end{bmatrix}$$ 那么如何进行奇异值分解呢？ 一般分两步进行： （1）将矩阵A变换成一个双对角矩阵（除了两行对角线元素非零，剩下的都是零），这个过程的计算量为$O(MN^2)$，如果矩阵是稀疏的，那么可以大大缩短计算时间； （2）将双对角矩阵变成奇异值分解的三个矩阵。这一步的计算量只是第一步的零头； 奇异值分解的一个重要目的是进行数据的低维度表示，即将$A_{MN}=X_{MM} \times B_{MN} \times Y_{NN}$ 转换为 $A_{MN}=X_{Mn} \times B_{nn} \times Y_{nN}\quad(n\le min\{M,N\})$ 直观一点的： 降维表示： 如何从原始的矩阵分解结果得到它的降维表示呢？为什么可以这么做？ 由于对角矩阵B的对角线上的元素的很多值相对于其他的值非常小，或者干脆为0，故而可以省略，例如，当B为如下情况时： $$B_{4 \times 5}=\begin{bmatrix}4 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 3 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; \sqrt{5} &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \end{bmatrix}$$ 则可以对B进行简化，省略都是0的行和列，得到B’： $$B’_{3 \times 3}=\begin{bmatrix}4 &amp; 0 &amp; 0 \\ 0 &amp; 3 &amp; 0 \\ 0 &amp; 0 &amp; \sqrt{5} \end{bmatrix}$$ 那么想对应地，X保留前n列，Y保留前n行 1.2. 如何求出SVD分解后的U,Σ,V这三个矩阵？对于下面表示形式的奇异值分解 $$A=U \Sigma V^T$$ 如果我们将A的转置和A做矩阵乘法，那么会得到n×n的一个方阵$A^TA$。既然$A^TA$是方阵，那么我们就可以进行特征分解，得到的特征值和特征向量满足下式 $$(A^TA)v_i = \lambda_i v_i$$ 这样我们就可以得到矩阵$A^TA$的n个特征值和对应的n个特征向量v了。将ATA的所有特征向量张成一个n×n的矩阵V，就是我们SVD公式里面的$V$矩阵了。一般我们将V中的每个特征向量叫做A的右奇异向量 如果我们将A和A的转置做矩阵乘法，那么会得到m×m的一个方阵$AA^T$。既然$AA^T$是方阵，那么我们就可以进行特征分解，得到的特征值和特征向量满足下式： $$(AA^T)u_i = \lambda_i u_i$$ 这样我们就可以得到矩阵$AA^T$的m个特征值和对应的m个特征向量u了。将AAT的所有特征向量张成一个m×m的矩阵U，就是我们SVD公式里面的$U$矩阵了。一般我们将U中的每个特征向量叫做A的左奇异向量 U和V我们都求出来了，现在就剩下奇异值矩阵$\Sigma$没有求出了 由于$\Sigma$除了对角线上是奇异值其他位置都是0，那我们只需要求出每个奇异值$\sigma$就可以了 由于 $$A=U\Sigma V^T \Rightarrow AV=U\Sigma V^TV \Rightarrow AV=U\Sigma \Rightarrow Av_i = \sigma_i u_i \Rightarrow \sigma_i = Av_i / u_i$$ 这样我们可以求出我们的每个奇异值，进而求出奇异值矩阵$\Sigma$ 那么为什么说$A^TA$的特征向量组成的就是我们SVD中的$V$矩阵，而$AA^T$的特征向量组成的就是我们SVD中的$U$矩阵？ 以$V$矩阵的证明为例 $$A=U\Sigma V^T \Rightarrow A^T=V\Sigma^T U^T \Rightarrow A^TA = V\Sigma^T U^TU\Sigma V^T = V\Sigma^2V^T$$ 上式证明使用了：$U^TU=I$, $\Sigma^T\Sigma=\Sigma^2$ 可以看出$A^TA$的特征向量组成的的确就是我们SVD中的$V$矩阵。类似的方法可以得到$AA^T$的特征向量组成的就是我们SVD中的$U$矩阵 进一步我们还可以看出我们的特征值矩阵等于奇异值矩阵的平方，也就是说特征值和奇异值满足如下关系： $$\sigma_i = \sqrt{\lambda_i}$$ 这样就可以不用 $\sigma_i=Av_i/u_i$ 来计算奇异值，也可以通过求出$A^TA$的特征值取平方根来求奇异值 1.3. SVD的一些性质对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例 也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵 $$A_{m \times n} = U_{m \times m}\Sigma_{m \times n} V^T_{n \times n} \approx U_{m \times k}\Sigma_{k \times k} V^T_{k \times n}$$ 由于这个重要的性质，SVD可以用于 PCA降维，来做数据压缩和去噪； 推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐； NLP中的算法，比如潜在语义索引（LSI） 1.4. 理解SVD分解所得三个矩阵的含义用一个打矩阵来表示成千上万篇文章和几十上百万个词的关联性 在这个矩阵中，每行表示一个词，每列表示一篇文章，如果有M篇文章，N个词，则可以得到一个MxN的矩阵： $$A=\begin{bmatrix}a_{11} &amp; a_{12} &amp; … &amp; a_{1M} \\a_{21} &amp; a_{22} &amp; … &amp; a_{2M} \\… &amp; … \\a_{N1} &amp; a_{N2} &amp; … &amp; a_{NM} \\\end{bmatrix}$$ 其中$a_{ij}$表示的是第j篇文章的第i个词的加权词频（比如用词的TF-IDF）。一般来说这个矩阵会非常非常大 对A进行奇异值分解后： 原始矩阵A的元素个数为$M \times N$，奇异值分解后得到的上小矩阵的元素总是为$M \times n + n \times n + n \times N = n(M+N+n)$，一般情况下$M \times N \gg n(M+N+n)$，这使得数据的存储量和计算量都远小于原始矩阵 这三个矩阵都有非常清晰的物理含义： 矩阵X 矩阵X是对词进行分类的结果，它的每一行表示一个词，每一列表示一个语义相近的词类，或者称为语义类 这一行的每个非零元素表示这个词在每个语义类的重要性（或者说是相关性），例如： $$X=\begin{bmatrix}0.7 &amp; 0.15 \\ 0.22 &amp; 0.49 \\ 0 &amp; 0.92\end{bmatrix}$$ 则第一个词与第一个词类最相关，而与第二个此类的关系比较弱，以此类推 矩阵Y 矩阵Y是对文本的分类结果，它的每一列对应一篇文章，每一行对应一个文章主题 这一列的每个非零元素表示这篇文章在每个的主题重要性（或者说是相关性），例如： $$Y=\begin{bmatrix}0.7 &amp; 0.15 &amp; 0.22 &amp; 0.39 \\ 0 &amp; 0.92 &amp; 0.08 &amp; 0.53\end{bmatrix}$$ 则第一篇文章很明显属于第一个主题，第二篇文章和第二个主题很相关，以此类推 矩阵B 中间的矩阵则表示词的类和文章的类之间的相关性，例如 $$B=\begin{bmatrix}0.7 &amp; 0.21 \\ 0.18 &amp; 0.63 \end{bmatrix}$$ 则第一个词的语义类与第一个主题相关，而第二个主题相关性较弱，而第二个词的语义类则相反 1.5. SVD用于PCA降维用PCA降维，需要找到样本协方差矩阵$X^TX$的最大的d个特征向量，然后用这最大的d个特征向量张成的矩阵来做低维投影降维。可以看出，在这个过程中需要先求出协方差矩阵$X^TX$，当样本数多样本特征数也多的时候，这个计算量是很大的 回顾上面SVD的计算过程，我们可以发现：求$X^TX$的d个最大的特征值对应的特征向量张成的矩阵，其实相当于对$X$进行奇异值分解得到右奇异矩阵$V$ SVD有个好处，有一些SVD的实现算法可以不求先求出协方差矩阵$X^TX$，也能求出我们的右奇异矩$V$。也就是说，我们的PCA算法可以不用做特征分解，而是做SVD来完成，这个方法在样本量很大的时候很有效 实际上，scikit-learn的PCA算法的背后真正的实现就是用的SVD，而不是我们我们认为的暴力特征分解 另一方面，注意到PCA仅仅使用了我们SVD的右奇异矩阵，没有使用左奇异矩阵，那么左奇异矩阵有什么用呢？ 假设我们的样本是m×n的矩阵X，如果我们通过SVD找到了矩阵$XX^T$最大的d个特征向量张成的m×d维矩阵$U$，则我们如果进行如下处理： $$X_{d \times n}’ = U_{d \times m}^TX_{m \times n}$$ 可以得到一个d×n的矩阵$X’$，这个矩阵和我们原来的m×n维样本矩阵X相比，行数从m减到了d，可见对行数进行了压缩。也就是说，左奇异矩阵可以用于行数的压缩。相对的，右奇异矩阵可以用于列数即特征维度的压缩，也就是我们的PCA降维。 2. 关键词权重的科学度量TF-IDF以短语“原子能的应用”为例，可以拆分成三个关键词：“原子能”、“的”和“应用” 主要思想：词出现次数较多的网页应该比它们出现较少的网页相关性高 缺点一：篇幅长度的影响 解决方案：根据篇幅长度，对关键词次数进行归一化，即$TF_c=\frac{n_c}{N}$，称为关键词的“单文本词频” (Term Frequency) 此时，要度量网页与查询之间的相关性，一个简单直接的方法就是：直接使用各个关键词在网页中出现的总词频 若查询包含N个关键词$w_1,w_2,…,w_N$，它们在某个特定网页中的词频分别是$TF_1,TF_2,…,TF_N$，则这个网页的与该查询之间的相关性为： $$TF_1+TF_2+…+TF_N$$ 缺点二：“停止词”的干扰 解决方案：在度量相关性时，不考虑这些词的频率 缺点三：没有考虑不同关键词的信息量。例如，“应用”是个通用的词，而“原子能”是个很专业的词，后者在相关性评估中应该比前者更重要 解决方案：对每个关键词施加一个权重，这个权重的设定必须满足： 预测主题的能力强，则权重大，否则，权重小； 停止词权重为0——不需要对第二个缺点做特殊的处理，在这里就顺带解决了第二个问题； 这样查询与某个网页之间的相关性就变成了： $$TF_1·IDF_1+TF_2·IDF_2+…+TF_N·IDF_N$$ 其中，$IDF_i$是第i个关键词对应的权重 那么具体该如何得到$IDF_i$呢？ 基于这样的常识：如果一个关键词只在很少的网页中出现，通过它就容易锁定搜索目标，它的权重就应该大；反之，如果一个词在大量的网页中都出现，看到它仍然难以确定要找什么内容，那么它的权重应该小 因此，假定一个关键词$w$在$D_w$个网页中出现过，那么$D_w$越小，$w$的权重就越大 在信息检索中，使用最多的权重是“逆文本频率指数” (Inverse Document Frequency, IDF) $$IDF_w=log(\frac{D}{D_w})$$ 3. 推荐系统的奥秘3.1. 基于用户数据：协同过滤算法归功于亚马逊工程师的“发明”——“一个客户买了这个东西，那么他也可能买另一个东西” 基本思想： 喜好相同的人和人之间有相似的消费/行为模式。喜好这个东西的人，倾向于也喜好另一个 实现的方法为“协同过滤”算法 (collaborative filtering) 下面以音乐推荐系统为例进行说明，基于对用户历史数据的不同侧重，可以分为以下两类应用情景： （1）基于用户：对每一个用户的听歌偏好作为向量，计算用户喜好之间的相似度，找到与某个用户X喜好最相似的一个其他用户Y，然后将Y的歌单里有而X没有的歌推荐给X （2）基于项目（单曲）：将用户对于一首歌的偏好作为向量，计算单曲之间的相似度，若某个用户喜欢/收藏了某一首歌，则将于这首歌相似的歌曲推荐给这个用户 但是，基于单一协同过滤算法的推荐系统会存在明显的误差： 除了用户及消费模式信息，不涉及被推荐单曲本身的任何信息 这使得热门音乐币冷门音乐更容易得到推荐，因为前者拥有更多数据 如果推荐系统只能给出热门歌曲的推荐，往往很难让人感到惊喜 而基于项目（单曲）的协同过滤，也有一个问题，就是相似使用模式下的内容异质。 例如你听了一张新专辑里面全部的歌，但除了主打歌，其他的一些插曲、翻唱曲以及混音曲可能都不是歌手的典型作品，那么协同过滤在这个时候，就会因为这些「噪音」而产生偏差。 最大的问题便是“没有数据，一切皆失效” 3.2. 基于内容：摆脱协同过滤算法对用户数据的过分依赖在数据量庞大且足够干净的时候，协同过滤算法是非常强大的，但如果作为一个新用户，在数据稀少的情况下，推荐系统该怎么获知我的口味？ 可以利用歌曲本身的信息来得到推荐结果，其基本思想是： 当你喜欢一首歌时，你会倾向也喜欢同类型的其他歌曲 不同歌曲有很多不同的属性，用一个向量去描述该单曲的属性，每一个维度的值代表一个属性的定量描述 按照这些属性，可以计算两首歌曲的相似度 基于内容的推荐算法更像是对协同过滤算法以上缺陷的一种补充——假如没有大量用户数据，或者想听冷门歌曲，我们就只能从音乐本身寻找答案了 前面提到，可以根据歌曲的不同维度的属性去构造一个特征向量去描述它，但是可供选择的属性实在是太多了，因此需要构造的特征向量维度过大——可以利用深度学习建立基于音频的推荐模型，通过特征的embedding和降维方法，把这么多特征映射到低维变量空间里 3.3. 相似度到底是怎么算出来的？可以拥有描述相似度的统计量为：欧式距离和余弦相似度 可以看出，在上图中，如果固定B，让A沿着直线OA方向移动，在移动过程中，AB的余弦夹角始终保持不变，而两点之间的绝对距离一直在变化 这种差异使得在使用它们进行相似度描述时，要考虑数据的特性： （1）欧式距离：能够突出数值绝对差异，在欧式距离下，用户对歌曲的偏好都可以被认为是一样的分数，可以简化歌曲相似度的计算； （2）余弦相似度：更多是从用户偏好方向上区分差异 4. 用余弦相似度进行文本分类 构造文本特征向量 在 《2. 关键词权重的科学度量TF-IDF》 我们已经讨论过用什么样的特征能够比较好地描述文本的特征信息 因此，这里我们使用TF-IDF来构造文本特征向量：对于一篇文本中的所有实词，计算它们的TF-IDF值 比如，比如词汇表有64000个词，其编号和词如下表所示： $$\begin{array}{c|c}\hline编号 &amp; 实词 \\ \hline 1 &amp; 阿 \\ 2 &amp; 啊 \\ 3 &amp; 阿斗 \\ … &amp; … \\ \hline \end{array}$$ 对于某一篇文本，这64000个词的TF-IDF值如下： $$\begin{array}{c|c}\hline编号 &amp; TF-IDF \\ \hline 1 &amp; 0 \\ 2 &amp; 0.0034 \\ 3 &amp; 0 \\ … &amp; … \\ \hline \end{array}$$ 基于余弦相似度进行文本分类的两种情况 （1）我们已经知道一些新闻类别的特征向量$x_1,x_2,…,x_n$ 对任何一个要被分类的文本Y，计算出它与每个特征向量的余弦相似性，基于最近邻算法（Nearest Neighbor）Algorithm），将它归类于于它最相似的那个类别里 （2）事先并不知道这些文本类别的特征向量 可以先采用层次聚类得到模糊的类别标签，至于聚类到什么程度停止，需要根据实际情况人为地控制——这样就得到了一个个小类，可以计算出每个小类的特征向量 这样问题就变成了第一种分类情况了 5. 加密算法5.1. 对称性加密与非对称性加密假设一个形象理解的场景： 考试时，超模君通过小天给学渣表妹传递答案 对称性加密 非对称性加密 加密和解密规则： 考试前超模君找到表妹，给了她一张纸片，纸片上有20行数，每行有4个数字，4个数字为乱序的1、2、3、4（如下图所示） 超模君考试时传递的小纸条中第一个数字X1表示纸片中X1行里的第x1个数字，第二个数字X2开始表示下一行中的第X2个数。例如，超模君的纸条上数字为2123，那么根据上面的纸片从第二行开始找数字，得到答案2、4、2、2（下图所示） 非对称性加密与对称性加密最大的区别就在于非对称性加密拥有两把钥匙，分别为私匙和公匙，其中只有公匙会传播出去，而私匙只会在自己手中，不会传播到外界 5.2. RSA加密算法5.2.1. 加密解密原理RSA算法是1977年由三位麻省理工学院教授——罗纳德·李维斯特（Ron Rivest）、阿迪·萨莫尔（Adi Shamir）和伦纳德·阿德曼（Leonard Adleman）一起提出。RSA就是他们三人姓氏开头字母拼在一起组成的 RSA算法过程： 如何得到公匙$e$和匙$d$ (1) 找出两个质数，一个是$p$，另一个是$q$ (2) 做一个乘法：$n=p\times q$ (3) 取一个函数：$\psi(n)=(p-1)\times(q-1)$ (4) 找出公匙$e$和匙$d$ $$\begin{cases}1&lt;e&lt;\psi(n) \\ e和\psi(n)需要互质 \\ e·d除以\psi(n)余数为1 \end{cases}$$ 如何加密和解密 假设传输的信息的数字为$m$，接下来我们就可以得到加密公式： $$c=m^e \% n$$ 其中$\%$为取余运算 c就是我们发送出去的加密后的数字 再来看看解密的公式： $$m=c^d\%n$$ 这个余数就是我们传输的信息——m 举个例子： (1) 找出两个质数，一个是7，另一个是13 (2) $n=p\times q=7 \times 13=91$ (3) $\psi(n)=(p-1)\times(q-1)=6 \times 12=72$ (4) 找出公匙$e$和匙$d$：$\begin{cases}e=5 \\ d=29\end{cases}$ (5) 假设我们要加密的数字是$m=4$ (6) 加密：$m^e \div n=4^5 \div 91…23$ (7) 解密：$c^d \div n=23^29 \div 91…4$ 5.2.2. 安全性分析为什么RSA算法是安全的（目前来说）？ 在传输的过程中，$e$（公匙）、$n$（质数乘积）、$c$（余数）是可以被黑客窃听到的，但参考上面加密公式可以知道 $d$（私匙）和 $\psi(n)$ 没有参与加密过程，所以窃听者并不知道$d$和$\psi(n)$ 那么窃听者能不能通过e、n、c算出私匙d呢？ $$\begin{cases}(e · d)\,\%\,\psi(n)=1 &amp; (1)\\\psi(n) = (p-1)(q-1) &amp; (2)\\n=p·q &amp; (3)\end{cases}$$ 看看用这3个公式黑客能不能算出私匙d： 假设黑客已经监听到了： $$e=5,\quad c=23,\quad n=p·q=91$$ 根据公式(1)知： $$5·d\,\%\,\psi(n)=1$$ 要想求出$d$就得知道$\psi(n)$，而$\psi(n) = (p-1)(q-1)$ 因此要想知道$\psi(n)$，就得知道$p$和$q$，而黑客已经知道了 $$n=p·q=91$$ 所以只需要对$n$进行质因式分解，可以得到 $$p=7,\quad q=13$$ 从上面的分析中可以看出，破解私匙的关键的一步是对$n$进行因式分解，虽然上面举的例子中的因式分解很简单，分分钟就能被破解，但是在实际使用中的$n$是一个很大的数——1024位的二进制数字，换算成十进制约为308位 目前还没有公式可以对这么大的一个数进行质因式分解，想硬解就需要用穷举法一个个的试出p、q 那么，用普通计算机进行穷举需要花费多久的时间呢？答案是整整一年 参考资料： (1) 吴军《数学之美（第二版）》 (2) 刘建平Pinard《奇异值分解(SVD)原理与在降维中的应用》 (3) 网易云音乐首次披露推荐算法: 让单身狗犹如过情人节的日推原来是这样生成的 (4) 超级数学建模《区区6位密码，凭什么守护我的百万家产？ 》]]></content>
      <categories>
        <category>MathIsFunGame</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习在生物信息学中的应用]]></title>
    <url>%2F2019%2F04%2F04%2FApplication-of-MachineLearning-in-Bioinformatics%2F</url>
    <content type="text"><![CDATA[1. 根据免疫组库TCRβ预测病人的CMV感染状态这个项目用到了641个样本（cohort 1），包括352 例CMV阴性(CMV-)和289例CMV阳性(CMV+) 外部验证用到了120个样本（cohort 2） 该机器学习的任务为： 讨论 TCRβ 免疫组的数据特点： 可能出现的TCRβ的集合非常大，而单个样本只能从中检测到稀疏的少数几个； 一个新样本中很可能会出现训练样本集合中未出现的TCRβ克隆类型； 对于一个给定的TCR，它对给定抗原肽的结合亲和力会受到HLA类型的调控，因此原始的用于判别分析的特征集合还受到隐变量——HLA类型的影响； 特征选择：鉴定表型相关的TCRβs 使用Fisher精确检验（单尾检验，具体实现过程请查看文末 *2.1. Fisher检验筛选CMV阳性（CMV+）相关克隆）： Fisher检验的阈值设为：$P&lt;1\times 10^{-4}$，FDR&lt;0.14（该FDR的计算方法见文末 *3. FDR的计算方法，且阈值的选择问题本质上是一个模型选择问题 (model selection) ，该问题会在这部分靠后的位置进行讨论），且富集在CMV+样本中，从而得到与CMV+相关的CDR3克隆集合，共有164个 通过下面的TCRβ克隆在两组中的发生率的散点图可以明显地看到，筛出的表型相关的TCRβ克隆的确显著地表达在CMV+组中 计算表型负荷（phenotype burden） 一个样本的表型负荷（phenotype burden）被定义为： 该样本的所有unique TCRβs中与阳性表型相关的克隆的数量所占的比例 若阳性表型相关的克隆的集合记为CDR，样本i的unique克隆集合记为CDRi，则它的表型负荷为： $$PB_i=\frac{||CDR_i \cap CDR||}{||CDR_i||}$$ 其中$||·||$表示集合·中元素的数量 下图是将上面的表型负荷计算公式中的分子与分母分别作为纵轴和横轴，画成二维的散点图 可以明显地看出两类样本在这个层面上来看，有很好的区分度 基于二项分布的贝叶斯判别模型 基本思想： 对于$CMV^+$相关TCR数为$k’$，total unique TCR数为$n’$的样本，认为它一个概率为它的表型负荷$\theta’$（$\theta’$服从Beta分布），$n=n’$, $k=k’$的二项分布（伯努利分布），根据贝叶斯思想，构造最优贝叶斯分类器，即 $$h(k’,n’)=arg \max_{x \in \{+,\,-\}} p(c=x \mid k’,\,n’)$$ 其中 $$p(c=x \mid k’,\,n’)=\frac{p(k’ \mid n’,\,c)p(c)}{p(k’)}$$ 而$p(k’)$是一个常数，对分类器的结果没有影响，可以省略 那么就需要根据训练集估计： 类先验概率$p(c)$ 类条件概率（似然）$p(k’ \mid n’,\,c)$ （1）首先根据概率图模型推出单个样本的概率表示公式 概率图模型如下： 则对于j样本，我们可以算出它的 $\theta_i$ 的后验分布： $$p(\theta_i \mid y_{ij},\,\alpha_i,\,\beta_i)=\frac{p(\theta_i \mid \alpha_i,\,\beta_i)p(y_{ij} \mid \theta_i)}{p(y_{ij})}$$ 其中， $p(y_{ij})$：表示事件$(k_{ij} \mid n_{ij})$发生的概率，即$p(k_{ij}\mid n_{ij},\,c_i)$ $p(y_{ij} \mid \theta_i)$：表示$Binomial(k_{ij} \mid n_{ij},\,\theta_i)=\left(\begin{matrix}n_{ij}\ k_{ij}\end{matrix}\right)\theta_i^{k_{ij}}(1-\theta_i)^{n_{ij}-k_{}ij}$ $p(\theta_i \mid \alpha_i,\,\beta_i)$：表示$\theta_i$的先验分布$Beta(\theta_i\mid\alpha_i,\,\beta_i)$ 对上面的公式进一步推导 $$\begin{aligned}&amp;p(\theta_i \mid y_{ij},\,\alpha_i,\,\beta_i) \\&amp;=\frac{p(\theta_i \mid \alpha_i,\,\beta_i)p(y_{ij} \mid \theta_i)}{p(y_{ij})}\\&amp;=\frac{1}{p(y_{ij})}Beta(\theta_i \mid \alpha_i,\,\beta_i)Binomial(k_{ij}\mid n_{ij},\,\theta_i) \\&amp;=\frac{1}{p(y_{ij})} \quad {\frac{1}{B(\alpha_i,\,\beta_i)}\theta_i^{\alpha_i-1}(1-\theta_i)^{\beta_i-1}} \quad {\left(\begin{matrix}n_{ij}\\ k_{ij}\end{matrix}\right)\theta_i^{k_{ij}}(1-\theta_i)^{n_{ij}-k_{ij}}} \\&amp;=\frac{1}{p(y_{ij})} \quad \frac{1}{B(\alpha_i,\,\beta_i)} \left(\begin{matrix}n_{ij}\\ k_{ij}\end{matrix}\right) \theta_i^{(\alpha_i+k_{ij})-1}(1-\theta_i)^{(\beta_i+n_{ij}-k_{ij})-1} \\&amp;=\frac{1}{p(y_{ij})} \quad \frac{B(\alpha_i+k_{ij},\,\beta_i+n_{ij}-k_{ij})}{B(\alpha_i,\,\beta_i)} \left(\begin{matrix}n_{ij}\\ k_{ij}\end{matrix}\right) \frac{1}{B(\alpha_i+k_{ij},\,\beta_i+n_{ij}-k_{ij})} \theta_i^{(\alpha_i+k_{ij})-1}(1-\theta_i)^{(\beta_i+n_{ij}-k_{ij})-1} \\&amp;=\frac{1}{p(y_{ij})} \quad \frac{B(\alpha_i+k_{ij},\,\beta_i+n_{ij}-k_{ij})}{B(\alpha_i,\,\beta_i)} \left(\begin{matrix}n_{ij}\\ k_{ij}\end{matrix}\right) \quad Beta(\alpha_i+k_{ij},\,\beta_i+n_{ij}-k_{ij})\end{aligned}$$ 根据Beta分布的先验分布的，已知 $$p(\theta_i \mid y_{ij},\,\alpha_i,\,\beta_i)=Beta(\alpha_i+k_{ij},\,\beta_i+n_{ij}-k_{ij})$$ 因此 $$p(k_{ij}\mid n_{ij},\,c_i)=p(y_{ij})=\left(\begin{matrix}n_{ij}\\ k_{ij}\end{matrix}\right) \frac{B(\alpha_i+k_{ij},\,\beta_i+n_{ij}-k_{ij})}{B(\alpha_i,\,\beta_i)} $$ 这样我们就得到单个样本的概率表示公式，其中$B(·)$是Beta函数，从上面的表达式中，我们可以看出$p(k_{ij}\mid n_{ij},\,c_i)$是$\alpha_i$和$\beta_i$的函数 （2）优化每个组的表型负荷 $\theta_i$ 的先验分布的两个参数 $\alpha_i$ 和 $\theta_i$——最大似然法 我们要最大化$c_i$组的样本集合它们的联合概率： $$\begin{aligned}p(k_i \mid n_i,\,c_i) &amp;=\prod_{j,j \in c_i} \left(\begin{matrix}n_{ij}\\ k_{ij}\end{matrix}\right) \frac{B(\alpha_i+k_{ij},\,\beta_i+n_{ij}-k_{ij})}{B(\alpha_i,\,\beta_i)}\\&amp;= \prod_{j,j \in c_i} \left(\begin{matrix}n_{ij}\\ k_{ij}\end{matrix}\right) \quad \prod_{j,j \in c_i} \frac{B(\alpha_i+k_{ij},\,\beta_i+n_{ij}-k_{ij})}{B(\alpha_i,\,\beta_i)}\end{aligned}$$ 其中，$\prod_{j,\,j \in c_i} \left(\begin{matrix}n_{ij}\\ k_{ij}\end{matrix}\right)$是常数，可以省略，则 $$p(k_i \mid n_i,\,c_i)=\prod_{j,j \in c_i} \frac{B(\alpha_i+k_{ij},\,\beta_i+n_{ij}-k_{ij})}{B(\alpha_i,\,\beta_i)}$$ 对它取对数，得到 $$l(\alpha_i,\,\beta_i)=\log p(k_i \mid n_i,\,c_i) \\ = \sum_{j,\,j \in c_i} \log B(\alpha_i+k_{ij}, \, \beta_i+n_{ij}-k_{ij})-N_i\log B(\alpha_i, \, \beta_i)$$ 其中，$N_i$是属于$c_i$组的样本数 因此优化目标为： $$(\alpha_i^*,\beta_i^*)=arg \, \max\limits_{\alpha_i,\,\beta_i}l(\alpha_i,\,\beta_i)$$ $l(\alpha_i,\,\beta_i)$分别对$\alpha_i$和$\beta_i$求偏导 $$\frac{\partial l}{\partial \alpha_i}=-N_i(\psi(\alpha_i)-\psi(\alpha_i+\beta_i))+\sum_{j,\,j \in c_i}(\psi(\alpha_i+k_{ij})-\psi(\alpha_i+\beta_i+n_{ij}))$$ $$\frac{\partial l}{\partial \beta_i}=-N_i(\psi(\beta_i)-\psi(\alpha_i+\beta_i))+\sum_{j,\,j \in c_i}(\psi(\beta_i+n_{ij}-k_{ij})-\psi(\alpha_i+\beta_i+n_{ij}))$$ 其中，$\psi(·)$是伽马函数 使用梯度上升（gradient ascent）法来求解优化目标，其中梯度的公式为： $$\alpha_i := \alpha_i+\alpha\frac{\partial l}{\partial \alpha_i}\\ \beta_i := \beta_i+\alpha\frac{\partial l}{\partial \beta_i}$$ 最终得到的解记为$\alpha_i^*$和$\beta_i^*$，其中 $$\alpha_+^*=4.0,\quad \beta_+^*=51,820.1\\\alpha_-^*=26.7,\quad \beta_-^*=2,814,963.8$$ （3）根据训练好的分类器对新样本进行分类 分类器为 $$\begin{aligned}h(k’,n’) &amp;= arg \max_{x \in \{CMV^+,\,CMV^-\}} p(c=x \mid n’,k’) \\&amp;=arg \max_{x \in \{CMV^+,\,CMV^-\}} p(k’ \mid n’, c=x)p(c=x) \\&amp;=arg \max_{x \in \{CMV^+,\,CMV^-\}}\left(\begin{matrix}n’ \\ k’\end{matrix}\right)\frac{B(\alpha_x^*+k’,\,\beta_x^*+n’-k’)}{B(\alpha_x^*,\,\beta_x^*)}\frac{N_x}{N}\end{aligned}$$ （4）选择合适的的阈值：model selection 使用交叉验证法中的留一法 (leave-one-out)来进行模型选择 定义该问题的优化目标为最小化cross-entropy loss： $$L(\phi)=-\frac{1}{N}\sum_{i=1}^N [c_i\log q_i(\phi)+(1-c_i)\log (1-q_i(\phi))]$$ 其中，$c_i$表示样本$i$的所属类别，$q_i(\phi)$表示样本$i$被判为$CMV^+$的概率，N为训练样本数（N个训练样本数的训练集它总共有N种留一法组合形式） 分析优化目标为什么选择最小化cross-entropy loss： 我们的目的是选择合适的阈值，那么什么叫合适的阈值？就是在该阈值下，能筛选出合适的features，基于这些features训练出的模型能有足够高的泛化性能，即模型的variance足够的小，也就是对于测试数据（这里就是留一法中的那一个样本）有足够高的预测准确性，定量化描述就是N次留一法的准确预测的概率的均值（几何平均数）最大化，即 $$\max_{\phi} \sqrt[N]{\prod_i^N q_i(\phi)^{c_i}(1-q_i(\phi)^{1-c_i}} \\ s.t.\quad c_i \in {0,\,1}$$ 由于取对数不影响优化方向，所以 $$\begin{aligned}&amp;\log \left[\prod_i^N q_i(\phi)^{c_i}(1-q_i(\phi)^{1-c_i} \right]^{1/N} \\ &amp;=\frac{1}{N}\log \prod_i^N q_i(\phi)^{c_i}(1-q_i(\phi)^{1-c_i} \\ &amp;=\frac{1}{N}\sum_{i=1}^N [c_i\log q_i(\phi)+(1-c_i)\log (1-q_i(\phi))] \end{aligned}$$ 从而得到上面的优化目标 最后得到的分类器的性能非常高 2. SC3：单细胞表达谱的无监督聚类该聚类方法名为SC3（Single-Cell Consensus Clustering） 该方法本质上就是K-means聚类，不过在执行K-means聚类的前后进行了一些特殊的操作： k-means聚类前：进行了数据预处理，即特征的构造，称为特征工程，该方法中是对输入的原始特征空间进行PCA变换或拉普拉斯矩阵变换，对变换后的新特征矩阵逐渐增加提取的主成分数，来构造一系列新特征； k-means聚类后：特征工程构造出来的一系列新特征集合，基于这些新特征集合通过k-means聚类能得到一系列不同的聚类结果，尝试对这些聚类结果总结出consensus clustering 本人比较好奇的地方是：怎么从一系列不同的聚类结果中总结出consensus clustering？ 使用CSPA算法（cluster-based similarity partitioning algorithm） （1）对每一个聚类结果按照以下方法构造二值相似度矩阵S：如果两个样本i和j在该聚类结果中被聚到同一个集合中，则它们之间的相似度为1，在二值相似度矩阵中对应的值 Si,j = 1，否则Si,j = 0； （2）对所有的聚类结果的二值相似度矩阵S取平均，得到consensus matrix； （3）基于consensus matrix进行层次聚类，得到最终的consensus clustering； 3. GATK的VQSR补充知识*1. beta分布*1.1. 什么是beta分布对于硬币或者骰子这样的简单实验，我们事先能很准确地掌握系统成功的概率。 然而通常情况下，系统成功的概率是未知的，但是根据频率学派的观点，我们可以通过频率来估计概率。为了测试系统的成功概率，我们做n次试验，统计成功的次数k，于是很直观地就可以计算出。然而由于系统成功的概率是未知的，这个公式计算出的只是系统成功概率的最佳估计。也就是说实际上也可能为其它的值，只是为其它的值的概率较小。因此我们并不能完全确定硬币出现正面的概率就是该值，所以也是一个随机变量，它符合Beta分布，其取值范围为0到1 用一句话来说，beta分布可以看作一个概率的概率密度分布，当你不知道一个东西的具体概率是多少时，它可以给出了所有概率出现的可能性大小。 Beta分布有和两个参数α和β，其中α为成功次数加1，β为失败次数加1。 *1.2. 理解beta分布举一个简单的例子，熟悉棒球运动的都知道有一个指标就是棒球击球率(batting average)，就是用一个运动员击中的球数除以击球的总数，我们一般认为0.266是正常水平的击球率，而如果击球率高达0.3就被认为是非常优秀的。 现在有一个棒球运动员，我们希望能够预测他在这一赛季中的棒球击球率是多少。你可能就会直接计算棒球击球率，用击中的数除以击球数，但是如果这个棒球运动员只打了一次，而且还命中了，那么他就击球率就是100%了，这显然是不合理的，因为根据棒球的历史信息，我们知道这个击球率应该是0.215到0.36之间才对啊。 对于这个问题，一个最好的方法来表示这些经验（在统计中称为先验信息）就是用beta分布，这表示在我们没有看到这个运动员打球之前，我们就有了一个大概的范围。beta分布的定义域是(0,1)这就跟概率的范围是一样的。s接下来我们将这些先验信息转换为beta分布的参数，我们知道一个击球率应该是平均0.27左右，而他的范围是0.21到0.35，那么根据这个信息，我们可以取α=81,β=219 之所以取这两个参数是因为： beta分布的均值 这个分布主要落在了(0.2,0.35)间，这是从经验中得出的合理的范围 Beta(81, 219) 在这个例子里，我们的x轴就表示各个击球率的取值，x对应的y值就是这个击球率所对应的概率密度。也就是说beta分布可以看作一个概率的概率密度分布。 那么有了先验信息后，现在我们考虑一个运动员只打一次球，那么他现在的数据就是“1中；1击”。这时候我们就可以更新我们的分布了，让这个曲线做一些移动去适应我们的新信息，移动的方法很简单 其中α0和β0是一开始的参数，在这里是81和219。所以在这一例子里，α增加了1（击中了一次）。β没有增加(没有漏球)。这就是我们的新的beta分布 Beta(81+1,219)，我们跟原来的比较一下： 可以看到这个分布其实没多大变化，这是因为只打了1次球并不能说明什么问题。但是如果我们得到了更多的数据，假设一共打了300次，其中击中了100次，200次没击中，那么这一新分布就是 Beta(81+100,219+200) ： 注意到这个曲线变得更加尖，并且平移到了一个右边的位置，表示比平均水平要高 有趣的现象： 根据这个新的beta分布，我们可以得出他的数学期望为：αα+β=82+10082+100+219+200=.303 ，这一结果要比直接的估计要小 100100+200=.333 。你可能已经意识到，我们事实上就是在这个运动员在击球之前可以理解为他已经成功了81次，失败了219次这样一个先验信息。 *2. 重复NG免疫组库TCRβ文章的图*2.1. Fisher检验筛选CMV阳性（CMV+）相关克隆目的是得到类似下面的结果： 提供的输入： Observed matrix：行为CDR3克隆，列为样本，矩阵中的值只能取0或1，0表示该feature在该样本中未被观测到，1为被观测到了 样本的分组信息文件（该文件非必须，若样本名中包含分组信息，则可以不提供这个文件，只需提供分从样本名中提取分组信息的正则表达式） 当前感兴趣的两个组，因为考虑到可能提供的样本是多组的，而Fisher检验只能进行两组间的比较，所以多组时需要指定分析的两组是谁 实现的思路： 原始提供的输入文件是profile matrix，设定一个阈值（默认为0），将大于阈值的设为1，等于阈值的设为0，从而得到Observed matrix。这一步操作不在下面脚本的功能当中，需要自己在执行下面的脚本之前完成这个操作； 将准备好的Observed matrix输入，行为TCR克隆（feature），列为sample，因为是要对TCR克隆分析它与分组的相关性，因此每次对矩阵的行执行Fisher检验，根据提供的样本的分组信息，得到该TCR克隆的2X2列联表 $$\begin{matrix}\hline&amp; Group \\ Observe &amp; A &amp; B \\ \hline Yes &amp; a &amp; b \\ \hline No &amp; c &amp; d \\ \hline \end{matrix}$$ 然后再对这个列联表计算Fisher检验的p-value，将TCR克隆，列联表中的a，b和算出的p-value输出，就得到了我们想要的结果 脚本名：FisherTestForMatrix.R 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141##################################################################### 该脚本用于对样本的Oberved矩阵执行Fisher精确检验（单尾），用于 ## 检测feature与分组的相关性 ###################################################################### 参数说明：# - （1）样本的Oberved矩阵，行为特征，列为样本，矩阵中的值只能取0# 或1，0表示该feature在该样本中未被观测到，1为被观测到了## - （2）是否提供样本的分组信息文件，1为是，0位否## - （3）若上一个参数选择1（提供分组信息文件），则该参数应该设为# 分组信息文件的路径，分组文件要求至少包含两列——SampleId和Group；# 若上一个参数选择0（不提供分组信息文件），则认为分组信息已经包# 含的样本的命名中，则可以通过提供分组信息在样本名中匹配的正则# 表达式## - （4）感兴趣的两组，例如是A组和B组，则写成&quot;A-B&quot;，即中间用连字# 符连接## - （5）设置的p值阈值## - （6）设置并行化线程数，若不设置这个参数，则默认不采用并行化计# 算方法library(stringr)library(ggplot2)library(parallel)Args &lt;- commandArgs(TRUE)MatFile &lt;- Args[1]Bool_GroupFile &lt;- Args[2]GroupFile &lt;- Args[3]TargetGroups &lt;- Args[4]pvalue &lt;- Args[5]# 开启并行化if(!is.na(Args[6]))&#123; no_cores &lt;- as.integer(Args[6]) cl &lt;- makeCluster(no_cores)&#125;Matrix &lt;- read.table(MatFile, header=T, row.names=1)TargetGroups &lt;- unlist(strsplit(TargetGroups, &apos;-&apos;))print(&quot;Loading Observed Matrix successfully&quot;)################################################## 1. 获得Oberved矩阵样本对应的分组信息#################################################if(Bool_GroupFile %in% c(1, 0))&#123; # 从提供的分组信息文件中获得 if(Bool_GroupFile==1)&#123; Group &lt;- read.table(GroupFile, header=T) # 考虑到可能存在Observe矩阵列名命名不规范，即以数字起始（会在开头 # 添加X字符），或其中包含连字符（将连字符替换为点）因此需要将 # Group变量中的SampleId进行相应的替换，以保证一致 colname_matrix &lt;- ifelse(grepl(&apos;^X&apos;,colnames(Matrix)),sub(&apos;^X&apos;,&apos;&apos;,colnames(Matrix)),colnames(Matrix)) colname_matrix &lt;- gsub(&apos;\\.&apos;,&apos;-&apos;,colname_matrix) SampleGroup &lt;- Group$Group[match(colname_matrix, Group$SampleId)] # 从样本名中用正则提取 &#125;else&#123; GroupPattern &lt;- GroupFile SampleGroup &lt;- unlist(str_extract(colnames(Matrix), GroupPattern)) &#125;&#125;else&#123; stop(&quot;参数指定错误！第二个参数必须为0或1&quot;)&#125;print(&quot;Load/get group info for coresponding samples in each col in Observed Matrix&quot;)################################################## 2. 对每个feature执行Fisher检验，得到检验结果################################################## 用于执行fisher检验的函数，最终返回的是TRUE或FALSEFisherTest &lt;- function(ObserveVec, GroupIndex, TargetGroups)&#123; # 初始化列联表 FisherMat &lt;- matrix(c(0,0,0,0), nrow = 2, dimnames = list(Observe=c(&apos;Yes&apos;,&apos;No&apos;), Group=TargetGroups )) # 为列联表的每一项填上对应的值 FisherMat[1,1] &lt;- sum(ObserveVec==1&amp;GroupIndex==colnames(FisherMat)[1]) FisherMat[1,2] &lt;- sum(ObserveVec==1&amp;GroupIndex==colnames(FisherMat)[2]) FisherMat[2,1] &lt;- sum(ObserveVec==0&amp;GroupIndex==colnames(FisherMat)[1]) FisherMat[2,2] &lt;- sum(ObserveVec==0&amp;GroupIndex==colnames(FisherMat)[2]) # 进行fisher检验，得到p值 pvalue &lt;- fisher.test(FisherMat, alternative = &quot;two.sided&quot;)$p.value # 返回向量：组1计数、组2计数、p值 c(FisherMat[1,1], FisherMat[1,2], pvalue)&#125;# 为每个feature（即矩阵的行）执行fisher检验，得到是每个feature的pvalue，每一个的返回值以列形式进行追加if(!is.na(Args[6]))&#123; StatOut &lt;- parApply(cl,Matrix, 1, FisherTest, SampleGroup, TargetGroups)&#125;else&#123; StatOut &lt;- apply(Matrix, 1, FisherTest, SampleGroup, TargetGroups)&#125;StatOut_Table &lt;- data.frame(Feature=colnames(StatOut),Group1=StatOut[1,],Group2=StatOut[2,],Pval=StatOut[3,])colnames(StatOut_Table) &lt;- c(&apos;Feature&apos;, paste(&quot;Group_&quot;,TargetGroups[1],sep=&apos;&apos;), paste(&quot;Group_&quot;,TargetGroups[2],sep=&apos;&apos;), &apos;Pval&apos;)print(&quot;Finish Fisher&apos;s Exact Test for each feature&quot;)# 将Fisher检验结果写入文件write.table(StatOut_Table, paste(str_extract(MatFile,&apos;^(.*?)\\.&apos;),TargetGroups[1],&quot;-&quot;,TargetGroups[2],&quot;.stat&quot;,sep=&apos;&apos;),row.names=F,col.names=T,sep=&quot;\t&quot;,quote=F)print(&quot;Finish writing Fisher&apos;s Exact Test Output into file&quot;)################################################## 3. 表型负荷（Phenotype Burden）相关的计算与画图################################################## 计算每个样本的表型负荷相关的两个值，该样本中与分组1相关的features数，以及该样本中观测到的features数if(!is.na(Args[6]))&#123; RelativeFeatures &lt;- parApply(cl, Matrix[,SampleGroup %in% TargetGroups], 2, function(x,y,p,z) sum(x==1&amp;y&lt;p&amp;z), StatOut_Table$Pval, pvalue, StatOut_Table[,2]&gt;StatOut_Table[,3])&#125;else&#123; RelativeFeatures &lt;- apply(Matrix[,SampleGroup %in% TargetGroups], 2, function(x,y,p,z) sum(x==1&amp;y&lt;p&amp;z), StatOut_Table$Pval, pvalue, StatOut_Table[,2]&gt;StatOut_Table[,3])&#125;stopCluster(cl)TotalFeatures &lt;- colSums(Matrix[,SampleGroup %in% TargetGroups])PhenotypeBurden &lt;- data.frame(Relative=RelativeFeatures, Total=TotalFeatures, Group=SampleGroup[SampleGroup %in% TargetGroups])print(&quot;Finish calculate statistics for Phenotype-Burden&quot;)# 画散点图print(&quot;Start dotplot&quot;)png(paste(&quot;PhenotypeBurdenDot_&quot;,pvalue,&quot;_&quot;,TargetGroups[1],&quot;-&quot;,TargetGroups[2],&quot;.png&quot;,sep=&apos;&apos;))ggplot(PhenotypeBurden)+geom_point(aes(x=Total, y=Relative, color=Group))dev.off()# 保存数据write.table(PhenotypeBurden,paste(str_extract(MatFile,&apos;^(.*?)\\.&apos;),pvalue,&quot;.&quot;, TargetGroups[1],&quot;-&quot;,TargetGroups[2],&quot;.data&quot;,sep=&apos;&apos;),row.names=T,col.names=T,sep=&quot;\t&quot;,quote=F) 用法： 1$ Rscript FisherTestForMatrix.R &lt;profile matrix&gt; &lt;1|0&gt; &lt;group info|regx&gt; &lt;2 interested group&gt; &lt;pval&gt; 具体的参数说明，请查看脚本的注释信息 *2.2. 绘制表型负荷相关散点图*2.2.1. 训练集在上一步 *2.1. Fisher检验筛选CMV阳性（CMV+）相关克隆 的操作中会同时得到这张图 画出的表型负荷相关散点图如下： *2.2.2. 测试集测试集的表型负荷相关散点图需要基于训练集的结果，需要执行另外的操作 需要提供的输入： 测试集的Observed matrix：行为CDR3克隆，列为样本，矩阵中的值只能取0或1，0表示该feature在该样本中未被观测到，1为被观测到了 样本的分组信息文件（该文件非必须，若样本名中包含分组信息，则可以不提供这个文件，只需提供分从样本名中提取分组信息的正则表达式） 当前感兴趣的两个组，因为考虑到可能提供的样本是多组的，而Fisher检验只能进行两组间的比较，所以多组时需要指定分析的两组是谁 训练集的Fisher检验输出结果 脚本名：PhenotypeBurden2TestCohort.R 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586#################################################### 该脚本用于对测试集的每个样本计算表型负荷相关 ## 的两个值：该样本中与分组1相关的features数，以 ## 及该样本中观测到的features数 ##################################################### 参数说明：# - （1）测试集Oberved矩阵，行为特征，列为样本，矩阵中的值只能取0# 或1，0表示该feature在该样本中未被观测到，1为被观测到了## - （2）是否提供样本的分组信息文件，1为是，0位否## - （3）若上一个参数选择1（提供分组信息文件），则该参数应该设为# 分组信息文件的路径，分组文件要求至少包含两列——SampleId和Group；# 若上一个参数选择0（不提供分组信息文件），则认为分组信息已经包# 含的样本的命名中，则可以通过提供分组信息在样本名中匹配的正则# 表达式## - （4）感兴趣的两组，例如是A组和B组，则写成&quot;A-B&quot;，即中间用连字# 符连接## - （5）设置的p值阈值## - （6）上一步对训练集执行Fisher检验的检验结果输出文件library(stringr)library(ggplot2)Args &lt;- commandArgs(TRUE)MatFile &lt;- Args[1]Bool_GroupFile &lt;- Args[2]GroupFile &lt;- Args[3]TargetGroups &lt;- Args[4]pvalue &lt;- Args[5]StatFile &lt;- Args[6]Matrix &lt;- read.table(MatFile, header=T, row.names=1)TargetGroups &lt;- unlist(strsplit(TargetGroups, &apos;-&apos;))StatOut &lt;- read.table(StatFile, header=T)print(&quot;Loading Observed Matrix and Fisher&apos;s Exact Test output successfully&quot;)################################################## 1. 获得Oberved矩阵样本对应的分组信息#################################################if(Bool_GroupFile %in% c(1, 0))&#123; # 从提供的分组信息文件中获得 if(Bool_GroupFile==1)&#123; Group &lt;- read.table(GroupFile, header=T) # 考虑到可能存在Observe矩阵列名命名不规范，即以数字起始（会在开头 # 添加X字符），或其中包含连字符（将连字符替换为点）因此需要将 # Group变量中的SampleId进行相应的替换，以保证一致 colname_matrix &lt;- ifelse(grepl(&apos;^X&apos;,colnames(Matrix)),sub(&apos;^X&apos;,&apos;&apos;,colnames(Matrix)),colnames(Matrix)) colname_matrix &lt;- gsub(&apos;\\.&apos;,&apos;-&apos;,colname_matrix) SampleGroup &lt;- Group$Group[match(colname_matrix, Group$SampleId)] # 从样本名中用正则提取 &#125;else&#123; GroupPattern &lt;- GroupFile SampleGroup &lt;- unlist(str_extract(colnames(Matrix), GroupPattern)) &#125;&#125;else&#123; stop(&quot;参数指定错误！第二个参数必须为0或1&quot;)&#125;################################################## 2. 表型负荷（Phenotype Burden）相关的计算与画图################################################## 计算每个样本的表型负荷相关的两个值，该样本中与分组1相关的features数，以及该样本中观测到的features数RelativeFeatures_list &lt;- StatOut$Feature[StatOut[,2]&gt;StatOut[,3]&amp;StatOut$Pval&lt;pvalue]RelativeFeatures &lt;- apply(Matrix[rownames(Matrix) %in% RelativeFeatures_list,SampleGroup %in% TargetGroups], 2, sum)TotalFeatures &lt;- colSums(Matrix[,SampleGroup %in% TargetGroups])PhenotypeBurden &lt;- data.frame(Relative=RelativeFeatures, Total=TotalFeatures, Group=SampleGroup[SampleGroup %in% TargetGroups])print(&quot;Finish calculate statistics for Phenotype-Burden&quot;)# 画散点图print(&quot;Start dotplot&quot;)png(paste(&quot;PhenotypeBurdenDot_&quot;,pvalue,&quot;_&quot;,TargetGroups[1],&quot;-&quot;,TargetGroups[2],&quot;.png&quot;,sep=&apos;&apos;))ggplot(PhenotypeBurden)+geom_point(aes(x=Total, y=Relative, color=Group))+ labs(title=&quot;CMV-associated vs. total-unique&quot;)dev.off()# 保存数据write.table(PhenotypeBurden,paste(str_extract(MatFile,&apos;^(.*?)\\.&apos;),pvalue,&quot;.&quot;, TargetGroups[1],&quot;-&quot;,TargetGroups[2],&quot;.data&quot;,sep=&apos;&apos;),row.names=T,col.names=T,sep=&quot;\t&quot;,quote=F) 用法： 1$ Rscript PhenotypeBurden2TestCohort.R &lt;profile matrix&gt; &lt;1|0&gt; &lt;group info|regx&gt; &lt;2 interested group&gt; &lt;pval&gt; &lt;stat&gt; *2.2. TCRβ在两组间分布偏好性的散点图就是画这幅图： 注意：这幅图的横纵坐标都进行了log10变换 在上一步 *2.1. Fisher检验筛选CMV阳性（CMV+）相关克隆 的操作中会同时得到这张图 画出的散点图如下： *3. FDR的计算方法*3.1. 回顾那些统计检验方法*3.1.1. T-test与Moderated t-Testt-test的统计量： $$t= \frac{\overline X_1(i)-\overline X_2(i)}{S(i)}$$ Moderated t-Test的统计量： $$d= \frac{\overline X_1(i)-\overline X_2(i)}{S(i)+S_0}$$ Moderated t-Test的统计量d与t-test的t的计算方法很相似，差别就在于分母中方差的计算方法， T1 T2 T3 C1 C2 C3 Gene XT,1 XT,2 XT,3 XC,1 XC,2 XC,3 由上面展示的该基因的实际样本分组，计算出方差$S(i)=S_{X_1X_2}\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}$ 然后随机打乱上面的样本分组，得到： T1 T2 T3 C1 C2 C3 Gene XC,2 XT,1 XC,3 XT,3 XT,2 XC,1 根据打乱的结果算出$S_0$，进行n次这样的随机打乱，计算得到$d_1,d_2,…,d_n$ 最后算出它的P值： $$P=\frac{\# \{d_i \ge d,i=1,2,…,n\}}{n} $$ 之所以不用t检验的统计量查表法，是因为Moderated t-Test的统计量已经不再符合某种统计分布了，而且这样算出来的P值也具有一定的统计意义 *3.2. 多重假设检验的必要性统计学中的假设检验的基本思路是： 设立零假设（null hypothesis）$H_0$，以及与零假设$H_0$相对应的非零假设（alternative hypothesis， or reject null hypothesis）$H_1$，在假设$H_0$成立的前提下，计算出$H_0$发生的概率，若$H_0$的发生概率很低，基于小概率事件几乎不可能发生，所以可以拒绝零假设 但是这些传统的假设检验方法研究的对象，都是一次试验 在一次试验中（注意：是一次试验， 即single test），0.05 或0.01的cutoff足够严格了(想象一下，一个口袋有100个球，95个白的，5个红的, 只让你摸一次，你能摸到红的可能性是多大？) 但是对于多次试验，又称多重假设检验，再使用p值是不恰当的，下面来分析一下为什么： 大家都知道墨菲定律：如果事情有变坏的可能，不管这种可能性有多小，它总会发生 用统计的语言去描述墨菲定律： 在数理统计中，有一条重要的统计规律：假设某意外事件在一次实验（活动）中发生的概率为p（p&gt;0），则在n次实验（活动）中至少有一次发生的概率为 $p_n=1-(1-p)^n$ 由此可见，无论概率p多么小（即小概率事件），当n越来越大时，$p_n$越来越接近1 这和我们的一句俗语非常吻合：常在河边走，哪有不湿鞋；夜路走多了，总能碰见鬼 在多重假设检验中，我们一般关注的不再是每一次假设检验的准确性，而是控制在作出的多个统计推断中犯错误的概率，即False Discover Rate（FDR），这对于医院的诊断情景下尤其重要： 假如有一种诊断艾滋病(AIDS)的试剂，试验验证其准确性为99%（每100次诊断就有一次false positive）。对于一个被检测的人（single test)）来说，这种准确性够了；但对于医院 （multiple test)）来说，这种准确性远远不够 因为每诊断10 000个个体，就会有100个人被误诊为艾滋病(AIDS)，每一个误诊就意味着一个潜在的医疗事故和医疗纠纷，对于一些大型医院，一两个月的某一项诊断的接诊数就能达到这个级别，如果按照这个误诊率，医院恐怕得关门，所以医院需要严格控制误诊的数量，宁可错杀一万也不能放过一个，因为把一个没病的病人误判为有病，总比把一个有病的病人误判为没病更保险一些 100 independent genes. (We have 100 hypotheses to test) No significant differences in gene expression between 2 classes (H0 is true). Thus, the probability that a particular test (say, for gene 1) is declared significant at level 0.05 is exactly 0.05. (Probability of reject H0 in one test if H0 is true = 0.05) However, the probability of declaring at least one of the 100 hypotheses false (i.e. rejecting at least one, or finding at least one result significant) is: $$1-(1-0.05)^{100}\approx 0.994$$ *3.3. 如何计算FDR？统计检验的混淆矩阵： $H_0$ is true $H_1$ is true Total Significant V S R Not Significant U T m-R Total m0 m-m0 m FWER (Family Wise Error Rate) 作出一个或多个假阳性判断的概率 $$FWER=Pr(V\ge 1)$$ 使用这种方法的统计学过程： The Bonferroni procedure Tukey’s procedure Holm’s step-down procedure FDR (False Discovery Rate) 在所有的单检验中作出假阳性判断比例的期望 $$FDR=E\left[\frac{V}{R}\right]$$ 使用这种方法的统计学过程： Benjamini–Hochberg procedure Benjamini–Hochberg–Yekutieli procedure *3.3.1. Benjamini-Hochberg procedure (BH)对于m个独立的假设检验，它们的P-value分别为：$p_i,i=1,2,…,m$ （1）按照升序的方法对这些P-value进行排序，得到： $$p_{(1)} \le p_{(2)} \le … \le p_{(m)}$$ （2）对于给定是统计显著性值$\alpha \in (0,1)$，找到最大的k，使得 $$p_{(k)} \le \frac{\alpha * k}{m}$$ （3）对于排序靠前的k个假设检验，认为它们是真阳性 (positive ) 即：$reject \, H_0^{(i)},\, 1 \le i \le k$ $$\begin{array}{c|l}\hlineGene &amp; p-value \\\hlineG1 &amp; P1 =0.053 \\\hlineG2 &amp; P2 =0.001 \\\hlineG3 &amp; P3 =0.045 \\\hlineG4 &amp; P4 =0.03 \\\hlineG5 &amp; P5 =0.02 \\\hlineG6 &amp; P6 =0.01 \\\hline\end{array}\, \Rightarrow \,\begin{array}{c|l}\hlineGene &amp; p-value \\\hlineG2 &amp; P(1) =0.001 \\\hlineG6 &amp; P(2) =0.01 \\\hlineG5 &amp; P(3) =0.02 \\\hlineG4 &amp; P(4) =0.03 \\\hlineG3 &amp; P(5) =0.045 \\\hlineG1 &amp; P(6) =0.053 \\\hline\end{array}$$ $$\alpha = 0.05$$ $P(4) =0.03&lt;0.05*\frac46=0.033$ $P(5) =0.045&gt;0.05*\frac56=0.041$ 因此最大的k为4，此时可以得出：在FDR&lt;0.05的情况下，G2，G6，G5 和 G4 存在差异表达 可以计算出q-value： $$p_{(k)} \le \frac{\alpha*k}{m} \, \Rightarrow \, \frac{p_{(k)}*m}{k} \le \alpha$$ Gene P q-value G2 P(1) =0.001 0.006 G6 P(2) =0.01 0.03 G5 P(3) =0.02 0.04 G4 P(4) =0.03 0.045 G3 P(5) =0.045 0.054 G1 P(6) =0.053 0.053 根据q-valuea的计算公式，我们可以很明显地看出： $$q^{(i)}=p_{(k)}^{(i)}*\frac{Total \, Gene \, Number}{rank(p^{(i)})}=p_{(k)}^{(i)}*\frac{m}{k}$$ 即，根据该基因p值的排序对它进行放大，越靠前放大的比例越大，越靠后放大的比例越小，排序最靠后的基因的p值不放大，等于它本身 我们也可以从可视化的角度来看待这个问题： 对于给定的$\alpha \in (0,1)$，设函数$y=\frac{\alpha}{m}x \quad (x=1,2,…,m)$，画出这条线，另外对于每个基因，它在图上的坐标为$(rank(p_{(k)}^{(i)}),p_{(k)}^{(i)})=(k,p_{(k)}^{(i)})$，图如下： 通过设置$\alpha$可以改变图中直线的斜率，$\alpha$越大，则直线的斜率越大，落在直线下方的点就越多，通过FDR检验的基因也就越多，反之，直线的斜率越小，落在直线下方的点就越少，通过FDR检验的基因也就越少 *3.4.2. Bonferroni 校正Bonferroni 校正的基本思想： 如果在同一数据集上同时检验n个独立的假设，那么用于每一假设的统计显著水平，应为仅检验一个假设时的显著水平的1/n 举个例子：如要在同一数据集上检验两个独立的假设，显著水平设为常见的0.05，此时用于检验该两个假设应使用更严格的0.025，即0.05* (1/2) 序列化的 Bonferroni 校正步骤： 对k个独立的检验，在给定的显著性水平α下，把每个检验对应的 P 值从小到大排列 $$p_{(1)} \le p_{(2)} \le … \le p_{(k)}$$ 首先看最小的 P 值 $p_{(1)}$，如果$p_{(1)} \le \frac{\alpha}{k}$，就认为对应的检验在总体上（table wide）α水平上显著；如果不是，就认为所有的检验都不显著； 当且仅当 $p_{(1)} \le \frac{\alpha}{k}$ 时，再来看第二个P值$p_{(2)}$。如果$p_{(2)} \le \frac{\alpha}{k-1}$，就认为在总体水平上对应的检验在α水平上是显著的； 之后再进行下一个P值……一直进行这个过程，直到 $p_{(i)} \le \frac{\alpha}{k-i+1}$不成立；下结论i和以后的检验都不显著 参考资料： (1) Emerson R O , Dewitt W S , Vignali M , et al. Immunosequencing identifies signatures of cytomegalovirus exposure history and HLA-mediated effects on the T cell repertoire[J]. Nature Genetics, 2017, 49(5):659-665. (2) Kiselev, V. Y. et al. SC3: consensus clustering of single-cell RNA-seq data[J]. Nat. Methods 14, 483–486 (2017). (3) CSDN·chivalry《二项分布和Beta分布》 (4) CSDN·Jie Qiao《带你理解beta分布》 (5) 贝塔分布（Beta Distribution）简介及其应用 (6) StatLect《Beta distribution》 (7) Storey, J.D. &amp; Tibshirani, R. Statistical signifcance for genomewide studies.Proc. Natl. Acad. Sci. USA 100, 9440–9445 (2003) (8) 国科大研究生课程《生物信息学》，陈小伟《基因表达分析》 (9) 新浪博客·菜鸟《Bonferroni 校正》 (10) 简书·Honglei_Ren《多重检验中的FDR错误控制方法与p-value的校正及Bonferroni校正》]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>Bioinformatics</tag>
        <tag>Machine-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux进阶笔记]]></title>
    <url>%2F2019%2F04%2F02%2FLinux-Advanced%2F</url>
    <content type="text"><![CDATA[安装LCD驱动2017-03-02-raspbian-jessie以前版本在/boot目录下，下载 LCD-show-161112.tar.gz : http://blog.lxx1.com/wp-content/uploads/2017/03/LCD-show-161112.tar.gz 然后解压，并赋予./LCD-show/LCD-show可执行权限 1chmod +x ./LCD-show/LCD35-show 安装完成后，树莓派会重启，然后3.5寸屏幕上就可以正常显示了。 2017-03-02-raspbian-jessie版本安装过程同上，安装包地址： wget http://blog.lxx1.com/wp-content/uploads/2017/03/LCD-show-170309.tar.gz LCD和HDMI相互切换 在正常使用LCD的情况下，外接HDMI是没有显示的，如需启用HDMI输出，需执行以下命令，树莓派会自动重启。再等待约30秒，HDMI显示屏开始显示。 1./LCD-show/LCD-hdmi 如需切换回LCD显示方式，则需执行以下命令： 1./LCD-show/LCD35-show 安装中文语言包CentOS (yum)先查看语言包是否存在 1$ locale -a 如果发现没有中文，则安装一个 12$ sudo yum install kde-l10n-Chinese$ sudo yum reinstall glibc-common Ubuntu (apt)Ubuntu中的中文语言包: language-pack-zh-hans 简体中文 language-pack-zh-hans-base language-pack-zh-hant 繁体中文 language-pack-zh-hant-base 安装中文语言包 1$ sudo apt-get install language-pack-zh-han* 在中文语言包安装完成后，就需要安装ibus输入法了，先安装ibus框架，需要在Terminal中输入： 1$sudo apt-get install ibus ibus-clutter ibus-gtk ibus-gtk3 ibus-qt4 切换到ibus框架 1$ im-config -s ibus 再安装拼音引擎 1$ sudo apt-get install ibus-pinyin 再调出ibus Preference来添加该拼音输入法： 1$ sudo ibus-setup 在弹出对话框中，添加Chinese-Pinyin输入法。 &lt;img src=/picture/Linux-advanced-ibus-config.png width=”800” /&gt; Raspbian先安装1234# 中文字体sudo apt-get install ttf-wqy-zenhei# 中文输入法sudo apt-get install scim-pinyin 设置默认的字体 1sudo raspi-config 然后选择change_locale，在Default locale for the system environment:中选择zh_CN.UTF-8,配置完成之后，输入命令sudo reboot 切换输入法：ctrl+space 更改键盘布局树莓派的默认键盘布局是英国(GB)，而我们用的键盘布局一般是美国(US)的。 通过 raspi-config 进入。 1sudo raspi-config 进入后，选择：5 Internationalisation Options -&gt; Change Keyboard LaySet -&gt; Generic 101-key PC -&gt; English(US, alternative international) 安装Gnome图形操作界面像安装CentOS，官方提供了较大的DVD IOS版的安装包，也提供了简化的Minimal IOS版的，如果选择的是Minimal版的，安装完之后一般只有命令行操作模式，不预装GNOME这样的GUI操作界面 这时如果想安装GNOME图形操作界面怎么实现？（如果一开始就知道有GUI的需求，最好直接下载完整版的 首先安装X(X Window System) 1# yum groupinstall &quot;X Window System&quot; 由于这个软件组比较大，安装过程会比较慢，安装完成会出现complete！ 检查一下我们已经安装的软件以及可以安装的软件 1# yum grouplist 然后安装我们需要的图形界面软件，GNOME(GNOME Desktop) 一定要注意：名称必须对应 1# yum groupinstall &quot;GNOME Desktop&quot; 安装完成后我们可以通过命令 startx 进入图形界面，第一次进入会比较慢，请耐心等待 更改软件源由于树莓派软件官方源在国外，所以连接不稳定，且速度慢，所以安装初次进入系统后，一定要修改一下软件源。 国内软件源推荐（中科大）：http://mirrors.ustc.edu.cn/raspbian/raspbian/ 查看当前所用的Debian版本： 1lsb_release -a 直接看到 Codename raspbian-stretch (基于Debian9)更改/etc/apt/source.list文件： 123deb http://mirrors.ustc.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpideb-src http://mirrors.ustc.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi 接着更改/etc/apt/source.list.d/raspi.list: 1deb http://mirrors.ustc.edu.cn/archive.raspberrypi.org/ stretch main ui raspbian-jessie (基于Debian8)更改/etc/apt/source.list文件： 123deb http://mirrors.ustc.edu.cn/raspbian/raspbian/ jessie main contrib non-free rpideb-src http://mirrors.ustc.edu.cn/raspbian/raspbian/ jessie main contrib non-free rpi 接着更改/etc/apt/source.list.d/raspi.list: 1deb http://mirrors.ustc.edu.cn/archive.raspberrypi.org/debian/ jessie main ui 软件管理：APT/dpkgAPT: APT——Advanced Package Tool 功能 具体语句 软件源设置 /etc/apt/sources.list 更新软件源数据 apt-get update 更新已安装软件 apt-get upgrade 更新系统版本 apt-get dist-upgrade 通过安装包或卸载包来修复依赖错误 apt-get -f install 搜索软件源数据 apt-cache search foo 解压安装软件包 apt-get install foo 重新安装软件包 apt-get –reinstall install foo 删除软件包释放的内容 apt-get remove foo 卸载软件，同时清除该软件配置文件 apt-get –purge remove foo 删除不需要的包 apt-get autoclean 删除所有已下载的包 apt-get clean 自动安装编译一软件所需要的包 apt-get build-dep foo dpkg: package manager for Debian 功能 具体语句 显示DEB包信息 dpkg -I xx.deb 显示DEB包文件列表 dpkg -c xx.deb 安装DEB包 dpkg -i xx.deb 安装DEB包（指定根目录） dpkg –root= -i xx.deb 显示所有已安装软件 dpkg -l 显示已安装包信息 dpkg -s foo 显示已安装包文件列表 dpkg -L foo 卸载包 dpkg -r foo 卸载软件包并删除其配置文件 dpkg -P foo 重新配置已安装程序 dpkg-reconfigure foo 升级及报错处理一般升级更新只需要运行以下两条命令即可： 12sudo apt-get update # 更新软件源数据sudo apt-get upgrade # 更新已安装软件 但是在实际操作过程中会出现一些问题： update 出错update过程出错一般是软件源地址无法连接，导致更新包无法下载到本地导致的，所以解决方法：更改软件源 upgrade 出错如果update过程顺利，但是到执行apt-get upgrade时报错： E: The value ‘stable’ is invalid for APT::Default-Release as such a release is not available in the sources 即系统指定的包管理工具的版本”stable”无效，需要更改为合适的版本，如“wheezy”或“jessie” 12345678# 寻找相应的配置文件grep -r &quot;stable&quot; /etc/apt/*# 得到输出：/etc/apt/apt.conf.d/99openmediavault-release:APT::Default-Release &quot;stable&quot;;# 说明配置文件为：/etc/apt/apt.conf.d/99openmediavault-release# 修改该文件，树莓派下vi/vim不太好用，建议使用nano。把&quot;stable&quot;改成&quot;jessie&quot;nano /etc/apt/apt.conf.d/99openmediavault-release 然后再执行apt-get upgrade就没问题了 文本处理双剑客: sed &amp; AWKsedAWK脚本结构 1234567891011awk &apos; BEGIN &#123;actions&#125; /pattern1/&#123;actions&#125; ... /patternN/&#123;actions&#125; END &#123;actions&#125; &apos;InputFile BEGIN{actions} 和END{actions} 是可选的 /pattern/和{actions} 可以省略，但不能同时省略 /pattern/省略时表示对所有的输入行执行指定的actions {actions} 省略时表示打印整行 编辑命令（一个、一组命令或一个命令文件） 模式：只编辑与模式相匹配的记录行；没有提供模式时，匹配所有行 命令：具体执行的编辑命令；没有指定命令时，打印整个记录行 特殊变量 变量 描述 $n 当前记录的第n个字段，字段间由FS分隔 $0 完整的输入记录 ARGC 命令行参数的数目 ARGIND 命令行中当前文件的位置（从0开始算） ARGV 包含命令行参数的数组 CONVFMT 数字转换格式（默认值为%.6g) ENVIRON 环境变量关联数组 ERRNO 最后一个系统错误描述 FIELDWIDTHS 字段宽度列表（用空格键分隔） FILENAME 当前文件名 FNR 同NR，但相对于当前文件 FS 字段分隔符（默认是任何空格） IGNORECASE 如果为真，则进行忽略大小写的匹配 NF 当前记录中的字段数 NR 当前记录数 OFMT 数字的输出格式（默认值是%.6g) OFS 输出字段分隔符（默认值是一个空格） ORS 输出记录分隔符（默认值是一个换行符） RLENGTH 由match函数所匹配的字符串的长度 RS 记录分隔符（默认是一个换行符） RSTART 由match函数所匹配的字符串第一个位置 SUBSEP 数组下标分隔符（默认值是\034） 硬链接与软链接文件由两部分构成：文件数据 + 文件元数据 文件数据：文件储存在硬盘上，硬盘的最小存储单位叫做”扇区”（Sector）。每个扇区储存512字节（相当于0.5KB）。操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个”块”（block）。这种由多个扇区组成的”块”，是文件存取的最小单位。”块”的大小，最常见的是4KB，即连续八个 sector组成一个 block。 文件元数据：文件数据都储存在”块”中，那么很显然，我们还必须找到一个地方储存文件的元信息，比如文件的创建者、文件的创建日期、文件的大小等等。这种储存文件元信息的区域就叫做inode，中文译名为”索引节点”。 inode包含文件的元信息： 文件的字节数 文件拥有者的User ID 文件的Group ID 文件的读、写、执行权限 文件的时间戳，共有三个：ctime指inode上一次变动的时间，mtime指文件内容上一次变动的时间，atime指文件上一次打开的时间。 链接数，即有多少文件名指向这个inode 文件数据block的位置 Unix/Linux系统内部不使用文件名，而使用inode号码来识别文件。对于系统来说，文件名只是inode号码便于识别的别称或者绰号。表面上，用户通过文件名，打开文件。 实际上，系统内部这个过程分成三步：首先，系统找到这个文件名对应的inode号码；其次，通过inode号码，获取inode信息；最后，根据inode信息，找到文件数据所在的block，读出数据。 硬链接： 一般情况下，文件名和inode号码是”一一对应”关系，每个inode号码对应一个文件名。但是，Unix/Linux系统允许，多个文件名指向同一个inode号码。这意味着，可以用不同的文件名访问同样的内容；对文件内容进行修改，会影响到所有文件名；但是，删除一个文件名，不影响另一个文件名的访问。这种情况就被称为”硬链接”（hard link）。 软链接： 文件A和文件B的inode号码虽然不一样，但是文件A的内容是文件B的路径。读取文件A时，系统会自动将访问者导向文件B。因此，无论打开哪一个文件，最终读取的都是文件B。这时，文件A就称为文件B的”软链接”（soft link）或者”符号链接（symbolic link）。 文件A依赖于文件B而存在，如果删除了文件B，打开文件A就会报错：”No such file or directory”。这是软链接与硬链接最大的不同：文件A指向文件B的文件名，而不是文件B的inode号码，文件B的inode”链接数”不会因此发生变化。 进程并行化最简单的并行化方法：&amp;+wait利用Bash的后台运行&amp;和wait函数，可实现最简单的批量作业并行化。 格式为： 123456789do &#123; code1; code2; ... coden; &#125; &amp;donewait 启用树莓派root用户树莓派使用的linux是debian系统，所以树莓派启用root和debian是相同的。 debian里root账户默认没有密码，但账户锁定。 当需要root权限时，由默认账户经由sudo执行，Raspberry pi 系统中的Raspbian 默认用户是pi 密码为raspberry 重新开启root账号，可由pi用户登录后，在命令行下执行 1sudo passwd root 执行此命令后系统会提示输入两遍的root密码，输入你想设的密码即可 搭建树莓派Linux服务器首先，你需要一根网线，将树莓派连接到外网 然后，开启树莓派的ssh服务 1$ sudo raspi-config 进入设置界面后选择：Advanced Options =&gt; SSH =&gt; enable 接着查看你的树莓派当前被分配的ip 1$ ifconfig -a 查看命令输出的头几行的其中一项”inet addr”就可以知道ip值了 现在Linux服务器就搭建好了，你只需要在你的电脑上安装上ssh终端工具即可登录到该服务器，是不是太简单了 解决Secure SSH Client登录服务器失败登录失败报错信息 Server responded”Algorithm negotiation failed”Key exchange with the remote host failed. This can happen forexample computer does not support the selected algorthms. 登录失败原因： Because i had past updated server, it’s that means i had updated the sshd algorithm negotiation on server meanwhile, but the sshd algorithm negotiation in client is the old, so server cannot compatible the algorithm negotiation from client. 解决方法： 在/etc/ssh/sshd_config文件中添加以下内容 123Ciphers aes128-cbc,aes192-cbc,aes256-cbc,aes128-ctr,aes192-ctr,aes256-ctr,3des-cbc,arcfour128,arcfour256,arcfour,blowfish-cbc,cast128-cbc MACs hmac-md5,hmac-sha1,umac-64@openssh.com,hmac-ripemd160,hmac-sha1-96,hmac-md5-96 KexAlgorithms diffie-hellman-group1-sha1,diffie-hellman-group14-sha1,diffie-hellman-group-exchange-sha1,diffie-hellman-group-exchange-sha256,ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,diffie-hellman-group1-sha1,curve25519-sha256@libssh.org 重启sshd服务后，即可正常连接: 1sudo /etc/init.d/ssh restart Shell编程参数传递传统方法：使用$在shell脚本中，命令$后加一些特殊的标记可以取脚本的传入参数，如$0取脚本输入的第一个参数，$1取第二个参数等： 参数 描述 $0 命令本身，类似c的argv[0] $1、$2…… 第1个参数、第2个参数…… $# 参数的个数，不包括$0 $@ 以列表的形式返回参数列表，不包括$0 $? 最后运行的命令结束代码 使用getoptgetopt不是标准的unix命令，但它在大多数的发行版中都会带有，getopt虽然是带个get但此函数其实主要不是获取而是规范 getopt后面接受-o选项表示程序可接受的短选项 如-o ab:c::，表示程序参数后面可接受的选项为-a -b -c 其中-a后面不接受参数，-b后面必须接受参数(:)，-c后面参数可选(::) getopt后面接受–long选项表示程序可接受的短选项 如--long along,blong:,clong::，长选项用逗号分隔开，后面接参数的标记号和短选项一致 在对参数进行解析前先通过getopt进行解析 12345ARGS=`getopt -o ab:c: --long along,blong:,clong: -- &quot;$@&quot;`#判断是否执行成功，没执行成功退出if [ $? != 0 ] ; then echo &quot;Terminating...&quot; &gt;&amp;2 ; exit 1 ; fi#重新设置参数eval set -- &quot;$ARGS&quot; 进行参数解析 12345678910111213141516171819202122while true;do case &quot;$1&quot; in -a) echo &quot;a&quot; shift;; -b) echo &quot;b&quot; shift;; -c) echo &quot;c&quot; shift;; --) echo &quot;--&quot; shift break ;; *) echo &quot;??&quot; shift ;; esacdone 使用getopts注意： getopts只能接受短参数，即一个字母作为一个参数 1getopts [option[:]] [DESCPRITION] VARIABLE option：表示为某个脚本可以使用的选项 &quot;:&quot;：如果某个选项（option）后面出现了冒号（”:”），则表示这个选项后面可以接参数（即一段描述信息DESCPRITION） VARIABLE：表示将某个选项保存在变量VARIABLE中 1234567891011121314151617while getopts &quot;:a:b:c:&quot; optdo case $opt in a) echo &quot;参数a的值$OPTARG&quot; ;; b) echo &quot;参数b的值$OPTARG&quot; ;; c) echo &quot;参数c的值$OPTARG&quot; ;; ?) echo &quot;未知参数&quot; exit 1;; esacdone 输出重定向当在运行一个程序，或者执行一个脚本时，会在执行过程中输出标准输出/标准错误，不论是标准输出还是标准错误，都是会默认输出到屏幕上，若想将它们输出到文件中，可以执行输出重定向 &gt;或1&gt; 标准输出重定向到文件中，标准错误还是默认输出（输出到屏幕） 2&gt; 标准错误重定向到文件夹中，标准输出还是默认输出（输出到屏幕） 1&gt; 2&gt;&amp;1 将标准输出与标准错误绑定，一起输出到文件中 若在执行echo时，想要指定输出的方式则可以采取以下方式： echo $i &gt;&amp;1 以标准输出方式输出 echo $i &gt;&amp;2 以标准错误方式输出 参考资料： (1) Raspberry 3.5inch RPi LCD-4 (2) ubuntu 16.04 英文版命令行安装中文语言包 (3) ubuntu16.4安装中文输入法 (4) 如何让树莓派显示中文？ (5) 树莓派开箱配置之更改键盘布局 (6) 树莓派3B+ 软件源更改 (7) 小伊老师Linux课程课件 (8) Linux节点理解 (9) Bash脚本实现批量作业并行化 (10) 树莓派 - 修改pi账号密码,开启root账号 (11) Ubuntu SSH Algorithm negotiation failed (12) shell学习 - 处理脚本的多参数输入 (13) shell中脚本参数传递的两种方式 (14) 【百度经验】Linux CentOS 7的图形界面安装（GNOME、KDE等）]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Raspi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python3爬虫笔记]]></title>
    <url>%2F2019%2F04%2F02%2FPython3-Webcrawler-Note%2F</url>
    <content type="text"><![CDATA[1. 搭建爬虫环境如果是在Linux操作系统下，建议建立一个私人的python开发环境 可以virtalenv也可以用anaconda/miniconda 注意：由于树莓派系统是基于ARM架构，在一些软件/安装包的安装上可能会出现一些问题 1.1. 树莓派中安装Anaconda/Miniconda树莓派的CPU是基于ARM构架，不同于常用的Intel的X86构架，因此在Anaconda/Miniconda官网上直接下载的安装文件，在树莓派上安装会报错，此时的解决方案是获取专门为ARM编写的安装文件： 12$ wget http://repo.continuum.io/miniconda/Miniconda3-latest-Linux-armv7l.sh$ bash Miniconda3-latest-Linux-armv7l.sh 1.2. 安装请求库1.2.1. 安装requests、selenium：pip用pip安装python包，若安装失败，可以尝试升级pip 升级pip的方法： 1234&gt;$ curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py&gt; &gt;$ sudo python get-pip.py&gt; 安装requests 1$ pip3 install requests 安装Selenium 1$ pip3 install selenium 1.2.2. 安装浏览器驱动然后要安装浏览器驱动，Chrome用ChromeDriver，Firefox用GeckoDriver ChromeDriver 先确定当前使用的Chrome浏览器的版本，然后到google官网上下载对应版本的ChromeDriver，并将其文件路径添加到环境变量中 在Windows中添加环境变量的方法： 计算机 =&gt; 属性 =&gt; 高级系统设置 =&gt; 环境变量 =&gt; 系统变量中选择”Path” =&gt; 点击下方的编辑 =&gt; 在Path环境变量后继续追加路径，用分号分隔 GeckoDriver Github地址： https://github.com/mozilla/geckodriver 在README主页上寻找release链接，打开后找到适合当前Firefox浏览器版本的文件下载下来 然后与ChromeDriver的操作类似，并将其文件路径添加到环境变量中 测试 ChromeDriver / GeckoDriver 先启动浏览器驱动： 然后在python交互环境中，通过命令尝试打开一个空白浏览器界面 1234567from selenium import webdriver# 通过ChromeDriver打开Chrome浏览器browser = webdriver.Chrome()# 通过GeckoDriver打开Firefox浏览器browser = webdriver.Firefox() 使用ChromeDriver和GeckoDriver我们就可以利用浏览器进行网页的抓取，但是这样可能有个不方便的地方：程序运行过程中需要一直开着浏览器，在爬取网页的过程中浏览器可能一直动来动去。所以这个时候还有另外一种选择——安装一个无界面浏览器：PantomJS PantomJS PhantomJS是一个无界面的、可脚本编辑的WebKit浏览器引擎 Selenium支持PhantomJS，运行的时候不会再弹出一个浏览器界面了 PhantomJS官网：http://phantomjs.org/，Windows下安装过程如上 Linux下基于源码的安装方法： 123456789# 获取source codegit clone git://github.com/ariya/phantomjs.gitcd phantomjsgit checkout 2.1.1git submodule initgit submodule update# 编译源码，这一步可能很耗时，请耐心等待python build.py 也可以基于apt进行一键式安装（推荐）： 1$ sudo apt-get install phantomjs 也可以从 PhantomJS 官网上下载 PhantomJS 的 Linux 二进制包 测试PantomJS 1234from selenium import webdriverbrowser = webdriver.PhantomJS()browser.get(&apos;https://www.baidu.com&apos;)print(browser.current_url) 1.2.3. 异步web服务：aiohttp之前提到的requests是一个阻塞式HTTP请求库，当发送一个请求后程序会一直等待服务器响应，知道得到响应才会进行下一步处理，这样很耗费时间 aiohttp则是一个提供异步web服务的库 1pip install aiohttp 另外，官方还推荐安装如下两个库： cchardet 字符编码检测库 aiodns 加速DNS解析库 1pip install cchardet aiodns 1.3. 安装解析库抓取网页后要做的是从网页中提取信息。提取信息的方式多种多样，可以使用正则表达式来提取，但是写起来比较繁琐。可以使用许多强大的解析库，此外还提供了强大的解析方法，如XPath解析和CSS选择器解析 lxml lxml支持HTML和XML的解析，支持XPath解析方式 1pip install lxml Beautiful Soup lxml支持HTML和XML的解析，其依赖于lxml 1pip install beautifulsoup4 验证： 123from bs4 import BeautifulSoupsoup = BeautifulSoup(&apos;&lt;p&gt;Hello&lt;/p&gt;&apos;,&apos;lxml&apos;)print(soup.p.string) 注意：虽然安装的是beautifulsoup4这个包，但是在引入的时候确实bs4，这是因为这个包源代码本身的库文件夹名称就是bs4 pyquery pyquery提供了和jQuery类似的语法来解析HTML，支持CSS选择器 1pip install pyquery tesserocr / pytesseract 在爬虫过程中难免会遇到各种各样的验证码，而大多数的验证码还是图像验证码，这时我们可以使用OCR来是识别 OCR，即 Optical Character Recognition，光学字符识别，是指通过扫描字符，通过其形状将其翻译成电子文本的过程。 对应网页中的验证码，我们可以使用OCR技术将其转化为电子文本，然后爬虫将识别结果提交给服务器，从而达到自动识别验证码的过程 tesserocr使Python的OCR识别库，但其实是对tesseract的一层Python API封装，它的核心是tesseract。因此在安装tesserocr之前，需要先安装tesseract 1、安装tesseract windows 下载地址：https://digi.bib.uni-mannheim.de/tesseract/ 注意：安装的时候要勾选Additional language data(download)，来安装OCR识别支持的语言包，这样OCR就可以识别多国语言 Linux 在linux下安装tesseract 123456789101112131415&gt; # 用apt安装&gt; $ sudo apt-get install tesseract-ocr libtesseract-dev libleptonica-dev&gt; &gt; # 用conda安装&gt; $ conda install -c bioconda tesseract&gt; &gt; # 从源码安装&gt; wget https://github.com/tesseract-ocr/tesseract/archive/3.05.01.tar.gz&gt; tar -zxvf 3.05.01.tar.gz&gt; cd tesseract-3.05.01&gt; ./autogen.sh&gt; PKG_CONFIG_PATH=/usr/local/lib/pkgconfig LIBLEPT_HEADERSDIR=/usr/local/include ./configure --with-extra-includes=/usr/local/include --with-extra-libraries=/usr/local/lib&gt; LDFLAGS=&quot;-L/usr/local/lib&quot; CFLAGS=&quot;-I/usr/local/include&quot; make&gt; make install&gt; 添加语言库 123&gt; git clone https://github.com/tesseract-ocr/tessdata.git&gt; cp tessdata/* /usr/share/tesseract-ocr/&gt; 测试tesseract 12$ tesseract image-english.png result -l eng$ tesseract image-chinese.png result -l chi-sim 2、安装tesserocr / pytesseract 12345# 安装tesserocrpip install tesserocr pillow安装pytesseractpip install pytesseract 若安装失败，可以尝试使用conda 1&gt; conda install -c simonflueckiger tesserocr 测试tesserocr 1234import tesserocrfrom PIL import Imageimage = Image.open(&apos;image.png&apos;)print(tesserocr.image_to_text(image)) 1.4. 安装爬虫框架1.4.1. pyspiderpyspider是国人binux编写的强大的网络爬虫框架 pyspider支持JavaScript渲染，而这个过程依赖于PhantomJS，所以需要同时安装好PhantomJS 1pip install pyspider 测试，直接在命令行下执行：spider all，出现如下输出： 这时pyspider的Web服务就会在本地的5000端口运行，直接在浏览器中打开http://localhost:5000即可进入pyspider的WebUI管理界面 1.4.2. ScrapyScrapy是一个十分强大的爬虫框架，依赖的库比较多，至少包括Twisted、lxml和pyOpenSSL，此时使用conda进行安装最为方便 1$ conda install Scrapy 测试：在命令行输入scrapy，若输出帮助文档则说明安装成功 1.4.3. Scrapy-SplashScrapy-Splash是一个Scrapy中支持的JavaScript渲染工具 Scrapy-Splash的安装分为两部分： Splash服务的安装 安装Scrapy-Splash的Python库 安装Splash服务 Splash介绍： Splash是一个Javascript渲染服务。它是一个实现了HTTP API的轻量级浏览器，Splash是用Python实现的，同时使用Twisted和QT。Twisted（QT）用来让服务具有异步处理能力，以发挥webkit的并发能力。 为了更加有效的制作网页爬虫，由于目前很多的网页通过javascript模式进行交互，简单的爬取网页模式无法胜任javascript页面的生成和ajax网页的爬取，同时通过分析连接请求的方式来落实局部连接数据请求，相对比较复杂，尤其是对带有特定时间戳算法的页面，分析难度较大，效率不高。而通过调用浏览器模拟页面动作模式，需要使用浏览器，无法实现异步和大规模爬取需求。鉴于上述理由Splash也就有了用武之地。一个页面渲染服务器，返回渲染后的页面，便于爬取，便于规模应用。 从splash网站上看，splash是容器安装的，Docker 是一个开源的应用容器引擎，在安装splash之前需要先安装好docker 1、在Windows上安装Docker 到Docker官网上下载安装包，官网默认用户使用的是Window10，若不是则需要下载 Dock Toolbox 2、Linux中安装Docker 123456# 用apt安装$ sudo apt-get install docker docker.io# 下载二进制包安装，二进制包的下载路径：https://download.docker.com/linux/static/stable/$ wget -c https://download.docker.com/linux/static/stable/armhf/docker-18.06.0-ce.tgz$ tar zxvf docker-18.06.0-ce.tgz 2. 爬虫基本库的使用2.1. urlliburllib包含以下3个主要模块： request 最基本的HTTP请求模块，可以用来模拟发送请求 error 异常处理模块，如果出现请求错误，可以捕获这些异常，然后进行重试或者其他操作以保证程序不会意外终止 parse 一个工具模块，提供了许多URL处理方法，比如拆分解析、合并等 2.1.1. 发送请求1、urlopen 基本用法 利用最基本的urlopen( )方法，可以完成最基本的简单网页的GET请求抓取 1234import urllib.requestresponse = urllib.request.urlopen(&apos;https://www.python.org&apos;)print(response.read().decode(&apos;utf-8&apos;)) 查看响应类型： 123type(response)&lt;class &apos;http.client.HTTPResponse&apos;&gt; 一些方法的使用： 12345678&gt;&gt;&gt; print(response.status)200&gt;&gt;&gt; print(response.getheaders())[(&apos;Server&apos;, &apos;nginx&apos;), (&apos;Content-Type&apos;, &apos;text/html; charset=utf-8&apos;), (&apos;X-Frame-Options&apos;, &apos;SAMEORIGIN&apos;), (&apos;x-xss-protection&apos;, &apos;1; mode=block&apos;), (&apos;X-Clacks-Overhead&apos;, &apos;GNU Terry Pratchett&apos;), (&apos;Via&apos;, &apos;1.1 varnish&apos;), (&apos;Content-Length&apos;, &apos;48809&apos;), (&apos;Accept-Ranges&apos;, &apos;bytes&apos;), (&apos;Date&apos;, &apos;Fri, 17 Aug 2018 06:31:57 GMT&apos;), (&apos;Via&apos;, &apos;1.1 varnish&apos;), (&apos;Age&apos;, &apos;3242&apos;), (&apos;Connection&apos;, &apos;close&apos;), (&apos;X-Served-By&apos;, &apos;cache-iad2145-IAD, cache-hkg17926-HKG&apos;), (&apos;X-Cache&apos;, &apos;HIT, HIT&apos;), (&apos;X-Cache-Hits&apos;, &apos;9, 10&apos;), (&apos;X-Timer&apos;, &apos;S1534487518.955612,VS0,VE0&apos;), (&apos;Vary&apos;, &apos;Cookie&apos;), (&apos;Strict-Transport-Security&apos;, &apos;max-age=63072000; includeSubDomains&apos;)]&gt;&gt;&gt; print(response.getheader(&apos;Server&apos;))nginx 传递参数 data参数：添加额外数据 123456import urllib.parseimport urllib.requestdata = bytes(urllib.parse.urlencode(&#123;&apos;word&apos;:&apos;hello&apos;&#125;),encoding=&apos;utf-8&apos;)response = urllib.request.urlopen(&apos;http://httpbin.org/post&apos;,data=data)print(response.read()) timeout参数：设置超时时间，单位为秒 1response = urllib.request.urlopen(&apos;http://httpbin.org/get&apos;,timeout=0.1) # 按常理，0.1秒是基本不可能得到服务器响应的 出现以下报错信息： 123456Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;/home/pi/miniconda3/lib/python3.4/urllib/request.py&quot;, line 161, in urlopen return opener.open(url, data, timeout) ...urllib.error.URLError: &lt;urlopen error timed out&gt; 其抛出的是URLError异常 可以利用try excep语句来实现，若长时间没有响应就跳过它的抓取 123456789import socketimport urllib.requestimport urllib.errortry: response = urllib.request.urlopen(&apos;http://httpbin.org/get&apos;,timeout=0.1)except urllib.error.URLError as e: if isinstance(e.reason,socket.timeout): print(&apos;TIME OUT&apos;) 2、Request 利用urlopen( ) 方法可以实现最基本的请求发起，但是这几个简单的参数还不足够构建一个完整的请求，如果想在请求中加入Header等信息，可以利用Request： 1234import urllib.requestrequest = urllib.request.Request(&apos;https://python.org&apos;)response = urllib.request.urlopen(request) 依然使用的是urlopen( ) 来发生请求，只不过这次该方法的参数不再是URL，而是一个Request类型对象 构造Request对象： 1urllib.request.Request(url,data=None,headers=&#123;&#125;,origin_req_host=None,method=None) 3、高级用法 处理高级操作：Cookies处理，代理设置等等 需要使用到Handler工具，它是urllib.request模块里的BaseHandler类，它是所有Handler的父类其中一个重要的Handler类是OpenerDirector类，可以称之为Opener 验证 有些网站在打开的时候会弹出提示框，直接提示你输入用户名和密码，验证成功后才能查看页面 如果要请求这样的页面，需要借助 HTTPBasicAuthHandler 123456789101112131415161718from urllib.request import HTTPPasswordMgrWithDefaultRealm,HTTPBasicAuthHandler,build_openerfrom urllib.error import URLErrorusername = &apos;username&apos;password = &apos;password&apos;url = &apos;https://...&apos;p = HTTPPasswordMgrWithDefaultRealm()p.add_password(None,url,username,password)auth_handler = HTTPBasicAuthHandler(p)opener = build_opener(auth_handler)try: result = opener.open(url) html = result.read().decode(&apos;utf-8&apos;) print(html)except URLError as e: print(e.reason) 代理 12345678910from urllib.error import URLErrorfrom urllib.request import ProxyHandler,build_openerproxy_handler=ProxyHandler(&#123;&apos;http&apos;:&apos;http://localhost:9743&apos;,&apos;https&apos;:&apos;https://localhost:9743&apos;&#125;)opener=build_opener(proxy_handler)try: response=opener.open(&apos;https://www.baidu.com&apos;) print(response.read().decode(&apos;utf-8&apos;))except URLError as e: print(e.reason) Cookies 12345678import http.cookiejar, urllib.requestcookie = http.cookiejar.CookieJar()handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open(&apos;http://www.baidu.com&apos;)for item in cookie: print(item.name+&quot;=&quot;+item.value) 2.1.2. 处理异常在发送请求后可能出现异常，如果不处理这些异常，程序很可能因报错而终止，所以异常处理十分必要 urllib的error模块定义了由request模块产生的异常，如果出现了问题，request模块便会抛出error模块中定义的异常： URLError和HTTPError类，其中HTTPError类URLError类的子类 URLError 12345from urllib import request, errortry: response = request.urlopen(&apos;http://cuiqingcai.com/index.htm&apos;)except error.URLError as e: print(e.reason) HTTPError 12345from urllib import request,errortry: response = request.urlopen(&apos;http://cuiqingcai.com/index.htm&apos;)except error.HTTPError as e: print(e.reason, e.code, e.headers, sep=&apos;\n&apos;) 通常会先去捕获子类的错误，再去捕获父类的错误 12345678910from urllib import request, errortry: response = request.urlopen(&apos;http://cuiqingcai.com/index.htm&apos;)except error.HTTPError as e: print(e.reason, e.code, e.headers, sep=&apos;\n&apos;)except error.URLError as e: print(e.reason)else: print(&apos;Request Successfully&apos;) 2.1.3. 解析链接1、 urlparse 该方法可以实现URL的识别和分段： 1234567from urllib.parse import urlparseresult = urlparse(&apos;http://www.baidu.com/index.html;user?id=5#comment&apos;)print(type(result), result)&lt;class &apos;urllib.parse.ParseResult&apos;&gt; ParseResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html&apos;, params=&apos;user&apos;, query=&apos;id=5&apos;, fragment=&apos;comment&apos;) 2、 urlunparse 它是作业与urlparse相反，其接受的参数长度必须为6 1234567from urllib.parse import urlunparsedata = [&apos;http&apos;, &apos;www.baidu.com&apos;, &apos;index.html&apos;, &apos;user&apos;, &apos;a=6&apos;, &apos;comment&apos;]print(urlunparse(data))http://www.baidu.com/index.html;user?a=6#comment 3、urlsplit 与urlparse相似，不过它不再单独解析parms这一部分，只返回5个结果 1234567from urllib.parse import urlsplitresult = urlsplit(&apos;http://www.baidu.com/index.html;user?id=5#comment&apos;)print(result)SplitResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html;user&apos;, query=&apos;id=5&apos;, fragment=&apos;comment&apos;) 4、urlunsplit 与urlunparse相似，唯一区别是长度必须是5 1234from urllib.parse import urlunsplitdata = [&apos;http&apos;, &apos;www.baidu.com&apos;, &apos;index.html&apos;, &apos;a=6&apos;, &apos;comment&apos;]print(urlunsplit(data)) 5、urljoin 用法：urljoin(url_1,url_2) 提供一个base_url作为第一个参数，将新的链接作为第二个参数，该方法会解析base_url的scheme、netloc和path这3个内容并对新链接缺失的部分进行补充 12345678910from urllib.parse import urljoinprint(urljoin(&apos;http://www.baidu.com&apos;, &apos;FAQ.html&apos;))print(urljoin(&apos;http://www.baidu.com&apos;, &apos;https://cuiqingcai.com/FAQ.html&apos;))print(urljoin(&apos;http://www.baidu.com/about.html&apos;, &apos;https://cuiqingcai.com/FAQ.html&apos;))print(urljoin(&apos;http://www.baidu.com/about.html&apos;, &apos;https://cuiqingcai.com/FAQ.html?question=2&apos;))print(urljoin(&apos;http://www.baidu.com?wd=abc&apos;, &apos;https://cuiqingcai.com/index.php&apos;))print(urljoin(&apos;http://www.baidu.com&apos;, &apos;?category=2#comment&apos;))print(urljoin(&apos;www.baidu.com&apos;, &apos;?category=2#comment&apos;))print(urljoin(&apos;www.baidu.com#comment&apos;, &apos;?category=2&apos;)) 输出 12345678http://www.baidu.com/FAQ.htmlhttps://cuiqingcai.com/FAQ.htmlhttps://cuiqingcai.com/FAQ.htmlhttps://cuiqingcai.com/FAQ.html?question=2https://cuiqingcai.com/index.phphttp://www.baidu.com?category=2#commentwww.baidu.com?category=2#commentwww.baidu.com?category=2 6、urlencode 它在构造GET请求参数的时候非常有用 123456789101112from urllib.parse import urlencodeparams = &#123; &apos;name&apos;: &apos;germey&apos;, &apos;age&apos;: 22&#125;base_url = &apos;http://www.baidu.com?&apos;url = base_url + urlencode(params)print(url)http://www.baidu.com?age=22&amp;name=germey urlencode( )方法可以将字典形式的变量序列化，转化为GET请求的参数 7、parse_qs 有了序列化，自然就有反序列化 1234567from urllib.parse import parse_qsquery = &apos;name=germey&amp;age=22&apos;print(parse_qs(query))&#123;&apos;age&apos;: [&apos;22&apos;], &apos;name&apos;: [&apos;germey&apos;]&#125; 8、quote URL中带有中文参数时，可能会导致乱码的问题，此时需要将中文字符转化为URL编码 12345678from urllib.parse import quotekeyword = &apos;壁纸&apos;url = &apos;https://www.baidu.com/s?wd=&apos; + quote(keyword)print(url)https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8 2.1.4. 分析Robots协议每个网站的Robots协议一般保存在站点的根目录下的robots.txt文件中 例如： 123456User-agent: BaiduspiderDisallow: /baiduDisallow: /s?Disallow: /ulink?Disallow: /link?Disallow: /home/news/data/ 可以使用robotparser模块解析robots.txt 1234567from urllib.robotparser import RobotFileParserrp = RobotFileParser()rp.set_url(&apos;http://www.jianshu.com/robots.txt&apos;)rp.read()print(rp.can_fetch(&apos;*&apos;, &apos;http://www.jianshu.com/p/b67554025d7d&apos;))print(rp.can_fetch(&apos;*&apos;, &quot;http://www.jianshu.com/search?q=python&amp;page=1&amp;type=collections&quot;)) 2.2. requestsurllib在使用过程中有一些不方便的地方，比如处理网页页面验证和Cookies时，需要写Opener和Handler来处理 requests可以更加方便的实现这些操作 2.2.1. 基本用法1234567import requests# GET方法r= requests.get(url,headers=headers,params=data) # 以字典形式传入headers和data# POST方法r= requests.post(url,headers=headers,data=data) 有时需要构造header，否则无法正常请求 123headers = &#123; &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36&apos;&#125; 抓取网页，然后用正则表达式进行匹配 12345678910import requestsimport reheaders = &#123; &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36&apos;&#125;r = requests.get(&quot;https://www.zhihu.com/explore&quot;, headers=headers)pattern = re.compile(&apos;explore-feed.*?question_link.*?&gt;(.*?)&lt;/a&gt;&apos;, re.S)titles = re.findall(pattern, r.text)print(titles) 抓取二进制数据 12345import requestsr = requests.get(&quot;https://github.com/favicon.ico&quot;)with open(&apos;favicon.ico&apos;, &apos;wb&apos;) as f: f.write(r.content) 以字符串形式直接打印会乱码： print(r.text) 以二进制形式打印： 123print(r.content)b&apos;\x00\x00\x01\x00\x02\x00\x10\x10\x00\x00\x01\x00 ...&apos; 响应 requests提供了一个内置的状态码查询对象requests.codess，可以用来判断请求是否成功 1234import requestsr = requests.get(&apos;http://www.jianshu.com&apos;)exit() if not r.status_code == requests.codes.ok else print(&apos;Request Successfully&apos;) 2.2.2. 高级用法2.2.2.1. 文件上传12345import requestsfiles = &#123;&apos;file&apos;: open(&apos;favicon.ico&apos;, &apos;rb&apos;)&#125;r = requests.post(&apos;http://httpbin.org/post&apos;, files=files)print(r.text) 2.2.2.2. Cookies获取Cookies 123456import requestsr = requests.get(&apos;https://www.baidu.com&apos;)print(r.cookies)for key, value in r.cookies.items(): print(key + &apos;=&apos; + value) 可以将Cookies在发送请求时加入headers中维持登录状态 123456789import requestsheaders = &#123; &apos;Cookie&apos;: &apos;q_c1=31653b264a074fc9a57816d1ea93ed8b|1474273938000|1474273938000; d_c0=&quot;AGDAs254kAqPTr6NW1U3XTLFzKhMPQ6H_nc=|1474273938&quot;; __utmv=51854390.100-1|2=registration_date=20130902=1^3=entry_date=20130902=1;a_t=&quot;2.0AACAfbwdAAAXAAAAso0QWAAAgH28HQAAAGDAs254kAoXAAAAYQJVTQ4FCVgA360us8BAklzLYNEHUd6kmHtRQX5a6hiZxKCynnycerLQ3gIkoJLOCQ==&quot;;z_c0=Mi4wQUFDQWZid2RBQUFBWU1DemJuaVFDaGNBQUFCaEFsVk5EZ1VKV0FEZnJTNnp3RUNTWE10ZzBRZFIzcVNZZTFGQmZn|1474887858|64b4d4234a21de774c42c837fe0b672fdb5763b0&apos;, &apos;Host&apos;: &apos;www.zhihu.com&apos;, &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36&apos;,&#125;r = requests.get(&apos;https://www.zhihu.com&apos;, headers=headers)print(r.text) 也可以通过cookies参数来设置，不过这需要构造一个RequestsCookieJar对象 12345678910111213import requestscookies = &apos;q_c1=31653b264a074fc9a57816d1ea93ed8b|1474273938000|1474273938000; d_c0=&quot;AGDAs254kAqPTr6NW1U3XTLFzKhMPQ6H_nc=|1474273938&quot;; __utmv=51854390.100-1|2=registration_date=20130902=1^3=entry_date=20130902=1;a_t=&quot;2.0AACAfbwdAAAXAAAAso0QWAAAgH28HQAAAGDAs254kAoXAAAAYQJVTQ4FCVgA360us8BAklzLYNEHUd6kmHtRQX5a6hiZxKCynnycerLQ3gIkoJLOCQ==&quot;;z_c0=Mi4wQUFDQWZid2RBQUFBWU1DemJuaVFDaGNBQUFCaEFsVk5EZ1VKV0FEZnJTNnp3RUNTWE10ZzBRZFIzcVNZZTFGQmZn|1474887858|64b4d4234a21de774c42c837fe0b672fdb5763b0&apos;jar = requests.cookies.RequestsCookieJar()headers = &#123; &apos;Host&apos;: &apos;www.zhihu.com&apos;, &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36&apos;&#125;for cookie in cookies.split(&apos;;&apos;): key, value = cookie.split(&apos;=&apos;, 1) jar.set(key, value)r = requests.get(&apos;http://www.zhihu.com&apos;, cookies=jar, headers=headers)print(r.text) 2.2.2.3. 会话维持在requests中，如果直接用get( ) 或post( )方法的确可以做到模拟网页的请求，但实际上相当于不同的会话 设想一个这样的场景： 第一个请求用post( ) 方法登录了某个网站，第二次想获取成功登录后的个人信息，你又用了一次get( ) 方法去请求个人信息的页面。实际上这相当于打开了第二个浏览器，是完全不同的会话，肯定不能获得你想要的个人信息 12345678910import requestsrequests.get(&apos;http://httpbin.org/cookies/set/number/123456789&apos;) # 请求测试网站，并设置一个cookie，名称为number，值为123456789r = requests.get(&apos;http://httpbin.org/cookies&apos;) # 请求http://httpbin.org/cookies网站，获取当前的cookieprint(r.text)&#123; &quot;cookies&quot;: &#123;&#125;&#125; 用session试试： 12345678910111213import requestss = requests.Session()s.get(&apos;http://httpbin.org/cookies/set/number/123456789&apos;)r = s.get(&apos;http://httpbin.org/cookies&apos;)print(r.text)&#123; &quot;cookies&quot;: &#123; &quot;number&quot;: &quot;123456789&quot; &#125;&#125; 2.2.2.4. SSL证书验证当发送HTTP请求的时候，它会检查SSL证书，我们可以使用verify参数控制是否检查次证书。默认会自动进行证书检查 测试12306网站的SSL证书 1234import requestsresponse = requests.get(&apos;https://www.12306.cn&apos;) # 认证失败报错print(response.status_code) 关闭自动认证： 1234response = requests.get(&apos;https://www.12306.cn&apos;, verify=False) # 虽然关闭了认证，但任然会给出一条警告信息print(response.status_code)200 屏蔽警告信息的方法： 1、 123456import requestsfrom requests.packages import urllib3urllib3.disable_warnings()response = requests.get(&apos;https://www.12306.cn&apos;, verify=False)print(response.status_code) 2、 12345import loggingimport requestslogging.captureWarnings(True)response = requests.get(&apos;https://www.12306.cn&apos;, verify=False)print(response.status_code) 我们也可以指定本地的认证证书 1234import requestsresponse = requests.get(&apos;https://www.12306.cn&apos;, cert=(&apos;/path/server.crt&apos;, &apos;/path/key&apos;))print(response.status_code) 2.2.2.5. 代理设置12345678import requestsproxies = &#123; &apos;http&apos;: &apos;http://10.10.1.10:3128&apos;, # 请换成自己的有效代理 &apos;https&apos;: &apos;http://10.10.1.10:1080&apos;,&#125;requests.get(&apos;https://www.taobao.com&apos;, proxies=proxies) 若代理需要使用HTTP Basic Auth，可以使用类似 http://user:password@host:port 这样的语法来设置代理 123456import requestsproxies = &#123; &apos;https&apos;: &apos;http://user:password@10.10.1.10:3128/&apos;,&#125;requests.get(&apos;https://www.taobao.com&apos;, proxies=proxies) 2.2.2.6. 身份认证在访问网站时，可能会遇到认证界面，此时可以使用requests自带的身份认证功能 12345import requestsfrom requests.auth import HTTPBasicAuthr = requests.get(&apos;http://localhost:5000&apos;, auth=HTTPBasicAuth(&apos;username&apos;, &apos;password&apos;))print(r.status_code) 更简单的写法（不需要传入一个HTTPBasicAuth类）： 1234import requestsr = requests.get(&apos;http://localhost:5000&apos;, auth=(&apos;username&apos;, &apos;password&apos;))print(r.status_code) 2.2.2.7. Prepared Request如同urllib中提到的，我们可以将请求表示为数据结构，这个数据结构就叫 Prepared Request 1234567891011121314from requests import Request, Sessionurl = &apos;http://httpbin.org/post&apos;data = &#123; &apos;name&apos;: &apos;germey&apos;&#125;headers = &#123; &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36&apos;&#125;s = Session()req = Request(&apos;POST&apos;, url, data=data, headers=headers)prepped = s.prepare_request(req)r = s.send(prepped)print(r.text) 2.3. 正则表达式Python的re库提供了整个正则表达式的实现 1、 match( ) match( )方法会尝试从字符串的起始位置匹配正则表达式 如果匹配就返回匹配成功的结果；如果不匹配，就返回None 1match(regex, str) 匹配得到的结果是一个SRE_Match对象，该对象有两个方法： group( )：输出匹配到的内容 span( )：输出匹配的范围 2、匹配目标 如果想从字符串中提取一部分内容，可以使用括号( )将想提取的子字符串括起来 1234567891011121314import recontent = &apos;Hello 1234567 World_This is a Regex Demo&apos;result = re.match(&apos;^Hello\s(\d+)\sWorld&apos;, content)print(result)print(result.group()) # 输出完整的匹配结果print(result.group(1)) # 输出第一个被括号包围的匹配结果print(result.span())&lt;_sre.SRE_Match object; span=(0, 19), match=&apos;Hello 1234567 World&apos;&gt;Hello 1234567 World1234567(0, 19) 3、贪婪与非贪婪 .* 贪婪匹配 .*? 非贪婪匹配 贪婪匹配 12345678910import recontent = &apos;Hello 1234567 World_This is a Regex Demo&apos;result = re.match(&apos;^He.*(\d+).*Demo$&apos;, content)print(result)print(result.group(1))&lt;_sre.SRE_Match object; span=(0, 40), match=&apos;Hello 1234567 World_This is a Regex Demo&apos;&gt;7 非贪婪匹配 12345678910import recontent = &apos;Hello 1234567 World_This is a Regex Demo&apos;result = re.match(&apos;^He.*?(\d+).*Demo$&apos;, content)print(result)print(result.group(1))&lt;_sre.SRE_Match object; span=(0, 40), match=&apos;Hello 1234567 World_This is a Regex Demo&apos;&gt;1234567 4、修饰符 . 匹配的是除换行符之外的任意字符，当遇到换行符时，.*?就不能匹配了，所以导致匹配失败 123456789101112import recontent = &apos;&apos;&apos;Hello 1234567 World_Thisis a Regex Demo&apos;&apos;&apos;result = re.match(&apos;^He.*?(\d+).*?Demo$&apos;, content)print(result.group(1))Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: &apos;NoneType&apos; object has no attribute &apos;group&apos; 这时，只需要添加一个修饰符 re.S 1result = re.match(&apos;^He.*?(\d+).*?Demo$&apos;, content,re.S) 修饰符 re.S 的作用是使 . 匹配包括换行符在内的所有字符 这个 re.S 在网页匹配中经常使用，因为HTML节点经常会有换行，加上它，就可以匹配节点与节点之间的换行了 其他常用的修饰符： 修饰符 描述 re.I 使匹配对大小写不敏感 re.S 使 . 匹配包括换行符在内的所有字符 5、search match( )方法是尝试从字符串的起始位置匹配正则表达式，一旦开头不匹配，那么整个匹配就失败了 12345678mport recontent = &apos;Extra stings Hello 1234567 World_This is a Regex Demo Extra stings&apos;result = re.match(&apos;Hello.*?(\d+).*?Demo&apos;, content)print(result)None 另外一个方法search( )，它在匹配时会扫描整个字符串，然后返回第一个成功匹配的结果。因此未来匹配方便，我们可以尽量使用search( )方法 实例： 提取class为active的li节点内部超链接包含的歌手名和歌名 123456789101112131415161718192021222324252627&gt; import re&gt; &gt; html = &apos;&apos;&apos;&lt;div id=&quot;songs-list&quot;&gt;&gt; &lt;h2 class=&quot;title&quot;&gt;经典老歌&lt;/h2&gt;&gt; &lt;p class=&quot;introduction&quot;&gt;&gt; 经典老歌列表&gt; &lt;/p&gt;&gt; &lt;ul id=&quot;list&quot; class=&quot;list-group&quot;&gt;&gt; &lt;li data-view=&quot;2&quot;&gt;一路上有你&lt;/li&gt;&gt; &lt;li data-view=&quot;7&quot;&gt;&gt; &lt;a href=&quot;/2.mp3&quot; singer=&quot;任贤齐&quot;&gt;沧海一声笑&lt;/a&gt;&gt; &lt;/li&gt;&gt; &lt;li data-view=&quot;4&quot; class=&quot;active&quot;&gt; # 提取范围&gt; &lt;a href=&quot;/3.mp3&quot; singer=&quot;齐秦&quot;&gt;往事随风&lt;/a&gt; # 提取范围&gt; &lt;/li&gt;&gt; &lt;li data-view=&quot;6&quot;&gt;&lt;a href=&quot;/4.mp3&quot; singer=&quot;beyond&quot;&gt;光辉岁月&lt;/a&gt;&lt;/li&gt;&gt; &lt;li data-view=&quot;5&quot;&gt;&lt;a href=&quot;/5.mp3&quot; singer=&quot;陈慧琳&quot;&gt;记事本&lt;/a&gt;&lt;/li&gt;&gt; &lt;li data-view=&quot;5&quot;&gt;&gt; &lt;a href=&quot;/6.mp3&quot; singer=&quot;邓丽君&quot;&gt;&lt;i class=&quot;fa fa-user&quot;&gt;&lt;/i&gt;但愿人长久&lt;/a&gt;&gt; &lt;/li&gt;&gt; &lt;/ul&gt;&gt; &lt;/div&gt;&apos;&apos;&apos;&gt; &gt; results = re.findall(&apos;&lt;li.*?active.*?singer=&quot;(.*?)&quot;&gt;(.*?)&lt;/a&gt;&apos;, html, re.S)&gt; if result:&gt; print(result.group(1),result.group(2)&gt; 输出： 12&gt; 齐秦往事随风&gt; 6、findall findall( )方法会搜索整个字符串，然后返回匹配正则表达式的所有内容 还是上面的HTML文本，获取所有的a节点的超链接、歌手和歌名 123456results = re.findall(&apos;&lt;li.*?href=&quot;(.*?)&quot;.*?singer=&quot;(.*?)&quot;&gt;(.*?)&lt;/a&gt;&apos;, html, re.S)print(results)print(type(results))for result in results: print(result) print(result[0], result[1], result[2]) 7、sub 使用正则表达式修改文本。可以使用字符串的replace( )方法，但是这样做太繁琐了，这时可以使用sub( )方法 实例：将所有数字去掉 12345678import recontent = &apos;54aK54yr5oiR54ix5L2g&apos;content = re.sub(&apos;\d+&apos;, &apos;&apos;, content)print(content)aKyroiRixLg 上面的HTML文本中，可以先用sub( )方法将a节点去掉，只留下文本，然后再利用findall( )提取 12345html = re.sub(&apos;&lt;a.*?&gt;|&lt;/a&gt;&apos;, &apos;&apos;, html)print(html)results = re.findall(&apos;&lt;li.*?&gt;(.*?)&lt;/li&gt;&apos;, html, re.S)for result in results: print(result.strip()) 8、compile compile( )方法可以将正则字符串编译成正则表达式对象，以便在后面的匹配中复用 12345678910import recontent1 = &apos;2016-12-15 12:00&apos;content2 = &apos;2016-12-17 12:55&apos;content3 = &apos;2016-12-22 13:21&apos;pattern = re.compile(&apos;\d&#123;2&#125;:\d&#123;2&#125;&apos;)result1 = re.sub(pattern, &apos;&apos;, content1)result2 = re.sub(pattern, &apos;&apos;, content2)result3 = re.sub(pattern, &apos;&apos;, content3)print(result1, result2, result3) 2.4. 实战：爬取猫眼电影排行1、网页分析 目标站点为http://maoyan.com/board/4 翻到下一页后URL变为：http://maoyan.com/board/4?offset=10 可以看到这个比之前那个URL多了一个参数offset=10，初步推断为一个偏移量的参数 2、抓取首页 1234567891011121314151617import requestsdef get_one_page(url): headers = &#123; &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36&apos; &#125; response = requests.get(url,headers=headers) if response.status_code == 200: return response.text return Nonedef main(): url = &apos;http://maoyan.com/board/4&apos; html = get_one_page(url) print(html)main() 3、正则提取 根据网页源代码，编辑正则表达式进行正则提取 123456789101112131415def parse_one_page(html): pattern = re.compile(&apos;&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;.*?name&quot;&gt;&lt;a&apos; + &apos;.*?&gt;(.*?)&lt;/a&gt;.*?star&quot;&gt;(.*?)&lt;/p&gt;.*?releasetime&quot;&gt;(.*?)&lt;/p&gt;&apos; + &apos;.*?integer&quot;&gt;(.*?)&lt;/i&gt;.*?fraction&quot;&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;&apos;,re.S) items = re.findall(pattern,html) # 原始的匹配结果比较杂乱，处理一下遍历提取结果并生成字典 for item in items: yield &#123; &apos;index&apos;: item[0], &apos;image&apos;: item[1], &apos;title&apos;: item[2], &apos;actor&apos;: item[3].strip()[3:], &apos;time&apos;: item[4].strip()[5:], &apos;score&apos;: item[5] + item[6] &#125; 4、写入文件 通过JSON库的dumps( )方法实现字典的序列化，并指定ensure_ascii参数为False，这样可以保证输出结果是中文形式而不是Unicode编码 123def write_to_file(content): with open(&apos;result.txt&apos;, &apos;a&apos;, encoding=&apos;utf-8&apos;) as f: f.write(json.dumps(content, ensure_ascii=False) + &apos;\n&apos;) 5、整合代码 12345def main(): url = &apos;http://maoyan.com/board/4&apos; html = get_one_page(url) for item in parse_one_page(html): write_to_file(item) 6、分页爬取 1234if __name__ == &apos;__main__&apos;: for i in range(10): main(offset=i * 10) time.sleep(1) 对main()方法稍作修改，接受一个offset值作为偏移量 123456def main(offset): url = &apos;http://maoyan.com/board/4?offset=&apos; + str(offset) html = get_one_page(url) for item in parse_one_page(html): print(item) write_to_file(item) 3. 解析库的使用使用正则表达式提取页面信息，比较繁琐 对于网页的节点来说，它可以定义id、class或其他属性。而且节点之间还有层次关系，在网页中可以通过XPath或CSS选择器来定位一个或多个节点。那么在页面解析的时候，利用XPath或CSS选择器来提取某个节点，然后再调用相应的方法获取它的正文内容或属性，就可以提取我们想要的任意信息了 3.1. 使用XPath需要使用到lxml库 XPath常用规则： 表达式 描述 / 从当前节点选择直接子节点 // 从当前节点选择子孙节点 . 选择当前节点 .. 选择当前节点的父节点 @ 选取属性 1234567891011121314151617from lxml import etree# 该段HTML文本中最后一个li节点没有闭合text = &apos;&apos;&apos;&lt;div&gt; &lt;ul&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt; &lt;/ul&gt; &lt;/div&gt;&apos;&apos;&apos;html = etree.HTML(text) # 构造XPatn解析对象，etree模块会自动修正HTML文本result = etree.tostring(html) # 得到修正后的HTML文本print(result.decode(&apos;utf-8&apos;)) 1、选取所有节点 1html.xpath(&apos;//*&apos;) 2、选取所有的 li 节点 1html.xpath(&apos;//li&apos;) 3、选取所有 li 节点下的 a 节点（a节点是li节点的直接子节点） 1html.xpath(&apos;//li/a&apos;) 4、选取ul节点下的a节点（a节点不是ul节点的直接子节点） 1html.xpath(&apos;//ul//a&apos;) 由于a节点不是ul节点的直接子节点，所以如果用//ul/a就无法获得任何结果 5、父节点：选取href属性为link4.html的a节点的父节点，然后获取它的class属性 1html.xpath(&apos;//a[@class=&quot;link4.html&quot;]/../@class&apos;) 也可以通过parent::来获取父节点 1html.xpath(&apos;//a[@class=&quot;link4.html&quot;]/parent::*/@class&apos;) 6、文本获取 使用XPath的text( )方法，利用获取节点中的文本 例如提取以下两个class属性值为 item-0 的 li 节点中的文本： 123&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt;&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; 若用//li[@class=&quot;item-0&quot;]/text()进行提取，会得到以下的结果： 1[&apos;\n `] 这是因为//li[@class=&quot;item-0&quot;]/定位的位置为： 123456 -&gt;| |&lt;-&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt; -&gt;|&lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;|&lt;-&lt;/li&gt; 而选取的区间内若出现其他节点，则会忽略这些节点，则在第一个li节点，选取的文本的范围为a节点尾标签到li节点尾标签之间的文本——无文本，则没有返回结果；第二个li节点，有换行符与下一行的多个缩进符 因此，如果想要获取li节点内部的文本，有两种方式 一种是选选取a节点再获取文本 12&gt; //li[@class=&quot;item-0&quot;]/a/text()&gt; 另一种是使用// 12&gt; //li[@class=&quot;item-0&quot;]//text()&gt; 7、属性匹配 属性多值匹配 用class属性匹配li节点，获得其下的a节点的文本 1&lt;li class=&quot;li li-first&quot;&gt;&lt;a href=&quot;link.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt; 如果使用//li[@class=&quot;li&quot;，会无法匹配，这时需要用contain( )函数 1html.xpath(&apos;//li[contains(@class, &quot;li&quot;)]/a/text()&apos;) 多属性匹配 根据多个属性确定一个节点，需要匹配多个属性，将多个属性匹配用and连接 例如，用class属性和name属性选择 li 节点 1&lt;li class=&quot;li li-first&quot; name=&quot;item&quot;&gt;&lt;a href=&quot;link.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt; XPath表达式 1html.xpath(&apos;//li[contains(@class, &quot;li&quot;) and @name=&quot;item&quot;]/a/text()&apos;) 8、按顺序选择 1234567891011121314151617181920text = &apos;&apos;&apos;&lt;div&gt; &lt;ul&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt; &lt;/ul&gt; &lt;/div&gt;&apos;&apos;&apos;html = etree.HTML(text)result = html.xpath(&apos;//li[1]/a/text()&apos;) # 选择第一个li节点result = html.xpath(&apos;//li[last()]/a/text()&apos;) # 选择最后一个li节点result = html.xpath(&apos;//li[position()&lt;3]/a/text()&apos;) # 选择前2个li节点result = html.xpath(&apos;//li[last()-2]/a/text()&apos;) # 选择倒数第3个li节点 9、节点轴选择 1234567891011121314151617181920212223242526text = &apos;&apos;&apos;&lt;div&gt; &lt;ul&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;&lt;span&gt;first item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt; &lt;/ul&gt; &lt;/div&gt;&apos;&apos;&apos;html = etree.HTML(text)result = html.xpath(&apos;//li[1]/ancestor::*&apos;) # 选择第1个li节点的所有祖先节点result = html.xpath(&apos;//li[1]/ancestor::div&apos;) # 选择第1个li节点的祖先节点中的div节点result = html.xpath(&apos;//li[1]/attribute::*&apos;) # 选择第1个li节点的所有属性值result = html.xpath(&apos;//li[1]/child::a[@href=&quot;link1.html&quot;]&apos;) # 选择第1个li节点的子节点中的a节点，且该节点的href属性值为link1.htmlresult = html.xpath(&apos;//li[1]/descendant::span&apos;) # 选择第1个li节点的子孙节点中的span节点result = html.xpath(&apos;//li[1]/following::*[2]&apos;) # 选择第1个li节点之后的第二个节点，li节点本身为第一个节点result = html.xpath(&apos;//li[1]/following-sibling::*&apos;) # 选择第1个li节点后的所有同级节点 3.2. 使用Beautiful SoupBeautiful Soup支持的解析器： 解析器 使用方法 Python标准库 BeautifulSoup(markup, ‘html.parser’) lxml HTML解析器 BeautifulSoup(markup, ‘lxml’) lxml XML解析器 BeautifulSoup(markup, ‘xml’) html5lib BeautifulSoup(markup,’html5lib’) 基本用法： 123456789101112131415161718from bs4 import BeautifulSoup# 声明一个HTML字符串，注意它并不是一个完整的HTML，因为body和html节点没有闭合html = &apos;&apos;&apos;&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&apos;s story&lt;/b&gt;&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&apos;&apos;&apos;soup = BeautifulSoup(html, &apos;lxml&apos;) # 初始化BeautifulSoup对象，同时会更正格式print(soup.prettify()) # 把要解析的字符串以标准的缩进格式输出print(soup.title.string) # 选择title节点，并获取其文本内容 3.2.1. 节点选择器1、选择元素 在BeautifulSoup对象后用 “.” 直接连接节点名即可 1soup.nodename 嵌套选择 在选择了某个节点后，还想基于该节点往下继续选择其子节点或子孙节点，则可以进行嵌套选择 1234html = &quot;&quot;&quot;&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&quot;&quot;&quot; 基于head节点获取title节点：`soup.head.title` 关联选择 在做选择的时候，有时候不能做到一步就选到想要的节点元素，需要先选中某一个节点元素，然后以它为基准再选择它的子节点、父节点、兄弟节点 （1）子节点和子孙节点 获取直接子节点可以调用contents属性 123456789101112131415161718html = &quot;&quot;&quot;&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse&apos;s story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot; 获取p节点的所有直接子节点 12345soup.p.contents[&apos;\n Once upon a time there were three little sisters; and their names were\n &apos;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;span&gt;Elsie&lt;/span&gt;&lt;/a&gt;, &apos;\n&apos;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;, &apos;\n and\n &apos;, &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;, &apos;\n and they lived at the bottom of a well.\n &apos;] 可以看到返回的结果是列表形式 也可以用children属性得到响应的结果 12for i, child in enumerate(soup.p.children): print(i, child) 如果想要得到所有的子孙节点，可以调用descendants属性 12for i, child in enumerate(soup.p.descendants): print(i, child) （2）父节点和祖先节点 parent属性：获取某个节点的父节点 1soup.a.parent parents属性：获取所有祖先节点 1soup.a.parents （3）兄弟节点 获取同级的节点 next_sibling：下一个兄弟节点 previous_sibling：上一个兄弟节点 next_siblings：后面的所有兄弟节点 previous_siblings：前面的所有兄弟节点 2、提取信息 选择好节点后，需要获得该节点的信息 1soup.nodename.atrrs[&apos;attr1&apos;] 3、获取文本内容 1soup.nodename.string 3.2.2. 方法选择器 find_all ( ) 查询所有符合条件的元素 1find_all(name,attrs,recursive,text) name 按照节点名来查询，例如：name=&quot;ul&quot; attrs 按照属性来查找，例如：attrs={&#39;id&#39;:&#39;list-1&#39;} text 用来匹配节点的文件，可以是字符串也可以是正则表达式对象，例如：attrs=&#39;abc&#39;或attrs=re.compile(&#39;.*?&#39;) find ( ) find( )方法返回第一个匹配的元素 还有许多其他的查询方法，其用法与前面提到的find_all( )、find( )方法完全相同，只不过查询范围不同 find_parents( )和find_parent( ) find_next_siblings( )和find_next_sibling( ) find_previous_siblings( )和find_previous_sibling( ) find_all_next( )和find_next( ) find_all_previous( )和find_previous( ) 3.3. 使用pyquery如果你比较喜欢用CSS选择器，如果你对jQuery有所了解，那么有一个更适合你的解析库 —— pyquery 3.3.1. 初始化1、字符串初始化 123from pyquery import PyQuery as pqdoc=pq(html) 2、URL初始化 123from pyquery import PyQuery as pqdoc=pq(url=&apos;https://www.baidu.com&apos;) 与下面的功能相同： 1234from pyquery import PyQuery as pqimport requestsdoc=pq(requests.get(&apos;https://www.baidu.com&apos;).text) 3、文件初始化 123from pyquery import PyQuery as pqdoc=pq(filename=&apos;demo.html&apos;) 3.3.2. 基本CSS选择器123456789101112131415html = &apos;&apos;&apos;&lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&apos;&apos;&apos;from pyquery import PyQuery as pqdoc = pq(html)print(doc(&apos;#container .list li&apos;)) # 选择id为container的节点，然后再选择其内部的class为list的内部节点的所有li节点print(type(doc(&apos;#container .list li&apos;))) 3.3.3. 查找节点1、子节点 查找子节点需要用到find( )方法，此时传入的参数是CSS选择器 12345678from pyquery import PyQuery as pqdoc = pq(html)items = doc(&apos;.list&apos;) # 使用CSS选择器，选取class为list的节点print(type(items))print(items)lis = items.find(&apos;li&apos;) # 在所有子孙节点中寻找li节点print(type(lis))print(lis) find( )的查找范围为节点的所有子孙节点 如果我们只想找子节点，需要使用children( )方法 1lis = items.children(&apos;.active&apos;) # 筛选子节点中class为active的节点 2、父节点 parent( ) 方法：获取某个节点的父节点 1items.parent() parents( ) 方法：获取所有祖先节点 1items.parents() 3、兄弟节点 siblings( )方法：获取兄弟节点 1items.siblings() 4、遍历 pyquery的选择结果可能是多个节点，对于多个节点的结果，我们需要遍历来获取。需要用到items( )方法： 123456from pyquery import PyQuery as pqdoc = pq(html)lis = doc(&apos;li&apos;).items() # 调用items( )方法后，会得到一个生成器，遍历一下就可以逐个得到li节点对象print(type(lis))for li in lis: print(li, type(li)) 3.3.4. 获取信息1、获取属性 调用attr( ) 方法 1a.attr(&apos;href&apos;) 当选中多个元素，然后调用attr( )方法，只会返回第一个元素的属性，如下： 1234567891011121314151617181920212223html = &apos;&apos;&apos;&lt;div class=&quot;wrap&quot;&gt; &lt;div id=&quot;container&quot;&gt; &lt;ul class=&quot;list&quot;&gt; &lt;li class=&quot;item-0&quot;&gt;first item&lt;/li&gt; &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0 active&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-1 active&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;&apos;&apos;&apos;from pyquery import PyQuery as pqdoc = pq(html)a = doc(&apos;a&apos;)print(a, type(a))print(a.attr(&apos;href&apos;))&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;a href=&quot;link3.html&quot;&gt;&lt;span class=&quot;bold&quot;&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt; &lt;class &apos;pyquery.pyquery.PyQuery&apos;&gt;link2.html 这时，如果想要获取所有a节点的属性，需要用到前面提到的遍历： 12345from pyquery import PyQuery as pqdoc = pq(html)a = doc(&apos;a&apos;)for item in a.items(): print(item.attr(&apos;href&apos;)) 2、获取文本 text( )方法：获取内部的文本 html( )方法：获取内部的HTML文本 4. 文件存储4.1. JSON一个JSON对象： 123456789[&#123; &quot;name&quot; : &quot;Bob&quot;, &quot;gender&quot; : &quot;male&quot;, &quot;birthday&quot; : &quot;1992-10-18&quot;&#125;,&#123; &quot;name&quot; : &quot;Selina&quot;, &quot;gender&quot; : &quot;female&quot;, &quot;birthday&quot; : &quot;1995-10-18&quot;&#125;] 注意：JSON数据需要用双引号包围 读取JSON数据 123456789101112131415import jsonstr = &apos;&apos;&apos;[&#123; &quot;name&quot; : &quot;Bob&quot;, &quot;gender&quot; : &quot;male&quot;, &quot;birthday&quot; : &quot;1992-10-18&quot;&#125;,&#123; &quot;name&quot; : &quot;Selina&quot;, &quot;gender&quot; : &quot;female&quot;, &quot;birthday&quot; : &quot;1995-10-18&quot;&#125;]&apos;&apos;&apos;data = json.loads(str) # 使用loads( )方法，将JSON文本字符串转为JSON对象 JSON对象其实可以看作是一个列表，要获取JSON对象中的内容，使用列表的索引即可 123# 以下两种方法均可以获得键值data[0][&apos;name&apos;]data[0].get(&apos;name&apos;) # 推荐该方法，这样如果键名不存在，也不会报错，而是会返回None，也可以传入第二个参数，作为键名不存在时返回的默认值 输出JSON 可以调用 dump( )方法，将JSON对象转化为字符串 1json.dump(data) 如果想保存JSON格式，可以再加一个参数indent，代表缩进字符个数 1json.dump(data, indent=2) 如果JSON中包含中文字符，还需要指定参数 ensure_ascii 为 False，另外还要规定文本输出的编码： 12with open(&apos;data.json&apos;, &apos;w&apos;, encoding=&apos;utf-8&apos;) as f: file.write(json.dump(data, indent=2, ensure_ascii=False)) 4.2. CSV 写入 1234567import csvwith open(&apos;data.csv&apos;, &apos;w&apos;, encoding=&apos;utf-8&apos;) as f: writer = csv.writer(f) # 初始化写入对象 writer.writerow([&apos;id&apos;,&apos;name&apos;,&apos;age&apos;]) writer.writerow([&apos;10001&apos;,&apos;MIke&apos;,20]) ... 默认以逗号分隔，可以指定分隔符，需要传入 delimiter 参数 1writer = csv.writer(f,delimiter=&quot; &quot;) # 初始化写入对象，以空格作为分隔符 一般情况下，爬虫抓取的都是结构化数据，一般会用字典表示，在csv库中也提供了字典的写入方式： 12345678import csvwith open(&apos;data.csv&apos;, &apos;w&apos;, encoding=&apos;utf-8&apos;) as f: fieldnames = [&apos;id&apos;,&apos;name&apos;,&apos;age&apos;] writer = csv.DicWriter(f,fieldnames=fieldnames) writer.writeheader() writer.writerow(&#123;&apos;id&apos;:&apos;10001&apos;,&apos;name&apos;:&apos;Bob&apos;,&apos;age&apos;:&apos;20&#125;) ... 参考资料： (1) 崔庆才 《Python3网络爬虫开发实战》 (2) 简书：Splash使用初体验]]></content>
      <categories>
        <category>Data-Mining</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习笔记：树莓派用户指南（第3版）]]></title>
    <url>%2F2019%2F04%2F02%2FRaspi-User-Guid-Note%2F</url>
    <content type="text"><![CDATA[Linux系统管理GUI与控制台切换加载GUI： startx 控制台：Ctrl+Alt+F1 使用外部存储设备1. 查看已连接驱动器列表： sudo fdisk -l /dev/sdXN中，X是驱动器号，N是分区编号 2. 创建挂载点： sudo mkdir /media/externaldrive 3. 开放用户访问权限： 1234# 设置该文件夹所归属的组chgrp -R group /media/externaldrive# 给组分配该文件夹的写权限chmod -R g+w /media/externaldrive 4. 挂载USB存储设备： 1mount /dev/sdXN /media/externaldrive -o=rw 磁盘物理分布第一个分区很小，为VFAT格式，与Windows使用的分区一致，可被windows识别。该分区挂载在/boot目录下 第二个分区大很多，是EXT4格式（Linux原生文件系统） 软件管理 更新apt缓存： apt-get update 查找软件： apt-cache search keyword 安装软件： apt-get install app 卸载软件： apt-get remove app或apt-get purge app remove与purge区别： remove 留下软件配置文件purge 删除所有相关文件 升级软件，有两种方法： 全部更新：apt-get upgrade升级单个软件包，再次安装该软件即可 故障排查鼠标与键盘常见问题：键盘重复某些字符 可能原因： 自身需要较大功率 内部芯片与电脑USB接口电路冲突 解决方法：更换合适的键盘鼠标 供电供电诊断方法：在Model B+ 中可以用电源指示灯做电压测试，如果指示灯闪烁或熄灭，表明当前电源供电不足4.65V 启动大多数情况下不能启动是由于SD卡的问题。 如果接上micro-USB供电后，电源指示灯亮，而ACT指示灯不亮，说明SD卡存在问题。 网络诊断工具：ifconfig Link encap：网络封装类型，物理网络端口显示为Ethernet Hwaddr： Mac地址 inet addr： IP地址 Mask：网络掩码，用于控制网络用户的最大量 关闭和重启网络端口： 1234# 关闭网络端口ifdown eth0# 重启网络端口ifup eth0 测试网络，通过发送数据到远程计算机并等待一个相应： 1ping -c -l www.raspberrypi.org 紧急内核树莓派有两个内核： 默认内核：/boot/kernal.img 紧急内核：/boot/kernal-emergency.img 加载紧急内核的方法：编辑/boot/config.txt，在文件中添加以下命令 1kernal=kernal-emergency.img 网络配置有线网络在某些情况下，树莓派所在的网络中不包含DHCP服务器，此时若要使用树莓派，就要手动配置树莓派网络。主要是编辑/etc/network/interfaces文件： 我们要编辑的那行以iface eth0 inet开始，首先用static替换该行最后的dhcp，然后回车开始新的一行，并将以下内容格式黏贴进来： 123address xxx.xxx.xxx.xxx # 指定的静态IP地址netmask xxx.xxx.xxx.xxx # 子网掩码gateway xxx.xxx.xxx.xxx # 网关，即路由器或调制解调器的IP地址 然后重启网络生效： 12ifdown eth0ifup eth0 如果要重新启用DHCP服务自动获取IP地址，则恢复回/etc/network/interfaces原来状态，然后重启网络即可 手动配置网络，设为静态IP后，将无法获得完整的DNS解析服务，即无法将网址解析为IP，这个时候需要手动修改设置DNS服务器，即修改/etc/resolv.conf，按照以下格式写入域名与IP的对应信息： 1nameserver 8.8.8.8 无线网络通过终端接入无线网路扫描周边的无线接入点： 1$ iwlist scan 用iwconfig检查当前网络状态。和ifconfig不同，iwconfig专门为无线网路而设计 网路设备连接名称：树莓派默认的无线连接名称为wlan0 无线标准：无线网卡支持的无线标准，图中支持的标准为IEEE 802.11bgn ESSID：无线网卡连接到的网络名称。如果未连接，则显示为off/any Mode：网卡当前模式。可选模式有： Managed：常规模式 Ad-Hoc：设备到设备无线网络 Monitor：监听网络传输的特殊模式，常用于网络查错 Repeater：增强网络信号的中继模式 Secondary：Repeater模式的一种，此时网卡作为备用中继器使用 将树莓派连如无线网络，需要在/etc/network/interfaces文件中加入以下几行： 1234auto wlan0 # 这一行是设定开机启动wifi连接，非必须项iface wlan0 inet dhcp# 指定配置文件，如果已经存在，则指向它，如果不存在则先指向一个空文件，随后再创建该文件wpa-conf /etc/wpa.conf 接着编辑wpa-conf指定的配置文件，在文件中添加以下要接入的无线网络的信息： 12345network=&#123; ssid=&quot;your_ssid&quot; key_mgmt=WPA-PSK psk=&quot;your_passwd&quot;&#125; 然后连接无线网络。 12ifdown wlan0ifup wlan0 接入外网我们的目的是将它作为一台永久运行的个人服务器，随时随地可以远程登录它。 要解决的问题实际上就是如何让外网访问局域网内的主机。通过一根网线把一台路由器和外网相连，路由器发射信号，手机、电脑等设备通过有线或无线的方式连接路由器，这些设备与路由器构成一个局域网。路由器与外网直接相连，被分配了一个公网IP，所以可被外网中的设备检测到，而局域网内的设备被分配的IP是由路由器生成的，不能被外网的设备检测到，所以外网设备无法访问在内网中搭建的服务器。 简单来说，就是对外网设备来说，有公网IP的路由器可见，而无公网IP的路由器内网不可见。解决方法就是：利用路由器的虚拟服务器功能将内网的服务器端口映射到路由器的公网IP上，这样外网设备通过路由器的公网IP找到了路由器，然后再根据端口找到了连接在该路由器下的内网服务器，从而实现服务器对外开放。 1.路由器静态分配内网IP不同的路由器，设置方法会有差异，我的路由器为MERCURY（水晶）。 设置静态分配。添加一个静态分配的规则，添加树莓派的MAC地址和静态分配的IP（要在局域网的网段，通常是192.168.1.XXX，而且不要在路由器的地址池范围内），我使用的是192.168.1.20. 2.外网IP和端口映射到内网如果想要从外网登陆的话，还需要做一步映射。因为路由器分出了很多ip，如果外网访问某一端口的话，到底是访问了哪一个ip呢？ 这时要添加映射规则。此处添加服务器的IP地址、端口以及映射的协议类型，请根据您的服务器实际端口填写。 内网ssh服务器的端口为22，所以内部端口应该设为22。 而对于外部端口的设置，由于部分运营商可能会屏蔽80等常用端口，导致无法访问对应服务器，所以建议修改9000以上，外网用户使用修改后的服务端口访问服务器。实际中我将外部端口与内部端口都设为22。 路由器设置好后，需要重启路由器才能让刚才的设置生效。 这样就可以用 路由器IP + 映射对应端口 在公网中成功访问局域网服务器了。 3. 向外网开放多个端口内网服务器的一个端口对应着一种服务，比如如果想向外网开放ssh服务，则用ssh的22端口做端口映射，实现对外开放ssh服务；如果想开放Web服务，则需要用Web服务器的80端口做端口映射。 如果只用22端口做端口映射，对外开放的只是ssh服务器服务，而无法获取Web服务。 但是如果想同时对外网开放ssh服务和Web服务，怎么办呢？同时开启两个端口映射任务，将两个端口同时映射到同一个内网服务器IP： 这样，就可以分别用 路由器IP:9000 访问Web服务器，用 路由器IP:22 访问SSH服务器 用户与权限管理创建新用户先创建一个用户应该归属的组： 1$ sudo groupadd Guest 创建一个归属于Guest组的新用户： 12$ sudo useradd -d /home/guest1 -m -s /bin/sh guest1$ sudo passwd guest1 useradd选项 选项 助记 说明 文件及字段 -c Comment 注释信息 /etc/passwd， 5 -d Directory 账户的家目录 /etc/passwd， 6 -e Expire 账号终止日期（YYYY-MM-DD） /etc/shadow， 8 -f — 帐号过期几日后永久停用 /etc/shadow， 7 -g Gid 初始默认组 /etc/passwd， 4 -G Groups 附属次要组 /etc/group， 4 -m — 家目录不存在时自动创建 — -M — 强制不创建家目录 — -s Shell 默认 shell /etc/passwd， 7 -u Uid 指定用户 UID /etc/passwd， 3 SUDOERS（sudo用户）树莓派上默认的pi用户是一个sudoer（sudo用户）。这种类型的用户在命令行上输入命令之前先输入sudo就能拥有root权限执行命令，也能够通过sudo su完全切换到root用户。 添加一个用户到sudoer用户组，需要使用一个sudo用户输入sudo visudo命令打开配置文件，找到文件中# User privilege specification下面的root ALL=(ALL:ALL) ALL。复制这一行并将其中的root替换为对应的用户名。为了免验证密码使用root权限，修改为NOPASSWD:ALL。下面的例子给予了bob用户免密码访问sudo权限： 123# User privilege specification root ALL=(ALL:ALL) ALL bob ALL = NOPASSWD: ALL 保存并退出以完成修改。千万小心，因为这些操作可能会意外移除你自己的sudo权限。 搭建Web服务器LAMP大部分的现代Web服务器是运行在Linux操作系统、Apache、MySQL和PHP的一个组合，简称LAMP Linux 底层操作系统 MySQL 后台数据库 Apache Web服务器 PHP 动态网页脚本语言 安装LAMP 1$ sudo apt-get install apache2 php5 php5-mysql mysql-server MySQL在安装过程中会要求你输入密码，然后才继续安装。 测试Apache是否安装成功。如果你和你的Web服务器处于同一个局域网内，则可以在浏览器中输入Web服务器的IP，就会进入Apache的默认界面 Web服务器的默认工作目录由 /etc/apache2/sites-enabled/*.conf 配置文件设定，可以修改该文件中的 DocumentRoot 一项来自定义工作目录，修改后需要重启服务器才能生效。 接着确认PHP脚本模块是否在Apache服务器正确装载，在Web服务器的默认工作目录下创建一个phptest.php文件： &lt;?php phpinfo(); ?&gt;。然后在浏览器中访问该php文件：http://ip/phptest.php WordPressWordPress是目前世界上最流行的开源博客平台之一。WordPress建立在PHP和JavaScript的基础上，并提供一个美观的、用于创建各种网站的、基于Web的界面。 安装： 1$ sudo apt-get install wordpress 为了让Apache服务器能找到需要的文件，需要将WordPress的安装目录 /usr/share/wordpress 链接到Apache的工作目录下。 1$ sudo ln -s /usr/share/wordpress /var/www/wordpress 接下来，使用下面一行命令运行WordPress的MySQL配置脚本： 12$ sudo gunzip /usr/share/doc/wordpress/examples/setup-mysql.gz$ sudo bash /usr/share/doc/wordpress/examples/setup-mysql -n wordpress localhost 这个命令添加了一个新的数据库到MySQL，以提高WordPress使用的数据库。此数据库存储你的用户账户、文章、评论和其他相关信息。 一旦这个脚本完成，你将被告知在浏览器中访问 http://localhost 来继续完成安装。这个地址其实不完整，应该是 http://localhost/wordpress。你也可以在内网中的另外一台计算机的浏览器地址栏输入 http://raspi-ip/wordpress。 将Web服务器接入外网将树莓派Web服务器接入外网的方法与 上文 一样。 不过 Python编程贪食蛇123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110#!/user/bin/env pythonimport pygame, sys, time, randomfrom pygame.locals import *# 启动pygamepygame.init() # pygame初始化fpsClock = pygame.time.Clock() # 控制游戏的速度# 创建一个pygame游戏显示层playSurface = pygame.display.set_mode((640, 480))pygame.display.set_caption(&apos;Raspberry Snake&apos;)# 定义一些颜色redColour = pygame.Color(255, 0, 0)blackColour = pygame.Color(0, 0, 0)whiteClour = pygame.Color(255, 255, 255)greyColour = pygame.Color(150, 150, 150)# 初始化一些程序中用到的变量snakePosition = [100,100]snakeSegments = [[100,100],[80,100],[60,100]]raspberryPosition = [300,300]raspberrySpawned = 1direction = &apos;right&apos;changeDirection = direction# 定义游戏结束game over时的任务：用大号字体将“Game Over”打印在屏幕上，停留5秒，然后退出def gameOver(): gameOverFont = pygame.font.font(&apos;freesansbold.ttf&apos;, 72) gameOverSurf = gameOverFont.render(&apos;Game Over&apos;, True, greyColour) gameOverRect = gameOverSurf.get_rect() gameOverRect.midtop = (320, 10) playSurface.blit(gameOverSurf, gameOverRect) pygame.display.flip() time.sleep(5) pygame.quit() sys.exit()while True: # 获取用户的键盘输入，用小键盘或WASD键来输入方向 for event in pygame.event.get(): if event.type == QUIT: pygame.quit() sys.exit() elif event.type == KEYDOWN: if event.key == K_RIGHT or event.key == ord(&apos;d&apos;): changeDirection = &apos;right&apos; if event.key == K_LEFT or event.key == ord(&apos;a&apos;): changeDirection = &apos;left&apos; if event.key == K_UP or event.key == ord(&apos;w&apos;): changeDirection = &apos;up&apos; if event.key == K_DOWN or event.key == ord(&apos;s&apos;): changeDirection = &apos;down&apos; if event.key == K_ESCAPE: pygame.event.post(pygame.event.Event(QUIT)) # 判断用户输入方向的合法性，若合法则更新当前运动方向 if changeDirection == &apos;right&apos; and not direction == &apos;left&apos;: direction = changeDirection if changeDirection == &apos;left&apos; and not direction == &apos;right&apos;: direction = changeDirection if changeDirection == &apos;up&apos; and not direction == &apos;down&apos;: direction = changeDirection if changeDirection == &apos;down&apos; and not direction == &apos;up&apos;: direction = changeDirection # 蛇运动，即更新蛇头位置 if direction == &apos;right&apos;: snakePosition[0] += 20 if direction == &apos;left&apos;: snakePosition[0] -= 20 if direction == &apos;up&apos;: snakePosition[1] -= 20 if direction == &apos;down&apos;: snakePosition[1] += 20 # 使蛇身体增长，若未吃到草莓则去掉尾，若吃到草莓则保留尾，实现蛇的运动 snakeSegments.insert(0,list(snakePosition)) if snakePosition[0] == raspberryPosition[0] and snakePosition[1] == raspberryPosition[1]: raspberrySpawned = 0 else: snakeSegments.pop() # 若草莓被吃掉，则在随机位置再生成一个草莓 if raspberrySpawned == 0: x = random.randrange(1,32) y = random.randrange(1,24) raspberryPosition = [int(x*20),int(y*20)] raspberrySpawned = 1 # 在界面上画东西 playSurface.fill(blackColour) # 填充背景为黑色 for position in snakeSegments: # 画蛇 pygame.draw.rect(playSurface,whiteColour,Rect(position[0], position[1], 20, 20)) pygame.draw.rect (playSurface,redColour,Rect(raspberryPosition[0], raspberryPosition[1], 20, 20)) # 画草莓 pygame.display.flip() # 更新界面，呈现新的画面 # 判断蛇是否出界 if snakePosition[0] &gt; 620 or snakePosition[0] &lt; 0: gameOver() if snakePosition[1] &gt; 460 or snakePosition[1] &lt; 0: gameOver() # 判断蛇是否碰到自己 for snakeBody in snakeSegments[1:]: if snakePosition[0] == snakeBody[0] and snakePosition[1] == snakeBody[1]: gameover() fpsClock.tick(30) # 控制游戏速度 网络连接12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#!/usr/bin/env python# IRC Channel Checker, written for theRaspberry Pi User Guide by Tom Hudsonimport sys, socket, time# IRC状态码RPL_NAMREPLY = &apos;353&apos;RPL_ENDOFNAMES = &apos;366&apos;# 设置连接服务器需要的变量irc = &#123; &apos;host&apos; : &apos;chat.freenode.net&apos;, &apos;port&apos; : 6667, &apos;channel&apos; : &apos;#raspiuserguide&apos;, &apos;namesinterval&apos; : 5&#125; # 设置存储用户信息的变量user = &#123; &apos;nick&apos; : &apos;botnick&apos;, &apos;username&apos; : &apos;botuser&apos;, &apos;hostname&apos; : &apos;localhost&apos;, &apos;servername&apos; : &apos;localhost&apos;, &apos;realname&apos; : &apos;Raspberry Pi Names Bot&apos;&#125; # 创建一个socket对象，提供程序所需的网络连接s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)# 连接IRC服务器print &apos;Connecting to %(host)s:%(port)s...&apos; % irc try: s.connect((irc[&apos;host&apos;], irc[&apos;port&apos;])) except socket.error: print &apos;Error connecting to IRC server %(host)s:%(port)s&apos; % irc sys.exit(1)# 向IRC服务器发送用户的身份信息，来鉴定用户身份s.send(&apos;NICK %(nick)s\r\n&apos; % user)s.send(&apos;USER %(username)s %(hostname)s %(servername)s :%(realname)s\r\n&apos; % user)s.send(&apos;JOIN %(channel)s\r\n&apos; % irc)s.send(&apos;NAMES %(channel)s\r\n&apos; % irc)# 从socket获取数据read_buffer = &apos;&apos; # 初始化缓冲区names = []while True: read_buffer += s.recv(1024) # 一次性读入1kb数据 lines = read_buffer.split(&apos;\r\n&apos;) # 以\r来分割字符串 read_buffer = lines.pop(); # 保留缓冲区中的最后一行，可能为不完整行 for line in lines: # 从这些行中提取需要的信息 response = line.rstrip().split(&apos; &apos;, 3) response_code = response[1] if response_code == RPL_NAMREPLY: names_list = response[3].split(&apos;:&apos;)[1] names += names_list.split(&apos; &apos;) if response_code == RPL_ENDOFNAMES: # Display the names print &apos;\r\nUsers in %(channel)s:&apos; % irc for name in names: print name names = [] # 设置每个请求之间的等待间隔，防止IRC服务器在短时间内收到太多的请求而导致被强制断开连接 time.sleep(irc[&apos;namesinterval&apos;]) s.send(&apos;NAMES %(channel)s\r\n&apos; % irc) 参考资料： (1) 树莓派用户指南（第3版） (2) [MW315R V1] 如何设置虚拟服务器？ (3) 树莓派用户管理]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Raspi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Perl进阶笔记]]></title>
    <url>%2F2019%2F04%2F01%2FPerl-Advanced%2F</url>
    <content type="text"><![CDATA[pod文档使用pod文档可以实现程序usage说明 12345678910111213=head1 part1 doc in part1=head2 part2 doc in part2...=cut # pod文档结束的标志 注意：每个=标签上下必须隔一行，否则就会错误解析。 用pod2doc $0可以将程序中的文档打印出来，不过一般用在程序内部，当程序参数设定错误时打印pod文档： 1die `pod2doc $0` if (...); Getopt::Long首先需要在脚本开头加上对该模块的引用 1use Getopt::Long; 使用GetOptions函数承接传递的参数： 1234567my ($var1,$var2,$var3,$var4); # 若使用&quot;use strict&quot;模式，则需要提前定义变量GetOptions( &quot;i:s&quot;=&gt;\$var1, &quot;o:s&quot;=&gt;\$var2, &quot;n:i&quot;=&gt;\$var3, &quot;m:i&quot;=&gt;\$var4 ); perl单行执行perl -h可以查看perl的所有参数使用说明，使用perl单行需要使用到-e参数 1$ perl -e &apos;print &quot;hello world!\n&quot;&apos; perl单行常用的场景为进行文本的逐行读取并操作，即隐式地开启while(&lt;&gt;)，需要使用-n参数 12345perl -ne &apos;BEGIN&#123;&#125; ... END&#123;&#125;&apos;filename BEGIN 和 END 区块根据需要进行添加 若需要在逐行读取的同时，自动将行中的元素打散 (split)，默认以\s（空字符，即空格或制表符）作为分割符，则需要使用-a参数，相当于在执行了@F = split $_，打散后的元素会保存在@F中 使用Hash遇到的坑在编写perl脚本的过程中，我们常常会将读入文件一行中的某两项（一行可能有多列，列与列之间用制表符\t隔开）作为相对应的两项，分别作为Hash的键（key)和值（value)，由于Hash这种数据结构要求key是唯一，而value可以重复，因此一般将唯一的那一项作为key，另一项作为value 但是，由于字符串首末端空字符的存在会导致一个意想不到的情况： 创建了两个Hash，让它们的key是一一对应的，而各自存储的value不同，当时在某些key字符串首末端混入了空字符，例如Hash1有一个key为&quot;KEY&quot;，Hash2有一个key为&quot;KEY &quot;，它们本来应该是一样的，但是由于空字符的存在，它们现在不一样了 这时候的解决方法是在构建Hash之前，不论实际的字符串的首末端有没有空字符串，都尝试将这些空字符串去掉： 12345678# 假设读入的文件只有用制表符隔开的两列while(&lt;IN&gt;)&#123; chomp; @recorder = split /\t/; $recorder[0] =~ s/(^\s+)|(\s+$)//g; # 去除开头和末尾的空字符串 $recorder[1] =~ s/(^\s+)|(\s+$)//g; # 去除开头和末尾的空字符串 $hash&#123;$recorder[0]&#125; = $recorder[1];&#125; Hash中的排序操作对key进行排序 其基本的语法结构为： 1sort &lt;排序规则&gt; &lt;排序对象&gt; 若要对keys进行排序则排序对象就是keys，所以最后一项要写成keys %hash 123456789# 按value排序## 对hash的keys按hash value排序（按ASCII码排序）sort &#123; $hash&#123;$a&#125; cmp $hash&#123;$b&#125; &#125; keys %hash## 对hash的keys按hash value排序（按数字大小排序）sort &#123; $hash&#123;$a&#125; &lt;=&gt; $hash&#123;$b&#125; &#125; keys %hash# 按key排序# 对hash的keys按hash key排序sort &#123;$a&lt;=&gt;$b&#125; keys %hash 安装Perl模块查看perl模块的安装目录，主要就是@INC这个默认变量 1perl -e &apos;&#123;print &quot;$_\n&quot; foreach @INC&#125;&apos; 若要临时添加perl模块的安装目录，则在perl脚本中shebang（#!/usr/bin/perl）后紧接着添加push(@INC,&quot;...&quot;);命令，若是在perl单行中，则写成BEGIN{push(@INC,&quot;...&quot;);} 若是要永久添加perl模块的安装目录，则修改PERL5LIB环境变量即可： 1export PERL5LIB=/PATH/TO/LIB 查看已安装的Perl模块： 1234# 查看系统中安装的Perl模块find `perl -e &apos;print &quot;@INC&quot;&apos;` -name &apos;*.pm&apos;# 查看当前环境下所有的模块（一般为用户自己安装的）instmodsh 查询单个perl模块的安装路径： 1perldoc -l Getopt::Long 查看安装的perl模块的版本号 1perl -MGetopt::Long -e &apos;print Getopt::Long-&gt;VERSION. &quot;\n&quot;&apos; 装Perl模块有两种方法 自动安装 (使用CPAN模块自动完成下载、编译、安装的全过程) 手工安装 (去CPAN网站下载所需要的模块，手工编译、安装) 使用CPAN模块自动安装首先你得已经安装了CPAN，若没有执行以下命令： 1# yum install perl-CPAN 安装前需要先联上网，有无root权限均可 1$ perl -MCPAN -e shell 1234cpan&gt;helpcpan&gt;mcpan&gt;install Net::Servercpan&gt;quit 查询：cpan[1]&gt; d /模块名字或者部分名字/ 查询结果中会给出所有含有模块名字或者部分名字的模块，选择您所需要的模块进行下载 下载安装：cpan[1]&gt; install 模块名字 同时会自动安装很多依赖的模块，非常方便。 手工安装 一般情况下不推荐这种安装方式，但是总是会有迫不得已的时候，而且尝试这种方式，能加深对perl模块的理解。 比如从 CPAN下载了Net-Server模块0.97版的压缩文件Net-Server-0.97.tar.gz，假设放在/usr/local/src/下。 12345cd /usr/local/srctar xvzf Net-Server-0.97.tar.gzcd Net-Server-0.97perl Makefile.PLmake test 如果测试结果报告all test ok，你就可以放心地安装编译好的模块了。 非root用户的另一个解决方案手动下载local::lib, 这个perl模块，然后自己安装在指定目录，也是能解决模块的问题！ 下载之后解压，进入： 123perl Makefile.PL --bootstrap=~/.perl ##这里设置你想把模块放置的目录make test &amp;&amp; make installecho &apos;eval $(perl -I$HOME/.perl/lib/perl5 -Mlocal::lib=$HOME/.perl)&apos; &gt;&gt; ~/.bashrc ##目录与前面要一致 等待几个小时即可！！！ 添加好环境变量之后，就可以用 1perl -MCPAN -Mlocal::lib -e &apos;CPAN::install(LWP)&apos; 或有更简单的写法： 1cpanm --local-lib=~/perl5 local::lib &amp;&amp; eval $(perl -I ~/perl5/lib/perl5/ -Mlocal::lib) 循环匹配在李恒的github博客 On the definition of sequence identity 当中看到这样一个Perl单行代码： 1$ perl -ane &apos;if(/NM:i:(\d+)/)&#123;$n=$1;$l=0;$l+=$1 while/(\d+)[MID]/g;print(($l-$n)/$l,&quot;\n&quot;)&#125;&apos; 这行代码的目的是为了计算SAM文件中每条记录BLAST identity BLAST identity是根据你比对到的碱基除去比对所涉及到的columns数目，换句话来说就是比对涉及到所有的碱基数目 例如这样的双序列比对： 1234&gt; Ref+: 1 CCAGTGTGGCCGATaCCCcagGTtgGC-ACGCATCGTTGCCTTGGTAAGC 49&gt; |||||||||||||| ||| || || ||||||||||||||||||||||&gt; Qry+: 1 CCAGTGTGGCCGATgCCC---GT--GCtACGCATCGTTGCCTTGGTAAGC 45&gt; 它们的BLAST identity就是43/50=86% 那么要计算SAM文件中每条reads的BLAST identity，总长可以通过叠加CIGAR中对应的M/I/D的数目得到，比对到的碱基数目等于总长减去NMtag（比对不上的碱基位置的标记） 李恒的这行代码中有一部分一开始没有读懂，就是下图红框中的那部分： 其实这是一种简写方式，正规完整且更容易读懂的形式可以写成下面这样： 123456789101112# 这里为了更好看，添加了适当的换行和缩进$ perl -ane \&apos;if(/NM:i:(\d+)/)&#123; $n=$1; $l=0; while(/(\d+)[MID]/g)&#123; $l+=$1; &#125; print(($l-$n)/$l,&quot;\n&quot;);&#125;&apos; while(/(\d+)[MID]/g)中的正则表达式/(\d+)[MID]/g，引起了我极大的好奇：它是在正则表达式后面添加了一个g字符，即开启了全局匹配，又由于是在while( )中进行的正则匹配，等于是开启了循环匹配，即对于CIGAR字符串18M3D22M，正则表达式/(\d+)[MID]/g，先会匹配上18M，然后会匹配上3D，最后匹配上22M 很有意思的用法 chomp带来空行匹配的失败我常用正则表达式/^\s+$/来进行文件中空行的匹配 在用perl单行进行文本处理，我喜欢用一个固定的格式： 1$ perl -ne &apos;chomp;...&apos; &lt;input&gt; chomp在这里的作用是在每读入一行后，去除末尾的换行符 此时如果你要匹配的空行是/^\n/形式的，即这行只有一个换行符，被chomp处理过之后这一行就变成了/^$/形式的空字符的行，此时如果再用正则表达式/^\s+$/进行匹配，就会匹配失败 那么遇到这种情况应该怎么匹配呢？ 使用/^\s?$/即可，元字符?的作用是匹配前面的字符0到多次 参考资料： (1) 生信菜鸟团：perl模块安装大全 (2) Heng Li’s blog: On the definition of sequence identity (3) 【简书】生信杂谈：怎样定义sequences比对的相似度？]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>Perl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周一道初中数学题]]></title>
    <url>%2F2019%2F04%2F01%2FMath-Subject-per-Week%2F</url>
    <content type="text"><![CDATA[最近在给面临中考的表妹补课，补的是数学和物理（化学尝试过，表示hold不住），补课会遇到了一些还挺有意思，稍有难度的题，就把它们的解题过程记录了下来 人到油腻中年，只能拿这样的东西练练脑子了，主要为了防止老年痴呆 Week-1题目是这样的： 正五边形广场ABCDE的边长为80m，甲、乙两个同学做游戏，分别从A、C两点处同时出发，沿A-B-C-D-E-A的方向绕广场行走，甲的速度为50m/min，乙的速度为46m/min，则两人第一次刚走到同一条边上时，（ ） A. 甲在顶点A处 B. 甲在顶点B处 C. 甲在顶点C处 D. 甲在顶点D处 求解思路： 这本质上是一个同向同时的追击问题 想象一下，甲追上乙即两者相遇时的场景，两人第一次刚走到同一条边上的场景肯定发生在两者相遇之前，也就是说题目要求的那个时刻那个场景是发生在追击过程中的 可以很容易地想到，两人第一次刚走到同一条边上时，落在后面的甲肯定刚好落在这个五边形的某一个顶点上，这意味着甲此时刚好完整地走完了x条边，则此时甲移动的路程为80x 由于此时，乙与甲在同一条边上，则可设此时甲乙之间的距离为k，则此时乙移动的路程为80x+k-160 考虑到甲乙是同时运动的，它们的运动时间总是相同的，则 $$\frac{80x}{50}=\frac{80x+k-160}{46}$$ 根据这个公式，可以推出K的表达式为： $$k=ax+b$$ a和b分别是某一个实数，可以通过上面的等式实际算出来，不过这里主要是为了讲解解题的思路，所以没有将实际算出的数字标上 前面提到，此时乙与甲是在同一条边上的，那么它们之间的距离k肯定不能超出这条边的长度，即 $$0 &lt; k = ax+b\leq80$$ 根据上面的不等式就可以算出x的取值范围： $$w_1 &lt; x \leq w_2$$ 则去x在该取值范围的最小正整数，通过它就可以知道此时甲到底位于哪个顶点，比如若此时x去3，则甲位于D点 Week-2题目： 已知抛物线$y=x^2-2x-3$进过点A(-1,b)，P(m,t)是抛物线上的一个动点，P关于原点的对称点为P’，问： 当点P’落在第二象限内，且$P’A^2$取得最小值时，求m的值 求解： 已知点P’是P关于原点的中心对称点，因此P’的坐标可以推出是(-m,-t)，也可以求出点A的坐标为(-1,0) 又由于点P’落在第二象限，因此： $$\begin{cases}-m0\end{cases}\to\begin{cases}m&gt;0 \\t&lt;0\end{cases}$$ 因为已知点A的坐标，也知道点P’坐标的表示方式，则可以将$P’A^2$表示出来： $$\begin{aligned}P’A^2 &amp;=(-m+1)^2+t^2 \\&amp;=m^2-2m+1+t^2\end{aligned}$$ 虽然我们用m和t将$P’A^2$表示了出来，但是两个未知数还是太多，需要想办法消去其中一个，这时可以利用点P在抛物线上这个条件，则 $$m^2-2m-3=t \to m^2-2m=t+3$$ 将这一步得到的结论$m^2-2m=t+3$带入上一步的结论中，从而得到 $$P’A^2=t^2+t+1 (t&lt;0)$$ 因此，此时题目的问题变成了：当t&lt;0时，$P’A^2=t^2+t+1$在何处取到最小值？ 很明显，抛物线$t^2+t+1$的开口向上，且其对称轴为$t=-\frac{1}{2}$，在抛物线的取值范围t&lt;0上，因此t取$-\frac{1}{2}$ 已经得到t的取值，将它带到抛物线表达式中，就可以将m求出了]]></content>
      <categories>
        <category>MathIsFunGame</category>
      </categories>
      <tags>
        <tag>初中数学</tag>
        <tag>智力游戏</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性代数学习笔记]]></title>
    <url>%2F2019%2F03%2F31%2FLinear-Algebra-Note%2F</url>
    <content type="text"><![CDATA[1. 理解向量与向量空间2. 理解矩阵与矩阵乘法2.1. 矩阵表示法和矩阵乘法规则是怎么来的？2.1.1. 求解线性方程组：高斯消元法电视的转播过程是这样的： 因此从电视信号线传过来的是YCrCb三个颜色通道的数字信号，此时如果使用的是彩色电视，就需要 $$YCrCb \to^{转换} RGB$$ 这种信号编码方式的转换本质上就是在解方程组： $$\begin{cases}0.299R &amp; + &amp; 0.587G &amp; + &amp; 0.114B &amp; = &amp; Y \\0.500R &amp; - &amp; 0.419G &amp; - &amp; 0.081B &amp; + &amp; 128 &amp; = &amp; Cr \\-0.169R &amp; - &amp; 0.331G &amp; + &amp; 0.500B &amp; + &amp; 128 &amp; = &amp; Cb\end{cases}$$ 那么如何解这个线性方程组呢？ 我们大家都学过的一种比较通用的方法就是高斯消元法 得到最终结果： $$\begin{cases}x &amp; + &amp; 0 &amp; + &amp; 0 &amp; = &amp; \frac{e_3}{a_{11}} \\0 &amp; + &amp; y &amp; + &amp; 0 &amp; = &amp; \frac{f_3}{b_{22}} \\0 &amp; + &amp; 0 &amp; + &amp; z &amp; = &amp; \frac{g_3}{c_{33}}\end{cases}$$ 2.1.2. 简化线性方程组的表示——得到原始的矩阵定义当然，解线性方程组使用高斯消元法，基本上就是最优的求解方法，但是整个求解过程若按照上面这样去表示，表示起来是比较复杂的 因此有一个英国的数学家叫阿瑟·凯莱就提出用矩阵去表示线性方程组，以及线性方程组的求解过程 以一个简单的线性方程组为例进行说明： $$\begin{cases}x &amp; + &amp; 2y &amp; = &amp; 3 \\3x &amp; + &amp; 4y &amp; = &amp; 5\end{cases}$$ 对于上述方程组，未知数x，y根本不重要，所以可以用一种称为矩阵的紧凑的阵列来表示，把未知数的系数提出来： $$\begin{bmatrix}1 &amp; 2 \\3 &amp; 4\end{bmatrix}$$ 称为系数矩阵，而把等号右边的数字一起提出来： $$\begin{bmatrix}1 &amp; 2 &amp; 3 \\3 &amp; 4 &amp; 5\end{bmatrix}$$ 称为增广矩阵 2.1.3. 用矩阵的表示方法表示高斯消元法解线性方程组的过程——得到矩阵乘法的原始定义还是以上面提到的方程组为例进行说明 高斯消元法的目标是进行下面形式的转换： $$\begin{cases}x &amp; + &amp; 2y &amp; = &amp; 3 \\3x &amp; + &amp; 4y &amp; = &amp; 5\end{cases}\to\begin{cases}x &amp; + &amp; 0y &amp; = &amp; ? \\0 &amp; + &amp; y &amp; = &amp; ?\end{cases}$$ 用矩阵表示就是： $$\begin{bmatrix}1 &amp; 2 &amp; 3 &amp; \\3 &amp; 4 &amp; 5 &amp;\end{bmatrix}\to\begin{bmatrix}&amp; 1 &amp; 0 &amp; ? \\&amp; 0 &amp; 1 &amp; ?\end{bmatrix}$$ 我们来看对这个原始方程组用高斯消元法进行消元的第一步 $$\begin{cases}x &amp; + &amp; 2y &amp; = &amp; 3 &amp; 【方程1】 &amp; \\3x &amp; + &amp; 4y &amp; = &amp; 5 &amp; 【方程2】 &amp;\end{cases}矩阵表示为\begin{bmatrix}&amp; 1 &amp; 2 &amp; 3 &amp; \\&amp; 3 &amp; 4 &amp; 5 &amp;\end{bmatrix}$$ 用第一个方程消去第二个方程的第一个系数： $$\frac{\begin{matrix}&amp; -3 &amp; 【方程1】 \\ + &amp; &amp; 【方程2】\end{matrix}}{【新方程2】}$$ 得到 $$\begin{cases}x &amp; + &amp; 2y &amp; = &amp; 3 &amp; \\0x &amp; - &amp; 2y &amp; = &amp; -4 &amp;\end{cases}矩阵表示为\begin{bmatrix}&amp; 1 &amp; 2 &amp; 3 &amp; \\&amp; 0 &amp; -2 &amp; -4 &amp;\end{bmatrix}$$ 我们已经成功尝试利用矩阵来表示方程组了，但是好像对方程组的求解并没有什么用，那么我们能否利用矩阵表示方式来简化方程组的求解过程呢？ 首先，可以将矩阵$\begin{bmatrix}&amp; 1 &amp; 2 &amp; 3 &amp; \\&amp; 3 &amp; 4 &amp; 5 \end{bmatrix}$看作是两个行向量$\begin{bmatrix}&amp; r_1 &amp; \\ &amp; r_2 &amp; \end{bmatrix}$，那么上面的计算可以通过矩阵表示为： $$\begin{bmatrix}&amp; 1 &amp; 2 &amp; 3 &amp; \\&amp; 3 &amp; 4 &amp; 5\end{bmatrix}\begin{matrix}r_2’=-3r_1+r_2 \\\to\end{matrix}\begin{bmatrix}&amp; 1 &amp; 2 &amp; 3 &amp; \\&amp; 0 &amp; -2 &amp; -4 &amp;\end{bmatrix}$$ 这个过程实际上包含了两个步骤： 第一行不变，即：$r_1’ = r_1$ 第二行改变，即：$r_2’=-3r_1+r_2$ 首先第一行不变，即 其次，第二行改变，即 凯莱规定，把第一行运算的结果放在第一行，第二行的结果放在第二行，即 2.1.5. 矩阵乘法的行观点和列观点首先，需要说明一下矩阵乘法的合法性： $m\times n$ 的矩阵只能和 $n\times p$ 的矩阵相乘； 相乘后的矩阵大小为 $m\times p$ 行观点 $$xA=y$$ 称为A右乘x 列观点 $$Ax=y$$ 称为A左乘x 2.2. 矩阵乘法的几何意义矩阵函数是一个向量空间向另一个向量空间的映射 例（一） $$A=\begin{bmatrix}1 &amp; -1 \\1 &amp; 1\end{bmatrix}\quadx=\begin{bmatrix}x_1 \\x_2\end{bmatrix}\quady=\begin{bmatrix}y_1 \\y_2\end{bmatrix}$$ $$Ax=y$$ 则为从$\mathbb{R}^2 \Rightarrow \mathbb{R}^2$ 例（二） $$\begin{bmatrix}1 &amp; -1 \\1 &amp; 1 \\1 &amp; 2\end{bmatrix}\begin{bmatrix}x_1 \\x_2\end{bmatrix}=\begin{bmatrix}y_1 \\y_2 \\y_3\end{bmatrix}$$ 则为从$\mathbb{R}^2 \Rightarrow \mathbb{R}^3$ 2.2.1. 基的变换2.2.1.1. 矩阵映射法则——基的变换在$\mathbb{R}^2$的向量空间中，它的自然基（笛卡尔坐标系）为： $$\vec i=\begin{bmatrix} 1 \\ 0 \end{bmatrix}\quad \vec j=\begin{bmatrix} 0 \\ 1 \end{bmatrix}$$ 令 $A=\begin{bmatrix} 1 &amp; -1 \\ 1 &amp; 1 \end{bmatrix}$ 自然基下向量 $a=\begin{bmatrix} 1 \\ 1 \end{bmatrix}=1 \vec i+1 \vec j$ 则 $Aa=b$ 根据矩阵乘法 $$Aa=\begin{bmatrix}1 &amp; -1 \\1 &amp; 1\end{bmatrix}\begin{bmatrix}1 \\1\end{bmatrix}=1\begin{bmatrix} 1 \\ 1 \end{bmatrix} + 1\begin{bmatrix} -1 \\ 1 \end{bmatrix}=\begin{bmatrix} 0 \\ 2 \end{bmatrix}=b$$ 为了看起来更清晰，我们令 $$\vec c_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix} \quad \vec c_2 = \begin{bmatrix} -1 \\ 1 \end{bmatrix}$$ 则 $A=[\vec c_1 \quad \vec c_2]$，因此$Aa=b$可以表示成以下形式： $$a = 1 \vec i + \vec j \quad \begin{matrix} A \\ \rightarrow \end{matrix} \quad b = 1 \vec c_1 + 1 \vec c_2$$ 从上面很容易能看出，这个矩阵的乘法规则就是：保持系数不变，但是自然基被矩阵列向量给替换了 从几何上感受一下 2.2.1.2. 基变换的一个实例——旋转矩阵通过旋转矩阵$\begin{bmatrix} \cos\theta &amp; -\sin\theta \\ \sin\theta &amp; \cos\theta\end{bmatrix}$，可以让$\mathbb{R}^2$中的x旋转$\theta$角得到y 来理解一下旋转矩阵是怎么做到的 单位圆中，与x轴夹角为$\theta$的向量表示如下： 则 再看看另一个正交向量的旋转 根据三角公式有 $$\begin{cases}-\sin\theta = \cos(\frac \pi2 + \theta) \\\cos\theta = \sin(\frac \pi2 + \theta)\end{cases}$$ 则向量 $\begin{bmatrix} -\sin\theta \\ \cos\theta \end{bmatrix}$表示的是有y轴夹角为$\theta$的向量，则 结合之前对映射法则的讲解，就可以理解旋转矩阵了： 旋转矩阵的原理，就是通过旋转基来实现的 2.2.2. 点积——向新基的投影还是使用上面用到的例子 $$A=\begin{bmatrix} 1 &amp; -1 \\ 1 &amp; 1 \end{bmatrix} \quad a=\begin{bmatrix} 1 \\ 1 \end{bmatrix}$$ 令 $\vec c_1 = [1 \quad -1 ]$，$\vec c_2 = [1 \quad 1 ]$，则$A=\begin{bmatrix} - \vec c_1 - \\ - \vec c_2 - \end{bmatrix}$ 则 $$Aa=\begin{bmatrix} - \vec c_1 - \\ - \vec c_2 - \end{bmatrix} [\vec a]=\begin{bmatrix} \vec c_1 \vec a \\ \vec c_2 \vec a \end{bmatrix}$$ 而我们知道，两个向量之间的点积运算规则为： $$\vec a · \vec b = |\vec a|·|\vec b|·\cos&lt;\vec a,\vec b&gt;$$ 即，$\vec a$ 的长度与 $\vec b$ 在 $\vec a$ 上的投影长度的乘积 从几何上感受一下 因此，从点积的角度来理解矩阵乘法的几何意义为（这里只讨论矩阵左乘，即为$Ax$形式的矩阵乘法）： 将$m\times n$的矩阵A看作是$m$个$n$维行向量，这就是新的基，然后将一个在自然基下的$n$维向量$x$向这个新基“投影”（分别向新基的$m$个基向量“投影”，注意这里的“投影”与我们通常所说的投影有些不同：投影后还要将两者的长度相乘），得到这个向量在新基张成的向量空间的新坐标$y$ 参考资料： (1) 微信公众号·马同学高等数学《图解线性代数》]]></content>
      <categories>
        <category>MathIsFunGame</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown+LaTex Math]]></title>
    <url>%2F2019%2F03%2F30%2FMarkdown-LaTexMath%2F</url>
    <content type="text"><![CDATA[实践出一整套便于互联网传播分享的数学公式跨平台编辑、跨环境显示是非常有必要的，如果还是停留在Word或PDF时代，那数学就会被限制在文档或图片里而无法通过最流行的网页方式进行传播，而且Word、PDF等文件处理软件里的数学公式编辑既麻烦，而且最重要的是与编程脱节。 数学公式的编辑与显示常用的文本编辑用Markdown就能高效地完成，但是Markdown有一个实现不了的功能——编辑数学公式，当然也不是说完全不能解决，折中一下，编辑成图片，以图片形式插入也行，但这毕竟不太高效和方便，也不利于传播，而在出版界用得比较多的LaTex语法虽然在数字的编辑上有一套成熟的解决方案，LaTex math，但是在普通文本的编辑上，它的语法格式与Markdown相比又显得太过复杂 那么，有没有一种方法，可以分别将Markdown的文本编辑优势和LaTex的数学公式编辑优势相互结合起来呢？ 有，就是Markdown+LaTex 与LaTex文档的比较 虽然很多数学学术论文整个文档就像使用Markdown一样是直接使用的LaTex语法来编辑的，但是仔细比对之后发现直接用LaTex语法来写整个文档来，它的效果和Markdown + LaTex Math 方式没有太大的区别。 但是LaTex的语法、编辑器、配置、中文支持等都要比Markdown要复杂的多，而且也不及Markdown已经非常成熟的生态（包括工具链、社区等）。 编辑器的选择Markdown的编辑器非常多，对于很多初学者来说，个人比较推荐使用VS Code VS Code汉化比较方便，想让更多人学会使用Python来学数学，有一个中文界面还是比较重要的；而且VS Code是跨平台的，Mac、Windows都可以上手； VS Code是一款极为优秀的代码编辑器，说起优秀，应该算是目前最为推荐的编辑器之一（可能没有之一）； VS Code插件丰富，Python的编译、Markdown的编写与预览、LaTex Math的显示等工具链相当完备； 数学公式与LaTex Math内联与块状在介绍数学公式之前，我们需要先来了解一下内联与块状的概念 所谓内联就是我们可以把数学符号嵌入到文字段落里面，比如： 1函数式：$f(x)=\frac&#123;P(x)&#125;&#123;Q(x)&#125;$ 函数式：$f(x)=\frac{P(x)}{Q(x)}$ 如果我们需要输出的数学公式比较复杂，或者我们需要凸出并独立显示公式，这个时候我们就需要使用到公式的块状输出，块状输出的语法使用4个美元符号$$数学公式$$，我们来看案例。 1$$f(x)=\frac&#123;P(x)&#125;&#123;Q(x)&#125;$$ $$f(x)=\frac{P(x)}{Q(x)}$$ 使用块状输出，函数会居中显示，值得一提的是我们在使用块状输出数学公式时，在Markdown里需要换行来写公式。 LaTex Math的基本语法 希腊字母 12345678910$\Gamma$$\iota$$\sigma$$\phi$$\upsilon$$\Pi$$\Bbbk$$\heartsuit$$\int$$\oint$ $\Gamma$、$\iota$、$\sigma$、$\phi$、$\upsilon$、$\Pi$、$\Bbbk$、$\heartsuit$、$\int$、$\oint$ 三角函数 123$\tan$$\sin$$\cos$ 运算符 1234567891011$+$$-$$=$$&gt;$$&lt;$$\times$$\div$$\equiv$$\leq$$\geq$$\neq$ $+$、$-$、$=$、$&gt;$、$&lt;$、$\times$、$\div$、$\equiv$、$\leq$、$\geq$、$\neq$ 集合符号 12345678910$\cup$$\cap$$\in$$\notin$$\ni$$\subset$$\subseteq$$\supset$$\supseteq$$\infty$ $\cup$、$\cap$、$\in$、$\notin$、$\ni$、$\subset$、$\subseteq$、$\supset$、$\supseteq$、$\infty$ 指数输出 12$x^3+x^9$ $x^y$ $x^3+x^9$、$x^y$ n次方根输出 1$\sqrt&#123;3x-1&#125;+\sqrt[5]&#123;2y^5-4&#125;$ $\sqrt{3x-1}+\sqrt[5]{2y^5-4}$ 输出分数 语法为\frac{分子}{分母} 1$$\frac&#123;x&#125;&#123;2y&#125; +\frac&#123;x-y&#125;&#123;x+y&#125; $$ $$\frac{x}{2y} +\frac{x-y}{x+y} $$ 累加求和输出 求和公式比较复杂，会涉及到上标和下标，在输出指数^时我们可以把它看成是上标，使用_来输出下标 1$$\sum_&#123;n=1&#125;^\infty k$$ $$\sum_{n=1}^\infty k$$ 极限的输出 输出极限就会使用到下标 1$$\lim\limits_&#123;x \to \infty&#125; \exp(-x) = 0$$ $$\lim\limits_{x \to \infty} \exp(-x) = 0$$ 输出矩阵 使用\begin{matrix}和\end{matrix}围住即可输出矩阵，矩阵之间用$来空格，用\\来换行 注意：在mathjax的环境下用\\可能无法成功解析，可以尝试\\\ 1234567$$ \begin&#123;matrix&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end&#123;matrix&#125; $$ $$ \begin{matrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end{matrix}$$ 复杂数学公式 分段函数的编写 分段函数是非常复杂的，这时候会用到LaTex的cases语法，用\begin{cases}和\end{cases}围住即可，中间则用\\来分段，具体我们来看下面的例子 12345678$$X(m,n)=\begin&#123;cases&#125;x(n), \\x(n-1) \\x(n-1)\end&#123;cases&#125;$$ $$X(m,n)=\begin{cases}x(n), \\x(n-1) \\x(n-1)\end{cases}$$ LaTex Math的跨平台显示（1）知乎、简书 简书的Markdown编辑器可以比较完美的支持Markdown语法以及Markdown Math语法，可以直接把用VS Code写的Markdown文件里的内容复制粘贴过去，然后进行一些简单的修改就可以了 知乎自带数学公式的插入，如果直接导入Markdown文件显示会出现一些问题，需要把数学公式用知乎自带的Tex编辑器重新书写，只需要把$$删除即可 （2）网页 由于我们的网页可以不用Markdown，用HTML替换Markdown排版语法就可以，所以我们只需要专注于如何在网页上显示数学公式即可。比较完美的解决方案是使用mathjax，我们只需要在&lt;head&gt;标签内插入mathjaxjs即可。 123&lt;script type=&quot;text/javascript&quot; async src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt; 要注意的是内联式的语法会有些不同，不再是$符号与公式$，而是：\(符号与公式\) 1234&lt;p&gt; 当 \(a \ne 0\)时, \(ax^2 + bx + c = 0\) 会有两个解，它们是： $$x = &#123;-b \pm \sqrt&#123;b^2-4ac&#125; \over 2a&#125;.$$&lt;/p&gt; （3）公众号 微信公众号封闭且奇葩，美化微信公众号的排版虽然用的是html和css语法，但是有很多需要注意的地方，因此排版也相对来说比较麻烦 要想让数学公式在公众号上显示就比较麻烦，微信公众号是不支持LaTex语法的，所以需要把公式做成图片，其他不支持LaTex的自媒体平台也可以这么处理 参考资料： (1) 【简书】HackWeek《使用Markdown输出LaTex数学公式》]]></content>
      <categories>
        <category>前后端技术</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
        <tag>LaTex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[无监督聚类算法]]></title>
    <url>%2F2019%2F03%2F14%2FUnsupervise-Clustering-Algorithmn%2F</url>
    <content type="text"><![CDATA[1. 谱聚类1.1. 一堆基础概念 单位矩阵 在矩阵中，n阶单位矩阵，是一个 nxn 的方形矩阵，其主对角线元素为1，其余元素为0。单位矩阵以 In 表示；如果阶数可忽略，或可由前后文确定的话，也可简记为I（或者E）。 如下图所示，便是一些单位矩阵： 单位矩阵中的第i列即为单位向量vi，单位向量同时也是单位矩阵的特征向量，特征值皆为1，因此这是唯一的特征值，且具有重数n。由此可见，单位矩阵的行列式为1，且迹数为n。 单位向量 向量空间中的单位向量就是长度为 1 的向量 两个单位向量的点积就是它们之间角度的余弦（因为它们的长度都是 1）： 补充知识——向量的乘法运算 一个非零向量u→的正规化向量（即单位向量）u^就是平行于u→的单位向量，记作： 这里||u→||是u→的范数（长度） 正交与正交矩阵 正交是垂直这一直观概念的推广，若内积空间中两向量的内积（即点积）为0，则称它们是正交的，相当于这两向量垂直，换言之，如果能够定义向量间的夹角，则正交可以直观的理解为垂直 正交矩阵（orthogonal matrix）是一个元素为实数，而且行与列皆为正交的单位向量的方块矩阵（方块矩阵，或简称方阵，是行数及列数皆相同的矩阵） 1.2. 拉普拉斯矩阵拉普拉斯矩阵（Laplacian matrix)），也称为基尔霍夫矩阵，是表示图的一种矩阵 给定一个有n个顶点的图 G=(V,E)，其拉普拉斯矩阵被定义为 L = D-A，D其中为图的度矩阵，A为图的邻接矩阵 例如给定下图G=(V,E) 把此“图”转换为邻接矩阵的形式，记为A（假设每条边的连接权重均为1）： 把W的每一列元素加起来得到N个数，然后把它们放在对角线上（其它地方都是零），组成一个N×N的对角矩阵，记为度矩阵D，如下图所示 根据拉普拉斯矩阵的定义L = D-A，可得拉普拉斯矩阵L 为： 显然，拉普拉斯矩阵都是对称 1.3. 拉普拉斯矩阵的性质介绍拉普拉斯矩阵的性质之前，首先定义两个概念，如下： 对于邻接矩阵，定义图中A子图与B子图之间所有边的权值之和如下： 其中，定义 w ij为节点 i 到节点 j 的权值，如果两个节点不是相连的，权值为零 与某结点邻接的所有边的权值和定义为该顶点的度d，多个d 形成一个度矩阵（对角阵） 拉普拉斯矩阵L具有如下性质： L 是对称半正定矩阵； L * v1→ = 0 * v1→，即 L 的最小特征值是 0，相应的特征向量是 v1→ 证明： L * v1→ = (D-W) * v1→ = 0 = 0 * v1→ 特征值和特征向量的定义： 若数字λ和非零向量v→满足 则v→为A的一个特征向量，λ是其对应的特征值 L 有n个非负实特征值 对于任何一个属于实向量 f∈Rn，有以下式子成立 其中 下面，来证明下上述结论，如下： 1.4. 谱聚类1.4.1. 算法思想及优化目标谱聚类本质上就是将聚类问题转换为图论问题 从图论的角度来说，聚类的问题就相当于一个图的分割问题。即给定一个图G = (V, E)，顶点集V表示各个样本，带权的边表示各个样本之间的相似度，谱聚类的目的便是要找到一种合理的分割图的方法，使得分割后形成若干个子图，连接不同子图的边的权重（相似度）尽可能低，同子图内的边的权重（相似度）尽可能高。物以类聚，人以群分，相似的在一块儿，不相似的彼此远离。 至于如何把图的顶点集分割/切割为不相交的子图有多种办法，如： cut/Ratio Cut Normalized Cut 不基于图，而是转换成SVD能解的问题 优化目标为：让被割掉各边的权值和最小 因为被砍掉的边的权值和越小，代表被它们连接的子图之间的相似度越小，隔得越远，而相似度低的子图正好可以从中一刀切断。 1.4.2. 构造Laplacian图（1）先要构造相似度矩阵，任意两个对象xi和xj，其相似度基于高斯核函数（也称径向基函数核）计算相似度定义为： 距离越大，代表其相似度越小 相似度矩阵就是Laplacian图的邻接矩阵W 根据邻接矩阵W可以得到度矩阵 最后Laplacian矩阵为L=D-W （2）子图A的指示向量如下： 1.4.3. 用Cut/Ratio Cut分割子图1.4.3.1. 优化目标如何切割才能得到最优的结果呢？ 要把图片分割为几个区域（或若干个组），要求是分割所得的 Cut 值最小，相当于那些被切断的边的权值之和最小 设 A1, A2, …, Ak 为图的几个子集（它们没有交集） ，为了让分割的Cut 值最小，谱聚类便是要最小化下述目标函数： 但很多时候，最小化cut 通常会导致不好的分割。以分成2类为例，这个式子通常会将图分成了一个点和其余的n-1个点。如下图所示，很明显，最小化的smallest cut不是最好的cut，反而把{A、B、C、H}分为一边，{D、E、F、G}分为一边很可能就是最好的cut： 为了让每个类都有合理的大小，目标函数尽量让 A1, A2, …, Ak 足够大。改进后的目标函数为： 其中| A i |表示 A i 组中包含的顶点数目 或者也可以将优化目标定义为最小化Ncut： 其中 1.4.3.2. 求解优化问题：最小化RatioCut与最小化 f’Lf 等价补充知识*1. 向量的乘法运算*1.1. 向量内积（点积） 使用矩阵乘法并把（纵列）向量当作n×1 矩阵，点积还可以写为： 向量内积性质 向量内积的物理意义 向量内积的物理意义是，力通过位移做功 *1.2. 向量内积的用途 求两个非零向量的夹角 判断两个非零向量是否垂直 简单的对应坐标相乘再求和，结果为0就垂直，否则就不垂直 *1.3. 向量外积 通过坐标进行外积的直接计算比较复杂，写成行列式的形式，再展开，方便记忆 向量外积的性质 向量外积的几何意义 再除以2的话，就是以 a，b 为边的三角形的面积 *1.4. 向量外积的用途求与三角形面积相关的问题 参考资料： (1) CSDN·珠穆拉玛峰《从拉普拉斯矩阵说到谱聚类》 (2) CSDN·鹅城惊喜师爷《【线性代数】向量的乘法运算》 (3) 行者无疆兮.csdn【图论】拉普拉斯矩阵（Laplacian matrix）]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>无监督学习</tag>
        <tag>图论</tag>
        <tag>聚类算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【整理·非原创】理解矩阵]]></title>
    <url>%2F2019%2F03%2F10%2FUnderStand-Matrix%2F</url>
    <content type="text"><![CDATA[“如果不熟悉线性代数的概念，要去学习自然科学，现在看来就和文盲差不多” “按照现行的国际标准，线性代数是通过公理化来表述的，它是第二代数学模型，…，这就带来了教学上的困难” ——瑞典数学家Lars Garding《Encounter with Mathematics》 当我们开始学习线性代数的时候，不知不觉就进入了“第二代数学模型”的范畴当中，这意味着数学的表述方式和抽象性有了一次全面的进化，对于从小一直在“第一代数学模型”，即以实用为导向的、具体的数学模型中学习的我们来说，在没有并明确告知的情况下进行如此剧烈的paradigm shift，不感到困难才是奇怪的。 1. 我们对于矩阵的种种疑惑——本质是线性代数教学中直觉性丧失对于矩阵的一些基础问题： （1）矩阵究竟是什么东西？向量可以被认为是具有n个相互独立的性质（维度）的对象的表示，矩阵又是什么呢？我们如果认为矩阵是一组列（行）向量组成的新的复合向量的展开式，那么为什么这种展开式具有如此广泛的应用？特别是，为什么偏偏二维的展开式如此有用？如果矩阵中每一个元素又是一个向量，那么我们再展开一次，变成三维的立方阵，是不是更有用？ （2）矩阵的乘法规则究竟为什么这样规定？为什么这样一种怪异的乘法规则却能够在实践中发挥如此巨大的功效？很多看上去似乎是完全不相关的问题，最后竟然都归结到矩阵的乘法，这难道不是很奇妙的事情？难道在矩阵乘法那看上去莫名其妙的规则下面，包含着世界的某些本质规律？如果是的话，这些本质规律是什么？ （3） 行列式究竟是一个什么东西？为什么会有如此怪异的计算规则？行列式与其对应方阵本质上是什么关系？为什么只有方阵才有对应的行列式，而一般矩阵就没有（不要觉得这个问题很蠢，如果必要，针对m x n矩阵定义行列式不是做不到的，之所以不做，是因为没有这个必要，但是为什么没有这个必要）？而且，行列式的计算规则，看上去跟矩阵的任何计算规则都没有直观的联系，为什么又在很多方面决定了矩阵的性质？难道这一切仅是巧合？ （4）矩阵为什么可以分块计算？分块计算这件事情看上去是那么随意，为什么竟是可行的？ （5）对于矩阵转置运算AT，有(AB)T = BTAT，对于矩阵求逆运算A-1，有(AB)-1 = B-1A-1。两个看上去完全没有什么关系的运算，为什么有着类似的性质？这仅仅是巧合吗？ （6）为什么说P-1AP得到的矩阵与A矩阵“相似”？这里的“相似”是什么意思？ （7）特征值和特征向量的本质是什么？它们定义就让人很惊讶，因为Ax =λx，一个诺大的矩阵的效应，竟然不过相当于一个小小的数λ，确实有点奇妙。但何至于用“特征”甚至“本征”来界定？它们刻划的究竟是什么？ 这样的问题如果不能获得回答，线性代数对于我们来说就是一个粗暴的、不讲道理的、莫名其妙的规则集合，我们会感到，自己并不是在学习一门学问，而是被不由分说地“抛到”一个强制的世界中，只是在考试的皮鞭挥舞之下被迫赶路，全然无法领略其中的美妙、和谐与统一。 之所以会存在这么多的为什么，很大原因是线性代数教学中直觉性丧失的后果，即知其然不知其所以然 上述这些涉及到“如何能”、“怎么会”的问题，虽然能够通过数学性证明来回答，但并不能够让提问者的疑惑得到解决 比如对于“矩阵为什么可以分块计算”这个问题 提问者其实真正的疑惑是在于：矩阵分块运算为什么竟然是可行的？究竟只是凑巧，还是说这是由矩阵这种对象的某种本质所必然决定的？如果是后者，那么矩阵的这些本质是什么？ 2. 对线形空间和矩阵的几个核心概念的理解2.1. 什么是空间空间有很多种。你要是去看某种空间的数学定义，大致都是“存在一个集合，在这个集合上定义某某概念，然后满足某些性质”，就可以被称为空间，即用“空间”来称呼一些具有某些特性的集合 我们一般人最熟悉的空间，毫无疑问就是我们生活在其中的（按照牛顿的绝对时空观）的三维空间，从数学上说，这是一个三维的欧几里德空间 我们先不管那么多，先看看我们熟悉的这样一个空间有些什么最基本的特点。仔细想想我们就会知道，这个三维的空间： 由很多（实际上是无穷多个）位置点组成； 这些点之间存在相对的关系； 可以在空间中定义长度、角度； 这个空间可以容纳运动，这里我们所说的运动是从一个点到另一个点的移动（变换），而不是微积分意义上的“连续”性的运动； 上面的这些性质中，最最关键的是第4条：容纳运动是空间的本质特征 认识到了这些，我们就可以把我们关于三维空间的认识扩展到其他的空间： 不管是什么空间，都必须容纳和支持在其中发生的符合规则的运动（变换） 在某种空间中往往会存在一种相对应的变换： 拓扑空间中有拓扑变换； 线性空间中有线性变换； 仿射空间中有仿射变换； 其实这些变换都只不过是对应空间中允许的运动形式而已 “空间”是容纳运动的一个对象集合，而变换则规定了对应空间的运动 2.2. 线性空间与线性变换的表示方式——矩阵既然我们承认线性空间是个空间，那么有两个最基本的问题必须首先得到解决，那就是： 空间是一个对象集合，线性空间也是空间，所以也是一个对象集合。那么线性空间是什么样的对象的集合？或者说，线性空间中的对象有什么共同点吗？ 线性空间中的任何一个对象，通过选取基和坐标的办法，都可以表达为向量的形式 举两个例子： （1）最高次项不大于n次的多项式的全体构成一个线性空间，也就是说，这个线性空间中的每一个对象是一个多项式。如果我们以 x 0, x 1, …, x n 为基，那么任何一个这样的多项式都可以表达为一组n+1维向量，其中的每一个分量 a i 其实就是多项式中 x (i-1) 项的系数 值得说明的是，基的选取有多种办法，只要所选取的那一组基线性无关就可以 （2）闭区间[a, b]上的n阶连续可微函数的全体，构成一个线性空间。也就是说，这个线性空间的每一个对象是一个连续函数。对于其中任何一个连续函数，根据魏尔斯特拉斯定理，一定可以找到最高次项不大于n的多项式函数，使之与该连续函数的差为0，也就是说，完全相等。这样就把问题归结为L1了 所以说，向量是很厉害的，只要你找到合适的基，用向量可以表示线性空间里任何一个对象 这里头大有文章，因为向量表面上只是一列数，但是其实由于它的有序性，所以除了这些数本身携带的信息之外，还可以在每个数的对应位置上携带信息。 线性空间中的运动如何表述的？也就是，线性变换是如何表示的？ 线性空间中的运动，被称为线性变换。也就是说，你从线性空间中的一个点运动到任意的另外一个点，都可以通过一个线性变化来完成。 那么，线性变换如何表示呢？ 在线性空间中，当你选定一组基之后，不仅可以用一个向量来描述空间中的任何一个对象，而且可以用矩阵来描述该空间中的任何一个运动（变换）。而使某个对象发生对应运动的方法，就是用代表那个运动的矩阵，乘以代表那个对象的向量。 在线性空间中选定基之后，向量刻画对象，矩阵刻画对象的运动，用矩阵与向量的乘法施加运动 是的，矩阵的本质是运动的描述 2.3. 进一步辨析”矩阵是运动的描述”：引出变换初等数学是研究常量的数学，是研究静态的数学，高等数学是变量的数学，是研究运动的数学 ——《高等数学·微积分》 大家口口相传，差不多人人都知道这句话。但是真知道这句话说的是什么意思的人，好像也不多 简而言之，在我们人类的经验里，运动是一个连续过程，从A点到B点，就算走得最快的光，也是需要一个时间来逐点地经过AB之间的路径，这就带来了连续性的概念。而连续这个事情，如果不定义极限的概念，根本就解释不了 古希腊人的数学非常强，但就是缺乏极限观念，所以解释不了运动，被芝诺的那些著名悖论（飞箭不动、飞毛腿阿喀琉斯跑不过乌龟等四个悖论）搞得死去活来。 不过在矩阵相关的研究中，“运动”的概念不是微积分中的连续性的运动，而是瞬间发生的变化 比如这个时刻在A点，经过一个“运动”，一下子就“跃迁”到了B点，其中不需要经过A点与B点之间的任何一个点。这样的“运动”，或者说“跃迁”，是违反我们日常的经验的。不过了解一点量子物理常识的人，就会立刻指出，量子（例如电子）在不同的能量级轨道上跳跃，就是瞬间发生的，具有这样一种跃迁行为。所以说，自然界中并不是没有这种运动现象，只不过宏观上我们观察不到。 为了避免产生歧义，作者对”矩阵是运动的描述”进行了更为精确的定义 “矩阵是线性空间里跃迁的描述” 可是这样说又太物理，也就是说太具体，而不够数学，也就是说不够抽象。因此我们最后换用一个正牌的数学术语——变换，来描述这个事情 因此，所谓变换，其实就是空间里从一个点（元素/对象）到另一个点（元素/对象）的跃迁 比如说，拓扑变换，就是在拓扑空间里从一个点到另一个点的跃迁。再比如说，仿射变换，就是在仿射空间里从一个点到另一个点的跃迁 123456附带说一下，这个仿射空间跟向量空间是亲兄弟做计算机图形学的朋友都知道，尽管描述一个三维对象只需要三维向量，但所有的计算机图形学变换矩阵都是4 x 4的。说其原因，很多书上都写着“为了使用中方便”，这在我看来简直就是企图蒙混过关。真正的原因，是因为在计算机图形学里应用的图形变换，实际上是在仿射空间而不是向量空间中进行的。 一旦我们理解了“变换”这个概念，矩阵的定义就变成： “矩阵是线性空间里的变换的描述” 到这里为止，我们终于得到了一个看上去比较数学的定义。 2.4. 理解“矩阵是线性空间中的线性变换的一个描述”“在一个线性空间V里的一个线性变换T，当选定一组基之后，就可以表示为矩阵” “线性变换：设有一种变换T，使得对于线性空间V中间任何两个不相同的对象x和y，以及任意实数a和b，有：T(ax + by) = aT(x) + bT(y)” ——《线性代数》 2.4.1. 什么是线性变换线性变换究竟是一种什么样的变换？ 变换是从空间的一个点跃迁到另一个点，而线性变换，就是从一个线性空间V的某一个点跃迁到另一个线性空间W的另一个点的运动。 这句话里蕴含着一层意思，就是说一个点不仅可以变换到同一个线性空间中的另一个点，而且可以变换到另一个线性空间中的另一个点去 不管你怎么变，只要变换前后都是线性空间中的对象，这个变换就一定是线性变换，也就一定可以用一个非奇异矩阵来描述。而你用一个非奇异矩阵去描述的一个变换，一定是一个线性变换 补充知识：奇异与非奇异 首先，看这个矩阵是不是方阵（即行数和列数相等的矩阵。若行数和列数不相等，那就谈不上奇异矩阵和非奇异矩阵）。 然后，再看此矩阵的行列式|A|是否等于0，若等于0，称矩阵A为奇异矩阵；若不等于0，称矩阵A为非奇异矩阵 同时，由|A|≠0可知矩阵A可逆，这样可以得出另外一个重要结论:可逆矩阵就是非奇异矩阵，非奇异矩阵也是可逆矩阵 2.4.2. 什么是基什么是基呢？ 为了方便理解，这里只要把基看成是线性空间里的坐标系就可以了 这样一来，“选定一组基”就是说在线性空间里选定一个坐标系。 2.4.3. 区别“线性变换”与“线性变换的一个描述”最后我们把矩阵的定义完善如下： “矩阵是线性空间中的线性变换的一个描述。在一个线性空间中，只要我们选定一组基，那么对于任何一个线性变换，都能够用一个确定的矩阵来加以描述。” 理解这句话的关键，在于把“线性变换”与“线性变换的一个描述”区别开。一个是那个对象，一个是对那个对象的表述。 同样的，对于一个线性变换，只要你选定一组基，那么就可以找到一个矩阵来描述这个线性变换。换一组基，就得到一个不同的矩阵。所有这些矩阵都是这同一个线性变换的描述，但又都不是线性变换本身。 这些对于同一个线性变换基于所选择的基的不同而得到的一组描述矩阵，它们之间互为相似矩阵 那么问题来了：你给我两个矩阵，我怎么知道这两个矩阵是描述的同一个线性变换呢？ 同一个线性变换的矩阵兄弟们的一个性质： 若矩阵A与B是同一个线性变换的两个不同的描述（之所以会不同，是因为选定了不同的基，也就是选定了不同的坐标系），则一定能找到一个非奇异矩阵P，使得A、B之间满足这样的关系： A = P-1BP 线性代数稍微熟一点的读者一下就看出来，这就是相似矩阵的定义 上面式子里那个矩阵P，其实就是A矩阵所基于的基与B矩阵所基于的基这两组基之间的一个变换关系 同一个线性变换的不同矩阵描述，从实际运算性质来看并不是不分好环的。有些描述矩阵就比其他的矩阵性质好得多。这很容易理解，同一头猪的照片也有美丑之分嘛。所以矩阵的相似变换可以把一个比较丑的矩阵变成一个比较美的矩阵，而保证这两个矩阵都是描述了同一个线性变换。 3. 对1、2的总结及引出下一部分 首先有空间，空间可以容纳对象运动的。一种空间对应一类对象。 有一种空间叫线性空间，线性空间是容纳向量对象运动的。 运动是瞬时的，因此也被称为变换。 矩阵是线性空间中运动（变换）的描述。 矩阵与向量相乘，就是实施运动（变换）的过程。 同一个变换，在不同的坐标系下表现为不同的矩阵，但是它们的本质是一样的，所以本征值相同。 引出下一部分： 矩阵不仅可以作为线性变换的描述，而且可以作为一组基的描述 作为变换的矩阵，不但可以把线性空间中的一个点给变换到另一个点去，而且也能够把线性空间中的一个坐标系（基）表换到另一个坐标系（基）去 4. 将矩阵当作对一组基（坐标系）的描述我们知道，线性空间里的基本对象是向量，而向量是这么表示的： [ a1, a2, a3, …, an ] 矩阵是这么表示的： a11, a12, a13, …, a1n a21, a22, a23, …, a2n … an1, an2, an3, …, ann 可以很容易看出，矩阵是一组向量组成的。特别的，n维线性空间里的方阵是由n个n维向量组成的 我们在这里只讨论这个n阶的、非奇异的方阵，因为理解它就是理解矩阵的关键，它才是一般情况，而其他矩阵都是意外，都是不得不对付的讨厌状况，大可以放在一边 如果一组向量是彼此线性无关的话，那么它们就可以成为度量这个线性空间的一组基，从而事实上成为一个坐标系体系，其中每一个向量都躺在一根坐标轴上，并且成为那根坐标轴上的基本度量单位（长度1）。 现在到了关键的一步：看上去矩阵就是由一组向量组成的，而且如果矩阵非奇异的话（我说了，只考虑这种情况），那么组成这个矩阵的那一组向量也就是线性无关的了，也就可以成为度量线性空间的一个坐标系。 结论：矩阵描述了一个坐标系 4.1. 为什么可以将矩阵看作坐标系你可能会产生下面的困惑：前面刚说了矩阵就是运动吗？怎么这会矩阵又是坐标系了？ 之所以矩阵又是运动，又是坐标系，那是因为—— “运动等价于坐标系变换” 或者说得更直白一点： “固定坐标系下一个对象的变换等价于固定对象所处的坐标系变换” 其实这就相当于物理中说到的，参考系选择的不同，运动的描述也就不同，即运动是相对的：打个比方，把线性空间的一个对象（即一个向量）当作是你，坐标系当作是地面，你在路上走，若以地面为参考系，那么你是运动的，若以你为参考系，地面是运动的 举个简单的例子： 把点(1, 1)变到点(2, 3)去，你可以有两种做法： 坐标系不动，点动，把(1, 1)点挪到(2, 3)去 点不动，变坐标系，让x轴的度量（单位向量）变成原来的1/2，让y轴的度量（单位向量）变成原先的1/3，这样点还是那个点，可是点的坐标就变成(2, 3)了 4.2. 比较运动视角与坐标系视角下的矩阵（1）从第一个方式来看，把矩阵看成是运动描述，矩阵与向量相乘就是使向量（点）运动的过程 Ma = b：向量a经过矩阵M所描述的变换，变成了向量b （2）从第二个方式来看，矩阵M描述了一个坐标系 Ma = b：有一个向量，它在坐标系M的度量下得到的度量结果向量为a，那么它在坐标系I的度量下，这个向量的度量结果是b 或者也可以这么理解： 在M为坐标系的意义下，如果把M放在一个向量a的前面，形成Ma的样式，我们可以认为这是对向量a的一个环境声明。它相当于是说： “注意了！这里有一个向量，它在坐标系M中度量，得到的度量结果可以表达为a。可是它在别的坐标系里度量的话，就会得到不同的结果。为了明确，我把M放在前面，让你明白，这是该向量在坐标系M中度量的结果。” 那么我们再看孤零零的向量b：多看几遍，你没看出来吗？它其实不是b，它是——Ib，也就是说：“在单位坐标系，也就是我们通常说的直角坐标系I中，有一个向量，度量的结果是b。” 而 Ma = Ib的意思就是说： “在M坐标系里量出来的向量a，跟在I坐标系里量出来的向量b，其实根本就是一个向量啊！” 4.3. 重新理解向量向量这个东西客观存在，但是要把它表示出来，就要把它放在一个坐标系中去度量它，然后把度量的结果（向量在各个坐标轴上的投影值）按一定顺序列在一起，就成了我们平时所见的向量表示形式。 你选择的坐标系（基）不同，得出来的向量的表示就不同。向量还是那个向量，选择的坐标系不同，其表示方式就不同。 因此，按道理来说，每写出一个向量的表示，都应该声明一下这个表示是在哪个坐标系中度量出来的。表示的方式，就是 Ma，也就是说，有一个向量，在M矩阵表示的坐标系中度量出来的结果为a。我们平时说一个向量是[2 3 5 7]T，隐含着是说，这个向量在 I 坐标系中的度量结果是[2 3 5 7]T，因此，这个形式反而是一种简化了的特殊情况。 注意到，M矩阵表示出来的那个坐标系，由一组基组成，而那组基也是由向量组成的，同样存在这组向量是在哪个坐标系下度量而成的问题。也就是说，表述一个矩阵的一般方法，也应该要指明其所处的基准坐标系。所谓M，其实是 IM，也就是说，M中那组基的度量是在 I 坐标系中得出的。从这个视角来看，M×N也不是什么矩阵乘法了，而是声明了一个在M坐标系中量出的另一个坐标系N，其中M本身是在I坐标系中度量出来的。 4.4. 回头理解“固定坐标系下一个对象的变换等价于固定对象所处的坐标系变换”回过头来说变换的问题。我刚才说，“固定坐标系下一个对象的变换等价于固定对象所处的坐标系变换”，那个“固定对象”我们找到了，就是那个向量。但是坐标系的变换呢？我怎么没看见？ Ma = Ib 我现在要变M为I，怎么变？对了，在前面乘以个M-1，也就是M的逆矩阵。换句话说，你不是有一个坐标系M吗，现在我让它乘以个M-1，变成I，这样一来的话，原来M坐标系中的a在I中一量，就得到b了 尝试去理解这个事件： 比如，你画一个坐标系，x轴上的衡量单位是2，y轴上的衡量单位是3，在这样一个坐标系里，坐标为(1，1)的那一点，实际上就是笛卡尔坐标系里的点(2, 3)。而让它原形毕露的办法，就是把原来那个坐标系: 在x方向度量缩小为原来的1/2，而y方向度量缩小为原来的1/3，这样一来坐标系就变成单位坐标系I了。保持点不变，那个向量现在就变成了(2, 3)了。 怎么能够让“x方向度量缩小为原来的1/2，而y方向度量缩小为原来的1/3”呢？就是让原坐标系被矩阵： 左乘，而这个矩阵就是原矩阵的逆矩阵。 下面我们得出一个重要的结论： “对坐标系施加变换的方法，就是让表示那个坐标系的矩阵与表示那个变化的矩阵相乘。” 参考资料： (1) 孟岩 《理解矩阵（一）》 (2) 孟岩 《理解矩阵（二）》 (3) 孟岩 《理解矩阵（三）》 (4) 《高等数学》 (5) 《线性代数》 (6) 百度百科《奇异矩阵》]]></content>
      <categories>
        <category>MathIsFunGame</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从CONCOCT入手理解宏基因组binning]]></title>
    <url>%2F2019%2F03%2F09%2FUnderstand-MetaBin-by-CONCOCT%2F</url>
    <content type="text"><![CDATA[1. 宏基因组binning简介Metagenome 组装完成后，我们得到的是成千上万的 contigs，我们需要知道哪些 contigs 来自哪一个基因组，或者都有哪些微生物的基因组。所以需要将 contigs 按照物种水平进行分组归类，称为 “bining” 123456Supervised binning methods: use databases of already sequenced genomes to label contigs into taxonomic classesUnsupervised (clustering) methods: look for natural groups in the dataBoth supervised and unsupervised methods have two main elements: a metric to define the similarity between a given contig anda bin, and an algorithm to convert those similarities into assignments 一个很容易想到的策略就是，将组装得到的片段与已知物种的参考基因组进行比对，根据同源性进行归类。然而目前大多数的微生物的基因组还没有测序出来，因此限制了这种方法的可行性。 目前主流的 bining 策略利用的是 contigs 的序列组成特点。 2. binning原理2.1. 可用于binning的特征 根据核酸组成信息来进行binning：k-mer frequencies 依据：来自同一菌株的序列，其核酸组成是相似的 例如根据核酸使用频率（oligonucleotide frequency variations），通常是四核苷酸频率（tetranucleotide frequency），GC含量和必需的单拷贝基因等 优势：即便只有一个样品的宏基因组数据也可以进行binning，这在原理上是可操作的 不足：由于很多微生物种内各基因型之间的基因组相似性很高，想利用1个样品的宏基因组数据通过核酸组成信息进行binning，效果往往并不理想或难度很大。利用核酸组成信息进行binning，基本上只适合那些群落中物种基因型有明显核酸组成差异的，例如低GC含量和一致的寡核苷酸使用频率 根据丰度信息来进行binning 依据：来自同一个菌株的基因在不同的样品中 ( 不同时间或不同病理程度 ) 的丰度分布模式是相似的【PMID: 24997787】。 原因：比如，某一细菌中有两个基因，A和B，它们在该细菌基因组中的拷贝数比例为 A:B = 2:1，则不管在哪个样品中这种细菌的数量有多少，这两个基因的丰度比例总是为 2:1 优势：这种方法更有普适性，一般效果也比较好，能达到菌株的水平 不足：必须要大样本量，一般至少要50个样本以上，至少要有2个组能呈现丰度变化 ( 即不同的处理，不同的时间，疾病和健康，或者不同的采样地点等 ) ，每个组内的生物学重复也要尽量的多 对于像质粒这样的可移动遗传单元 (mobile genetic elements (MGEs))，由于其复制独立于细菌染色体，则同一种细菌的不同个体，该质粒的拷贝数可能存在差异，使得无法用丰度信息进行有效地bining 同时依据核酸组成和丰度变化信息 将核酸组成信息和丰度差异信息创建一个综合的距离矩阵，既能保证binning效果，也能相对节约计算资源，现在比较主流的binning软件多是同时依据核酸组成和丰度变化信息 根据基因组甲基化模式 依据：不同的细菌，其基因组甲基化模式不同，平均一种细菌有3种特意的甲基化 motif。MGEs (mobile genetic elements) 中含有 MTase 基因，其基因水平转移是细菌甲基化组多样性的驱动因素。虽然 MGEs 在不同个体的拷贝数不同，但是都存在，因此具有相同 MGEs 的细菌个体，其总遗传物质（包括染色体和 MGEs ）都会受到相同的MTase的作用而得到相同的甲基化模式。 2.2. 从哪些序列下手进行binning？从原始的clean reads，还是从组装成的contig，还是从预测到的gene，都可以。根据基于聚类的序列类型的不同，暂且分为reads binning， contig binning和 genes binning 比较这三种binning的优劣： contig binning 由于核酸组成和物种丰度变化模式在越长的序列中越显著和稳定，基于contig binning效果可能更好 reads binning 基于reads binning的优势是可以聚类出宏基因组中丰度非常低的物种 考虑到在宏基因组组装中reads利用率很低，单样品5Gb测序量情况下，环境样品组装reads利用率一般只有10%左右，肠道样品或极端环境样品组装reads利用率一般能达到30%，这样很多物种，尤其是低丰度的物种可能没有被组装出来，没有体现在gene 或者contig 中，因此基于reads binning 才有可能得到低丰度的物种 如 Brian Cleary 等 (DOI:10.1038/nbt.3329.Detection) 利用基于 reads binning 的 latent strain analysis 可以聚类出丰度低至0.00001%的菌株。此方法虽然得到更全面的 bins，但低丰度 bins 信息依旧不完整。 genes binning 应用非常广泛 原因可能是（1）基于genes丰度变化模式进行binning可操作性比较强，宏基因组分析中肯定都会计算gene丰度，一般不会计算contig丰度，gene丰度数据可以信手拈来；（2）基于genes binning有很多可参考的文献，过程也并不复杂，可复制性强；（3）对计算机资源消耗比较低 总体来说应用最广泛的就是基于genes binning 和 contig binning Naseer Sangwan 等 (DOI: 10.1186/s40168-016-0154-5) 总结了 contig binning 的算法和软件（如下表） 基于Genes abundance binning的一般流程 在宏基因组做完组装和基因预测之后，把所有样品中预测到的基因混合在一起，去冗余得到unique genes集合，对这个unique genes集合进行binning，主要是根据gene在各个样品中的丰度变化模式，计算gene之间的相关性，利用这种相关性进行聚类 该图中的聚类过程类似于K-means聚类：随机选择几个seed genes作为诱饵，计算其他基因丰度分布模式与seed genes的相关性，按照固定的相关性值PCC&gt;0.9，将它们归属于不同seed genes所代表的类，然后在聚好的类内重新选择seed genes，进行迭代，最终聚类得到一个个基因集合，较大的集合（超过700个基因）称为 metagenomic species (MGS)，较小的集合称为 co-abundance gene group (CAG) 基于 binning 结果进行单菌组装： Sequence reads from individual samples that map to the MGS genes and their contigs are then extracted and used to assembly a draft genome sequence for an MGS 3. 详述CONCOCT的binning原理结合序列组成特征 (sequence composition) 和跨样本覆盖度特征 (coverage across multiple samples) 进行binning 在进行binning之前需要将所有样本的reads进行混拼 (coassembly) 得到contigs Sequence composition features 以 k-mer 长度等于5为例 将相互之间成反向互补关系的 5-mers pairs 记做一种，则总共有512种 5-mers，对每一条contig计算其各自 5-mers 的组成频率从而构造出一个长度为v=512的向量 Zi： Zi = (Zi,1, …, Zi,v) 为了保证每个 5-mers 频率的计数非零（为后面的对数转换做准备），进行伪计数处理，然后用该序列的 5-mers的总数进行标准化，得到新的向量 Zi‘： Contig coverage features 用段序列比对软件，将各个样本（总共有M个样本）的reads比对到contigs上，计算每条contigs在每个样本中的 coverage (Mapped reads * read length / contig length)，得到表示 congtig i 的 coverage 的向量 Yi： Yi = (Yi,1, …, Yi,M) 两轮标准化处理： 伪计数处理：额外添加一条比对到该 contig 上的 read；再用该样本内所有 contigs（contigs总数为N）的 coverage 进行标准化，得到新的向量 Yi‘： 然后再在contig内部进行标准化，得到新的向量 Yi‘’： Combine two features 将表示某一个 contig i 的序列组成特征的向量 Zi‘ 和 coverage 特征向量 Yi‘’合并成组成一个新的特征向量 Xi（向量长度为E=V+M），同时进行对数转换： Xi = { log(Zi‘) , log(Yi‘’) } PCA降维，保留能解释至少90%的方差的主成分（共保留前D个主成分，D &lt; E） 矩阵维数变化：N x E =&gt; N x D cluster contigs into bins 使用高斯混合模型 (Gaussian mixture model) 基于高斯混合模型(GMM)的期望最大化（EM）聚类： 可以把高斯混合模型简单理解为k-means算法的概率统计版本 回顾k-means聚类 k-means聚类有这样一个数据分布的假设：属于同一簇的数据样本聚集成近乎于圆形的团 k-means的优点是速度非常快，因为我们真正要做的就是计算点和组中心之间的距离；计算量少！因此，它具有线性复杂性o（n） 另一方面，k-means有两个缺点。首先，您必须先确定聚类的簇数量。理想情况下，对于一个聚类算法，我们希望它能帮我们解决这些问题，因为它的目的是从数据中获得一些洞察力。k-均值也从随机选择聚类中心开始，因此它可能在算法的不同运行中产生不同的聚类结果。因此，结果可能不可重复，缺乏一致性。 GMM算法 k-means的一个主要缺点是它简单地使用了集群中心的平均值，对于以圆形方式聚集成簇的数据，k-means算法的聚类效果很好，但是当簇不是圆形时，k均值会失效，这也是将均值用作簇中心的后果 通过下面的图片，我们可以看到为什么这不是最好的方式 在左手边，人眼可以很明显地看到，有两个半径不同的圆形星团以相同的平均值为中心。k-means不能处理这个问题，因为不同簇的平均值非常接近 高斯混合模型（gmms）具有比K-means更好的灵活性。使用GMMs，其基于的数据分布的假设为数据点是高斯分布，可以简单理解为属于同一簇的数据样本聚集成近乎于环状或椭圆形的团，相对于环形或椭圆形的数据而言，这个假设的严格程度与均值相比弱很多 那高斯混合模型是如何实现聚类的呢？ 与k-means算法类似，首先需要知道数据需要聚成k个簇，然后随机初始化k个簇各种所对应的高斯分布的两个参数：均值μ与方差σ，然后基于EM算法从数据中学到各个簇最佳的参数：C1 ~ N(μ1, σ12), C2 ~ N(μ2, σ22) … Ck ~ N(μk, σk2) 接着就可以基于上一步学习到的参数对每个数据点划分簇的归属了，比如对于样本i，其簇的归属为： 4. 拆解CONCOCT的流程4.1. Assembling Metagenomic Reads1234567# 将多个样本的测序数据fastq文件，按照双端分别进行合并$ cat $CONCOCT_TEST/reads/Sample*_R1.fa &gt; All_R1.fa$ cat $CONCOCT_TEST/reads/Sample*_R2.fa &gt; All_R2.fa# 拼接$ velveth velveth_k71 71 -fasta -shortPaired -separate All_R1.fa All_R2.fa$ velvetg velveth_k71 -ins_length 400 -exp_cov auto -cov_cutoff auto velveth: takes in a number of sequence files, produces a hashtable, then outputs two files in an output directory (creating it if necessary), Sequences and Roadmaps, which are necessary to velvetg. 语法： 12&gt; ./velveth output_directory hash_length [[-file_format][-read_type] filename]&gt; velvetg: Velvetg is the core of Velvet where the de Bruijn graph is built then manipulated 4.2. Cutting up contigs将大片段的contigs (&gt;=20kb)，切成一个个10kb的小片段，当切到尾部只剩不到20kb时，停止切割，以防切得过碎 1python $CONCOCT/scripts/cut_up_fasta.py -c 10000 -o 0 -m contigs/velvet_71.fa &gt; contigs/velvet_71_c10K.fa 4.3. Map, Remove Duplicate and Quant Coverage 使用 Bowtie2 执行 mapping 操作 用 MarkDuplicates（Picard中的一个工具） 去除 PCR duplicates 用 BEDTools genomeCoverageBed 基于 mapping 得到的 bam 文件计算每个contigs的coverage （1） Map, Remove Duplicate 其中1、2步操作可以由CONCOCT中提供的脚本map-bowtie2-markduplicates.sh完成 先要自行建好这些contigs的bowtie2索引 12# index for contigs$ bowtie2-build contigs/velvet_71_c10K.fa contigs/velvet_71_c10K.fa 用map-bowtie2-markduplicates.sh脚本完成 mapping -&gt; remove duplicate 123456for f in $CONCOCT_TEST/reads/*_R1.fa; do mkdir -p map/$(basename $f); cd map/$(basename $f); bash $CONCOCT/scripts/map-bowtie2-markduplicates.sh -ct 1 -p &apos;-f&apos; $f $(echo $f | sed s/R1/R2/) pair $CONCOCT_EXAMPLE/contigs/velvet_71_c10K.fa asm bowtie2; cd ../..;done -c option to compute coverage histogram with genomeCoverageBed -t option is number of threads -p option is the extra parameters given to bowtie2. In this case -f -k 保留中间文件 随后的5个参数： pair1, the fasta/fastq file with the #1 mates pair2, the fasta/fastq file with the #2 mates pair_name, a name for the pair used to prefix output files assembly, a fasta file of the assembly to map the pairs to assembly_name, a name for the assembly, used to postfix outputfiles outputfolder, the output files will end up in this folder 如果要自己逐步执行第1、2两步，则可以通过以下方式实现： 12345678910111213141516171819202122# Index reference, Burrows-Wheeler Transform$ bowtie2-build SampleA.fasta SampleA.fasta# Align Paired end, sort and indexbowtie2 \ -p 32 \ -x SampleA.fasta \ -1 $Data/SampleA.1.fastq \ -2 $Data/SampleA.2.fastq | \ samtools sort -@ 18 -O BAM -o SampleA.sort.bamsamtools index SampleA.sort.bam# Mark duplicates and indexjava -Xms32g -Xmx32g -XX:ParallelGCThreads=15 -XX:MaxPermSize=2g -XX:+CMSClassUnloadingEnabled \ -jar picard.jar MarkDuplicates \ I=./SampleA.sort.bam \ O=./SampleA.sort.md.bam \ M=./SampleA.smd.metrics \ VALIDATION_STRINGENCY=LENIENT \ MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=1000 \ REMOVE_DUPLICATES=TRUE # 该参数默认为false，即在输出中不过滤duplicate，但是会对这些记录的flag进行修改标记samtools index ./SampleA.sort.md.bam （2）Quant Coverage 第3步，计算每个contigs的coverage，用gen_input_table.py脚本 12345678# usage: gen_input_table.py [-h] [--samplenames SAMPLENAMES] [--isbedfiles] fastafile bamfiles [bamfiles ...]# --samplenames 写有样品名的文件，每个文件名一行# --isbedfiles 如果在上一步map时运行了genomeCoverageBed，则可以加上此参数后直接用 *smds.coverage文件。如果没运行genomeCoverageBed，则不加此参数，依旧使用bam文件。$ python $CONCOCT/scripts/gen_input_table.py --isbedfiles \ --samplenames &lt;(for s in Sample*; do echo $s | cut -d&apos;_&apos; -f1; done) \ ../contigs/velvet_71_c10K.fa */bowtie2/asm_pair-smds.coverage \ &gt; concoct_inputtable.tsv 注： 这个脚本可以接受两种类型的输入 （1）对bamfiles执行genomeCoverageBed (bedtools genomecov得到的*smds.coverage文件，此时要使用--isbedfiles参数，这样脚本只执行下面提到的第2步操作——计算每条contig的平均depth（又称为这条contig的abundance）； （2）原始的bamfiles，则脚本要执行下面提到的两步操作； 也可以自己写命令逐步实现，这样有利于加深对工具的理解 计算每条contig的depth分布（histograms） 1$ bedtools genomecov -ibam ./SampleA.smds.bam &gt; ./SampleA.smds.coverage bedtools genomecov默认计算histograms，如输出为chr1 0 980 1000，则说明在contig chr1上depth=0的碱基数为980bp，该contig长度为1000bp 例如： 12345678910111213141516&gt; $ cat A.bed&gt; chr1 10 20&gt; chr1 20 30&gt; chr2 0 500&gt; &gt; $ cat my.genome&gt; chr1 1000&gt; chr2 500&gt; &gt; $ bedtools genomecov -i A.bed -g my.genome&gt; chr1 0 980 1000 0.98&gt; chr1 1 20 1000 0.02&gt; chr2 1 500 500 1&gt; genome 0 980 1500 0.653333&gt; genome 1 520 1500 0.346667&gt; 输出格式为： chromosome depth of coverage from features in input file number of bases on chromosome (or genome) with depth equal to column 2 size of chromosome (or entire genome) in base pairs size of chromosome (or entire genome) in base pairs 计算每条contig的平均depth 有两种计算方法： 或 第二种计算方法本质上就是加权平均 12345678910awk &apos;BEGIN &#123;pc=&quot;&quot;&#125; &#123; c=$1; if (c == pc) &#123; cov=cov+$2*$5; &#125; else &#123; print pc,cov; cov=$2*$5; pc=c&#125;&#125; END &#123;print pc,cov&#125;&apos; SampleA.smds.coverage | tail -n +2 &gt; SampleA.smds.coverage.percontig （3）Generate linkage table 接着要构建 linkage per sample between contigs，目前不是很理解它这一步的目的 尝试作简单的理解： 123456789101112131415161718192021222324# usage: bam_to_linkage.py [-h] [--samplenames SAMPLENAMES] [--regionlength REGIONLENGTH] [--fullsearch] [-m MAX_N_CORES] [--readlength READLENGTH] [--mincontiglength MINCONTIGLENGTH] fastafile bamfiles [bamfiles ...]# --samplenames 写有样品名的文件，每个文件名一行# --regionlength contig序列中用于linkage的两端长度 [默认 500]# --fullsearch 在全部contig中搜索用于linkage# -m 最大线程数，每个ban文件对应一个线程# --readlength untrimmed reads长度 [默认 100]# --mincontiglength 识别的最小contig长度 [默认 0]cd $CONCOCT_EXAMPLE/mappython bam_to_linkage.py -m 8 --regionlength 500 --fullsearch --samplenames sample.txt $DATA/SampleA.fasta ./SampleA.smds.bam &gt; SampleA_concoct_linkage.tsvmv SampleA_concoct_linkage.tsv ../concoct-input# 输出文件格式# 共2+6*i列 (i样品数)，依次为contig1、contig2、nr_links_inward_n、nr_links_outward_n、nr_links_inline_n、nr_links_inward_or_outward_n、read_count_contig1_n、read_count_contig2_n# where n represents sample name. # Links只输出一次，如 contig1contig2 输出，则 contig2contig1 不输出# contig1: Contig linking with contig2# contig2: Contig linking with contig1# nr_links_inward: Number of pairs confirming an inward orientation of the contigs -&gt;&lt;-# nr_links_outward: Number of pairs confirming an outward orientation of the contigs &lt;--&gt; # nr_links_inline: Number of pairs confirming an outward orientation of the contigs -&gt;-&gt;# nr_links_inward_or_outward: Number of pairs confirming an inward or outward orientation of the contigs. This can be the case if the contig is very short and the search region on both tips of a contig overlaps or the --fullsearch parameter is used and one of the reads in the pair is outside# read_count_contig1/2: Number of reads on contig1 or contig2. With --fullsearch read count over the entire contig is used, otherwise only the number of reads in the tips are counted. 参考资料： (1) Quince C, Walker A W, Simpson J T, et al. Shotgun metagenomics, from sampling to analysis[J]. Nature Biotechnology, 2017, 35(9):833. (2) Nielsen H B, Almeida M, Juncker A S, et al. Identification and assembly of genomes and genetic elements in complex metagenomic samples without using reference genomes[J]. Nature Biotechnology, 2014, 32(8):822-828. (3) Sangwan N, Xia F, Gilbert J A. Recovering complete and draft population genomes from metagenome datasets[J]. Microbiome, 2016, 4(1):8. (4) Abubucker, S. et al. Metabolic reconstruction for metagenomic data and its application to the human microbiome. PLoS Comput. Biol. 8, e1002358(2012). (5) Beaulaurier J, Zhu S, Deikus G, et al. Metagenomic binning and association of plasmids with bacterial host genomes using DNA methylation.[J]. Nature Biotechnology, 2017, 36(1). (6) Alneberg, J. et al. Binning metagenomic contigs by coverage and composition. Nat. Methods 11, 1144–1146 (2014). (7) CONCOCT’s documentation (8) Manual for Velvet (9) BEDtools官网 (10) 【Yue Zheng博客】宏基因组binning-CONCOCT (11) AI研习社《数据科学中必须熟知的5种聚类算法》]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>Bioinformatics</tag>
        <tag>聚类算法</tag>
        <tag>metagenomics</tag>
        <tag>Machine-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【编程技巧】shell+R双重并行化——加速分析过程]]></title>
    <url>%2F2019%2F02%2F24%2FDual-Parallelization-for-shell-R%2F</url>
    <content type="text"><![CDATA[问题描述在进行宏基因组shotgun数据分析时，数据集分case和control两组，每组11个样本，通过Gene Profiling得到400多万行的profile（以RPKM方式进行定量），得到的proflie文件有600+MB 接着想用wilcox检验来筛选两组gene abundance存在差异的基因，结果跑了一天一夜没跑出结果 在排除脚本问题后，基本确定问题是数据量太大，计算效率太低导致的，在解决这个问题过程中总结出了一种加速技巧：shell+R的双重并行化 shell并行化我目前所知道的在linux环境下的并行化实现方式有两种： （1）利用Slurm排队系统 可以使用--array=1-11%4参数来指定任务队列编号为1-11，每次最多并行执行4个任务 有两种书写格式： 123456781. 作为sbatch的参数$ sbatch --array=1-11%4 tasks.sh2. 写在shell脚本内部如： #!/bin/bash #SBATCH --array=1-11%4 ... （2）使用do{...} &amp;;done;wait语句 该语句的格式为： 1234567891011121314151617while read ido &#123; ... &#125; &amp;donewait或for((i=0;i&lt;n;i++)do &#123; ... &#125; &amp;donewait 例如： 12345678for((i=0;i&lt;10;i++))do &#123; echo $i sleep 5 &#125; &amp;donewait 上面的命令如果不使用并行化，则总的执行时间为10*5=50秒，而并行之后只需要5秒 可以在该命令模块外面再套一层循环，来控制每次并行化的任务数量，这个需求在大多数情况下是非常必要的：若总的任务很多，如果不对每一次并行化的任务数进行控制，而是一股脑地全部提交并行任务，非常容易到达服务器的负荷上限，那么等着你的很可能就是其他用户的骂声一片和服务器管理员的警告了！ 例如，从1到100，每次输出5个数，每次输出后间隔5秒后再继续下一轮的输出： 1234567891011for((i=0;5*i&lt;100;i++))do for((j=5*i+1;j&lt;5*i+6;j++)) do &#123; echo $j sleep 5 &#125; &amp; done waitdone R并行化R的并行化是在 *apply 系列函数的基础上产生的，所以在介绍R的并行化之前，有必要对 *apply 系列函数作一个简单的说明，下面只对apply( )进行说明 函数语法格式：apply(x, margin, fun, ...) x：一个data.frame或者是一个matrix margin：选择1或者2，1表示行，2表示列 fun：一个函数对象，可以是R自带的，也可以是用户自定义的函数 ...：传递给函数fun的其他变量 例如：apply(x,1,sum)，将变量x逐行传递给函数sum，进行求和，得到的是变量x每行的和的列表 一个任务之所以能够进行并行化处理，是因为该任务可以被拆分成许多个相互独立的小任务，每个小任务的求解对其他任务没有任何影响，因此可以对它们进行分而治之，最后将每个小任务求解结果进行汇总，即简单地合并，apply函数实现的就是这样的任务，将一个比较大的变量X按照行（margin=1）或列（margin=2）进行拆分，然后对每个行分别独立地进行求解 但是*apply系列函数的分治策略并没有进行并行化，是一个只利用了一个线程的串行任务，但是由于它本身的分治属性，对它进行简单地改造和封装，就可以实现高效地并行化，这便是parallel包 1234567891011121314151617library(parallel)# 初始化一个并行化的集群## 设置集群使用的核心数if (is.na(Args[4]))&#123; no_cores &lt;- detectCores() - 10&#125; else &#123; no_cores &lt;- as.integer(Args[4])&#125;## 按照指定的核心数，初始化一个集群对象cl &lt;- makeCluster(no_cores)# 调用parApply函数，执行并行化任务out &lt;- parApply(cl,data,1,fun)# 任务结束后，关闭集群对象stopCluster(cl) shell+R双重并行化实现案例有了上面提到的shell与R的并行化的实现方法，那么我们就可以将其运用于本文一开始提到的问题 并行化的思路： （1）先将原始的大profile文件进行拆分，若每m行拆成一个subprofile，可以拆成n个subprofiles （2）对这n个subprofiles，按照每i个执行一次并行化任务（shell并行化），给每个并行化任务j个核心（R并行化） shell脚本 脚本名：ParWilcox.sh 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 参数说明：# - Profile files# - rows per sub-profile# - maximum tasks per run# - outdirprofiles=$1nrow_per=$2num_tasks=$3outdir=$4if [ ! -d $outdir ]then mkdir -p $outdirfiif [ -f $outdir/SubProfiles.list ]then rm $outdir/SubProfiles.listfi##################################需要修改的变量##################################workdir=/Path/To/Workdir################################################################################# 分割原始Profilesnline=$(wc -l $profiles | awk &apos;&#123;printf $1&#125;&apos;)for((i=0;i*nrow_per&lt;nline;i++))do awk -v j=$i -v n=$nrow_per &apos;NR==1 || (NR&gt;j*n+1 &amp;&amp; NR&lt;=(j+1)*n+1)&apos; $profiles &gt;$outdir/SubProfiles-$&#123;i&#125; echo &quot;SubProfiles-$&#123;i&#125;&quot; &gt;&gt;$outdir/SubProfiles.listdone# 并行化执行多个sub-profile的wilcox检验num_SubProfiles=$(wc -l $outdir/SubProfiles.list | awk &apos;&#123;printf $1&#125;&apos;)for((i=0;i*num_tasks&lt;num_SubProfiles;i++))do awk -v j=$i -v n=$num_tasks &apos;NR&gt;j*n &amp;&amp; NR&lt;=(j+1)*n&apos; $outdir/SubProfiles.list &gt; $outdir/current_subprofiles.List while read subprofile do &#123; Rscript $workdir/Script/wilcox-sided-parallel.R $outdir/$subprofile $workdir/Gene_abundance/SampleGroup.txt $outdir/statOut.$subprofile 2 &#125; &amp; done &lt; $outdir/current_subprofiles.List waitdonecat $outdir/statOut.* &gt;$outdir/statOutrm $outdir/statOut.* 被shell脚本调用的执行wilcox检验的R脚本 脚本名：wilcox-sided-parallel.R 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# 三个参数，按顺序分别为：# - profiles matrix file# - sample group file# - outfile name# - threads number（默认为服务器总线程-10）Args &lt;- commandArgs(T)library(parallel)# Initiate clusterif (is.na(Args[4]))&#123; no_cores &lt;- detectCores() - 10&#125; else &#123; no_cores &lt;- as.integer(Args[4])&#125;cl &lt;- makeCluster(no_cores)data &lt;- read.table(Args[1],head=T,row.names=1,sep=&apos;\t&apos;)group &lt;- read.table(Args[2],head=T,sep=&apos;\t&apos;)# 找出profiles中每列代表的样本所属的组col_group1 &lt;- colnames(data) %in% sub(&apos;-&apos;,&apos;.&apos;,paste(group$Sample[group$Group==1],&apos;.rpkm&apos;,sep=&apos;&apos;))col_group2 &lt;- colnames(data) %in% sub(&apos;-&apos;,&apos;.&apos;,paste(group$Sample[group$Group==2],&apos;.rpkm&apos;,sep=&apos;&apos;))# 对单个基因进行wilcox检验，并输出统计检验结果，输出格式如下：# geneName,pvalue,group1Median,group2Median,directionwilcox_fun &lt;- function(data,col_group1,col_group2)&#123; g1 &lt;- as.numeric(data[col_group1]) g2 &lt;- as.numeric(data[col_group2]) # 执行wilcox检验 stat &lt;- wilcox.test(g1,g2,paired = FALSE, exact=NULL, correct=TRUE, alternative=&quot;two.sided&quot;) if(is.na(stat$p.value))&#123; stat$p.value &lt;- 1 &#125; # 对有统计学意义的基因进行判断，是上调&quot;up&quot;还是下调&quot;down&quot;或者是不变&quot;-&quot;（对于组2，即不吃药组） if(stat$p.value &lt; 0.1 &amp; median(g1) &lt; median(g2))&#123; G12 &lt;- c(ifelse(is.na(stat$p.value),1,stat$p.value),median(g1),median(g2),&apos;down&apos;) &#125; else if(stat$p.value &lt; 0.1 &amp; median(g1) &gt; median(g2))&#123; G12 &lt;- c(ifelse(is.na(stat$p.value),1,stat$p.value),median(g1),median(g2),&apos;up&apos;) &#125; else if(stat$p.value &lt; 0.1 &amp; median(g1) == median(g2))&#123; G12 &lt;- c(ifelse(is.na(stat$p.value),1,stat$p.value),median(g1),median(g2),&apos;-&apos;) &#125; else&#123; G12 &lt;- c() &#125; G12&#125;statOut &lt;- parApply(cl,data,1,wilcox_fun,col_group1,col_group2)stopCluster(cl)# 删除为NULL的列表元素for(i in names(statOut))&#123; if(is.null(statOut[[i]]))&#123; statOut[[i]] &lt;- NULL &#125;&#125;# 将结果写入文件中if(!is.null(statOut))&#123; # 转换成数据框 statOut &lt;- as.data.frame(t(as.matrix(as.data.frame(statOut)))) colnames(statOut) &lt;- c(&apos;pvalue&apos;,&apos;group1Median&apos;,&apos;group2Median&apos;,&apos;direction&apos;) # 写入文件 write.table(statOut,Args[3],sep = &apos;\t&apos;,row.names = T,col.names = T,quote = F)&#125; 执行方法如下： 1$ bash ParWilcox.sh &lt;raw profile&gt; &lt;rows per sub-profile&gt; &lt;tasks per run&gt; &lt;threads&gt; 脚本中用到的样本分组文件，如下： 123456789Sample GroupNP-003A 1NP-017A 1NP-018A 1...NP-007A 2NP-010A 2NP-011A 2...]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[跟着李永乐老师学数学/物理]]></title>
    <url>%2F2019%2F02%2F24%2FFollow-LYL-learning-math-physics%2F</url>
    <content type="text"><![CDATA[数学部分1. 祖暅原理：我国古代数学家算球体积的方法 2. 容斥原理：春节集卡活动集齐的概率有多大 3. 无理数的数学公理化：如何证明0.999… = 1 4. 杨辉三角：性质与应用 物理部分1. 电容：触摸屏原理与触电原因 参考资料： (1) 李永乐老师B站视频]]></content>
      <categories>
        <category>MathIsFunGame</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>物理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实用小脚本]]></title>
    <url>%2F2019%2F02%2F17%2FPractical-Scripts%2F</url>
    <content type="text"><![CDATA[Perl1. 拆分FASTA文件将FASTA文件按照用户指定的序列条数进行拆分，即每n条序列写到一个文件中，或者按照指定的输出文件数进行拆分，则每个文件中包含的序列数相同 脚本名：splitFasta.pl1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#!/usr/bin/perluse strict;use warnings;use Getopt::Long;use POSIX;# 帮助文档=head1 Description This script is used to split fasta file, which is too large with thosands of sequence=head1 Usage $0 -i &lt;input&gt; -o &lt;output_dir&gt; [-n &lt;seq_num_per_file&gt;] [-m &lt;output_file_num&gt;] =head1 Parameters -i [str] Input raw fasta file -o [str] Output file to which directory -n [int] Sequence number per file, alternate chose paramerter &quot;-n&quot; or &quot;-m&quot;, if set &quot;-n&quot; and &quot;-m&quot; at the same time, only take &quot;-n&quot; parameter -m [int] Output file number (default:100)=cutmy ($input,$output_dir,$seq_num,$file_num);GetOptions( &quot;i:s&quot;=&gt;\$input, &quot;o:s&quot;=&gt;\$output_dir, &quot;n:i&quot;=&gt;\$seq_num, &quot;m:i&quot;=&gt;\$file_num );die `pod2text $0` if ((!$input) or (!$output_dir));# 设置每个文件的序列条数if(!defined($seq_num))&#123; if(!defined($file_num))&#123; $file_num=100; my $total_seq_num=`awk &apos;BEGIN&#123;n=0&#125; /^&gt;/&#123;n++&#125; END&#123;print n&#125;&apos; $input`; chomp $total_seq_num; $seq_num=ceil($total_seq_num/$file_num); &#125;else&#123; my $total_seq_num=`awk &apos;BEGIN&#123;n=0&#125; /^&gt;/&#123;n++&#125; END&#123;print n&#125;&apos; $input`; chomp $total_seq_num; $seq_num=ceil($total_seq_num/$file_num); &#125;&#125;open IN,&quot;&lt;$input&quot; or die &quot;Cann&apos;t open $input\n&quot;;my $n_seq=0; # 该变量用于记录当前扫描到的序列数my $n_file=1; # 该变量用于记录当前真正写入的文件的计数my $input_base=`basename $input`;chomp $input_base;open OUT,&quot;&gt;$output_dir/$&#123;input_base&#125;_$&#123;n_file&#125;&quot; or die &quot;Cann&apos;t create $output_dir/$&#123;input_base&#125;_$&#123;n_file&#125;\n&quot;;while(&lt;IN&gt;)&#123; next if (/^\s+$/); # 跳过空行 chomp; if (/^&gt;/)&#123; $n_seq++; # 判断目前已经扫描到的序列数，若大于设定的split的序列数，则创建新文件 if ($n_seq&gt;$seq_num)&#123; $n_seq=1; $n_file++; close OUT; open OUT,&quot;&gt;$output_dir/$&#123;input_base&#125;_$&#123;n_file&#125;&quot; or die &quot;Cann&apos;t create $output_dir/$&#123;input_base&#125;_$&#123;n_file&#125;\n&quot;; print OUT &quot;$_\n&quot;; &#125;else&#123; print OUT &quot;$_\n&quot;; &#125; &#125;else&#123; print OUT &quot;$_\n&quot;; &#125;&#125;close IN; 执行方法： 1$ perl splitFasta.pl -i &lt;input&gt; -o &lt;output_dir&gt; [-n &lt;seq_num_per_file&gt;] [-m &lt;output_file_num&gt;] 具体的参数使用说明可以执行 perl splitFasta.pl，查看脚本使用文档 2. 从双端FASTQ文件中抽取指定数据量(bp)的序列按照用户指定的数据量，即多少bp，从原始的双端FASTQ文件中随机抽取序列，要求双端FASTQ文件中的序列必须配对 脚本名：extractFastq.pl 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182#!/usr/bin/perluse strict;use warnings;use Getopt::Long;# 脚本帮助文档=head1 Description Thise script is used to extract a number of fastq recorders from original input fastq file=head1 Usage $0 -n &lt;totalReads&gt; -e &lt;base-pairs to extract&gt; -l &lt;readLength&gt; -1 &lt;input1.fastq&gt; -2 &lt;input2.fastq&gt; [-o &lt;outdir&gt;]=head1 Parameters -n [int] Total reads number of input.fastq -e [int] Base-pairs per end you want to extract -l [int] The reads length -1 [str] Input 1st-end fastq file -2 [str] Input 2nd-end fastq file -o [str] Output directory [default: current folder]=cutmy ($totalReads,$bpNum,$length,$Input1,$Input2,$Outdir);GetOptions( &quot;n:i&quot;=&gt;\$totalReads, &quot;e:i&quot;=&gt;\$bpNum, &quot;l:i&quot;=&gt;\$length, &quot;1:s&quot;=&gt;\$Input1, &quot;2:s&quot;=&gt;\$Input2, &quot;o:s&quot;=&gt;\$Outdir );$Outdir=`pwd` unless (defined($Outdir));die `pod2text $0` if ((!$totalReads) or (!$bpNum)) or (!$length) or (!$Input1) or (!$Input2);open FQ1,&quot;&lt;$Input1&quot; or die &quot;$!\n&quot;;open FQ2,&quot;&lt;$Input2&quot; or die &quot;$!\n&quot;;my $Input1_basename=`basename $Input1`;chomp $Input1_basename;my $Input2_basename=`basename $Input2`;chomp $Input2_basename;open OUT1,&quot;&gt;$Outdir/$&#123;Input1_basename&#125;.extract&quot; or die &quot;$!\n&quot;;open OUT2,&quot;&gt;$Outdir/$&#123;Input2_basename&#125;.extract&quot; or die &quot;$!\n&quot;;my $readsRemain=$bpNum/$length;my $remainCount=0;while(! eof($FQ1))&#123; # 读入1st-end fastq 文件的四行 my $fq1_1=&lt;FQ1&gt;; my $fq1_2=&lt;FQ1&gt;; my $fq1_3=&lt;FQ1&gt;; my $fq1_4=&lt;FQ1&gt;; chomp($fq1_1,$fq1_2,$fq1_3,$fq1_4); # 读入2nd-end fastq 文件的四行 my $fq2_1=&lt;FQ2&gt;; my $fq2_2=&lt;FQ2&gt;; my $fq2_3=&lt;FQ2&gt;; my $fq2_4=&lt;FQ2&gt;; chomp($fq2_1,$fq2_2,$fq2_3,$fq2_4); # 随机抽取 if (rand()&lt;$readsRemain/$totalReads)&#123; $remainCount++; print OUT1 &quot;$fq1_1\n$fq1_2\n$fq1_3\n$fq1_4\n&quot;; print OUT2 &quot;$fq2_1\n$fq2_2\n$fq2_3\n$fq2_4\n&quot;; &#125;&#125;print &quot;Total reads: $totalReads\n&quot;;print &quot;Theorical remained reads: $readsRemain\n&quot;;print &quot;Practical remained reads: $remainCount\n&quot;;close FQ1;close FQ2;close OUT1;close OUT2; 执行方法： 1$ perl extractFastq.pl -n &lt;totalReads&gt; -e &lt;base-pairs to extract&gt; -l &lt;readLength&gt; -1 &lt;input1.fastq&gt; -2 &lt;input2.fastq&gt; [-o &lt;outdir&gt;] 具体的参数使用说明可以执行 perl extractFastq.pl，查看脚本使用文档 3. 双端FASTQ文件进行双端配对脚本名：PairsMate.pl 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#!/usr/bin/perluse strict;use warnings;use Getopt::Long;# 脚本帮助文档=head1 DescriptionThise script is used to match the pair-end reads from one sample=head1 Usage$0 -1 &lt;input1.fastq&gt; -2 &lt;input2.fastq&gt; [-o &lt;outdir&gt;]=head1 Parameters-1 [str] Input 1st-end fastq file-2 [str] Input 2nd-end fastq file-o [str] Output directory [default: current folder]=cutmy ($Input1,$Input2,$Outdir);GetOptions(&quot;1:s&quot;=&gt;\$Input1,&quot;2:s&quot;=&gt;\$Input2,&quot;o:s&quot;=&gt;\$Outdir);$Outdir=`pwd` unless (defined($Outdir));die `pod2text $0` if (!$Input1) or (!$Input2);open FQ1,&quot;&lt;$Input1&quot; or die &quot;Cann&apos;t open $Input1\n&quot;;open FQ2,&quot;&lt;$Input2&quot; or die &quot;Cann&apos;t open $Input2\n&quot;;my $Input1_basename=`basename $Input1`;chomp $Input1_basename;my $Input2_basename=`basename $Input2`;chomp $Input2_basename;open OUT1,&quot;&gt;$Outdir/$&#123;Input1_basename&#125;.match&quot; or die &quot;Cann&apos;t create $Outdir/$&#123;Input1_basename&#125;.match\n&quot;;open OUT2,&quot;&gt;$Outdir/$&#123;Input2_basename&#125;.match&quot; or die &quot;Cann&apos;t create $Outdir/$&#123;Input2_basename&#125;.match\n&quot;;# 载入两个fastq文件，保存成哈希my (%fq1_seq,%fq1_qua,%fq2_seq,%fq2_qua);while (!eof(FQ1))&#123; my ($id,)=split(/\s/,&lt;FQ1&gt;); $fq1_seq&#123;$id&#125;=&lt;FQ1&gt;; &lt;FQ1&gt;; $fq1_qua&#123;$id&#125;=&lt;FQ1&gt;; chomp ($fq1_seq&#123;$id&#125;,$fq1_qua&#123;$id&#125;);&#125;while (!eof(FQ2))&#123; my ($id,)=split(/\s/,&lt;FQ2&gt;); $fq2_seq&#123;$id&#125;=&lt;FQ2&gt;; &lt;FQ2&gt;; $fq2_qua&#123;$id&#125;=&lt;FQ2&gt;; chomp ($fq2_seq&#123;$id&#125;,$fq2_qua&#123;$id&#125;);&#125;close FQ1;close FQ2;# 双端配对foreach my $key (sort keys %fq1_seq)&#123; if(defined($fq2_seq&#123;$key&#125;))&#123; print OUT1 &quot;$key\n$fq1_seq&#123;$key&#125;\n+\n$fq1_qua&#123;$key&#125;\n&quot;; print OUT2 &quot;$key\n$fq2_seq&#123;$key&#125;\n+\n$fq2_qua&#123;$key&#125;\n&quot;; &#125;&#125;close OUT1;close OUT2; 执行方法： 1$ perl PairsMate.pl -1 &lt;input1.fastq&gt; -2 &lt;input2.fastq&gt; [-o &lt;outdir&gt;] 具体的参数使用说明可以执行 perl PairsMate.pl，查看脚本使用文档 4. 根据序列Id提取FASTA序列提供FASTA文件，和包含序列Id的文件（有多列，用制表符隔开，其中一列为序列Id），提取该序列Id对应的FASTA序列 脚本名：extractSeqFromFasta.pl 12345678910111213141516171819202122232425262728293031323334353637#!/usr/bin/perluse strict;use warnings;my geneList = $ARGV[0];my fasta = $ARGV[1];my out = $ARGV[2];my %Hash_fasta;my $seqId;# 读入fasta文件，存为哈希open FA,&quot;&lt;$fasta&quot; or die &quot;$!\n&quot;;while(&lt;FA&gt;)&#123; chomp; next if(/^\s?$/); if(/^&gt;(.+)$/)&#123; $seqId = $1; &#125;else&#123; $Hash_fasta&#123;$seqId&#125; .= $_; &#125;&#125;close(FA);# 逐行读入geneList文件，并从上一步的哈希中将该序列提取出来open LIST,&quot;&lt;$geneList&quot; or die &quot;$!\n&quot;;open OUT,&quot;&gt;$out&quot; or die &quot;$!\n&quot;;while(&lt;LIST&gt;)&#123; chomp; next if(/^\s?$/); my $gene = $_; if($Hash_fasta&#123;$gene&#125;)&#123; print OUT &quot;&gt;$gene\n$Hash_fasta&#123;$gene&#125;\n&quot;; &#125;&#125;close(LIST);close(OUT); 使用方法： 1$ perl extractSeqFromFasta.pl &lt;in.fasta&gt; &lt;gene list&gt; &lt;out.fasta&gt; 5. 统计FastQC输出统计FastQC结果，注意脚本需要在FastQC输出结果所在目录下运行。脚本的统计项目及输出如下： Sample Total Reads GC Content Q20 Q30 NP006_RRS05401_1.clean.fq.extract_fastqc 39998924 49 0.999430935 0.946251454 NP006_RRS05401_2.clean.fq.extract_fastqc 39998924 49 0.986292131 0.883245809 NP007_RRS05402_1.clean.fq.extract_fastqc 40003323 49 0.999429311 0.946197936 脚本名：fastqc_stat.pl 12345678910111213141516171819202122232425262728293031323334353637383940opendir (DIR, &quot;./&quot;) or die &quot;can&apos;t open the directory!&quot;;@dir = readdir DIR;foreach $file ( sort @dir) &#123;# 跳过不需要的文件/文件夹，留下需要的文件夹next unless -d $file;next if $file eq &apos;.&apos;;next if $file eq &apos;..&apos;;# 提取total reads$total_reads= `grep &apos;^Total&apos; ./$file/fastqc_data.txt`;$total_reads=(split(/\s+/,$total_reads))[2];# 提取%GC$GC= `grep &apos;%GC&apos; ./$file/fastqc_data.txt`;$GC=(split(/\s+/,$GC))[1];chomp $GC;# 提取Q20，Q30## 读入Per sequence quality scores部分的信息，保存成哈希open FH , &quot;&lt;./$file/fastqc_data.txt&quot;;while (&lt;FH&gt;) &#123; next unless /#Quality/; while (&lt;FH&gt;) &#123; @F=split; $hash&#123;$F[0]&#125;=$F[1]; last if /&gt;&gt;END_MODULE/; &#125; &#125;## 统计Q20，Q30$all=0;$Q20=0;$Q30=0;$all+=$hash&#123;$_&#125; foreach keys %hash;$Q20+=$hash&#123;$_&#125; foreach 0..20;$Q30+=$hash&#123;$_&#125; foreach 0..30;$Q20=1-$Q20/$all;$Q30=1-$Q30/$all;print &quot;$file\t$total_reads\t$GC\t$Q20\t$Q30\n&quot;;&#125; 执行方法： 1$ perl fastqc_stat.pl Python1. 格式化FASTA文件在规范的Fasta文件中，你看到的一条序列记录包括两部分： 以&gt;起始的序列名称，占一行； 由核苷酸ATCGN（核酸序列）或氨基酸字符组成的字符串，一般每60个字符一行，若一条序列很长，那么它可能会占多行； 有的时候因为一些原因（一般都是自己在上游分析时图方便生成的）你得到的fasta文件中的序列部分没有按照60个字符一行的形式进行组织，而是将整条序列放在一行里，虽然一般来说这并不会对你的分析产生太大的影响，但是进行查看的时候会有一些不方便，比如 这个时候如果想将它调整组成规范的格式，要怎么实现呢？ 用BioPython将原始FASTA文件读入，然后在写出，就能得到你想要的效果 这个脚本很简单，只有四行代码 脚本名：formatFasta.py 123456from Bio import SeqIOimport sysSeq = [ seq for seq in SeqIO.parse(sys.argv[1],&apos;fasta&apos;)]SeqIO.write(Seq,sys.argv[2],&apos;fasta&apos;) 用法： 1$ python formatFasta.py &lt;in.fa&gt; &lt;out.fa&gt; R1. 合并多个样本的定量结果成一个大矩阵 (profile matrix)在RNAseq或者metagenome shotgun中，需要对每个样本进行逐一定量，生成每个样本各种的定量结果，然后需要合并每个样本的定量结果，形成一个包含所有样本定量的profile matrix 我们这个脚本的要实现的就是合并多个样本的定量结果成一个大矩阵 (profile matrix) 实现思路： 想要保证所有样本的定量结果文件都保存在一个文件夹下，且每个文件的命名形式为：sample+suffix，即样本名+固定后缀； 每个文件中只有两列信息，第一列为feature（对于RNAseq，它的feature是gene或transcript；对于metagenome shotgun，它的feature是各个物种分类层级），第二列是定量结果（quant，对于RNAseq，它的定量可以是reads count、FPKM/RPKM、TPM等；对于metagenome shotgun，它的定量一般是一个取值在[0,1]的比例值）； 根据指定的文件夹（下面记作dir）和文件后缀（下面记作pattern），将文件路径为dir/*pattern的文件逐一读入，获取它们的第一列，即features的列表，从而得到total unique features的列表，同时记下样本列表（样本名为：文件名-pattern）； 根据上一步得到的total unique features列表和样本列表，创建一个行数等于total unique features列表长度，列数等于样本列表的矩阵（记作$\text{MergeMatrix}_{ij}$，i表示$\text{feature}_i$，j表示$\text{sample}^j$），行名对应total unique features列表，列名对应样本列表，矩阵中每个元素的值初始化为0； 再一次将文件路径为dir/*pattern的文件逐一读入，更新MergeMatrix对应列的值：若当前读入的文件名为sample_j.pattern，则当前样本为$\text{sample}^j$，则根据$\text{MergeMatrix}_{ij}=\text{sample}i^j$，对矩阵$\text{MergeMatrix}{ij}$进行更新； 脚本名：Merge2ProfileMatrix.R 12345678910111213141516171819202122232425262728loadMatrix &lt;- function(dir,pattern)&#123; files &lt;- list.files(dir) clones &lt;- vector() # 用于保存所有样本中出现的克隆 samples &lt;- vector() # 用于保存所有样本的id # 获取所有样本中出现的unique克隆，与所有样本的id for (file in files) &#123; if (grepl(pattern,file))&#123; data &lt;- read.table(paste(dir,file,sep=&quot;/&quot;),header=F,sep=&quot;\t&quot;) clones &lt;- c(clones,as.character(data$V1)) samples &lt;- c(samples,sub(pattern,&quot;&quot;,file)) &#125; &#125; # 创建用于保存所有样本整合的表达谱，以0填充 clones &lt;- unique(clones) dataMat &lt;- matrix(rep(0,length(samples)*length(clones)),nrow = length(clones),ncol = length(samples)) rownames(dataMat) &lt;- clones colnames(dataMat) &lt;- samples # 逐一读入样本的count文件，对matrix中对应的元素进行赋值 for (file in files) &#123; if (grepl(pattern,file))&#123; data &lt;- read.table(paste(dir,file,sep=&quot;/&quot;),header=F,sep=&quot;\t&quot;) sample &lt;- as.character(sub(pattern,&quot;&quot;,file)) index &lt;- match(data$V1,rownames(dataMat)) dataMat[index,sample] &lt;- data$V2 &#125; &#125; dataMat&#125; 该脚本只定义了一个函数，可以在R交互模式下，载人这个脚本中的函数，然后调用它，或者直接对这个脚本进行修改，然后用Rscript直接执行修改后的脚本 2. 对profiles进行wilcox检验（并行化）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 四个参数，按顺序分别为：# - profiles matrix file# - sample group file # - outfile name# - threads number（默认为服务器总线程-10）Args &lt;- commandArgs(T)library(parallel)# Initiate clusterif(is.na(Args[4])) &#123; no_cores &lt;- detectCores() - 10&#125; else &#123; no_cores &lt;- as.integer(Args[4])&#125;cl &lt;- makeCluster(no_cores)data &lt;- read.table(Args[1],head=T,row.names=1,sep=&apos;\t&apos;)group &lt;- read.table(Args[2],head=T,sep=&apos;\t&apos;)# 找出profiles中每列代表的样本所属的组col_group1 &lt;- colnames(data) %in% sub(&apos;-&apos;,&apos;.&apos;,paste(group$Sample[group$Group==1],&apos;.rpkm&apos;,sep=&apos;&apos;))col_group2 &lt;- colnames(data) %in% sub(&apos;-&apos;,&apos;.&apos;,paste(group$Sample[group$Group==2],&apos;.rpkm&apos;,sep=&apos;&apos;))# 对单个基因进行wilcox检验，并输出统计检验结果，输出格式如下：# geneName,pvalue,group1Median,group2Median,directionwilcox_fun &lt;- function(data,col_group1,col_group2)&#123; g1 &lt;- as.numeric(data[col_group1]) g2 &lt;- as.numeric(data[col_group2]) # 执行wilcox检验 stat &lt;- wilcox.test(g1,g2,paired = FALSE, exact=NULL, correct=TRUE, alternative=&quot;two.sided&quot;) if(is.na(stat$p.value))&#123; stat$p.value &lt;- 1 &#125; # 对有统计学意义的基因进行判断，是上调&quot;up&quot;还是下调&quot;down&quot;或者是不变&quot;-&quot;（对于组2，即不吃药组） if(stat$p.value &lt; 0.1 &amp; median(g1) &lt; median(g2))&#123; G12 &lt;- c(ifelse(is.na(stat$p.value),1,stat$p.value),median(g1),median(g2),&apos;down&apos;) &#125; else if(stat$p.value &lt; 0.1 &amp; median(g1) &gt; median(g2))&#123; G12 &lt;- c(ifelse(is.na(stat$p.value),1,stat$p.value),median(g1),median(g2),&apos;up&apos;) &#125; else if(stat$p.value &lt; 0.1 &amp; median(g1) == median(g2))&#123; G12 &lt;- c(ifelse(is.na(stat$p.value),1,stat$p.value),median(g1),median(g2),&apos;-&apos;) &#125; else&#123; G12 &lt;- c() &#125; G12&#125; statOut &lt;- parApply(cl,data,1,wilcox_fun,col_group1,col_group2)stopCluster(cl)# 删除为NULL的列表元素for(i in names(statOut))&#123; if(is.null(statOut[[i]]))&#123; statOut[[i]] &lt;- NULL &#125;&#125;# 将结果写入文件中if(!is.null(statOut))&#123; # 转换成数据框 statOut &lt;- as.data.frame(t(as.matrix(as.data.frame(statOut)))) colnames(statOut) &lt;- c(&apos;pvalue&apos;,&apos;group1Median&apos;,&apos;group2Median&apos;,&apos;direction&apos;) # 写入文件 write.table(statOut,Args[3],sep = &apos;\t&apos;,row.names = T,col.names = T,quote = F)&#125; 3. 对差异显著的subprofile进行可视化——箱线图与热图想法： 从上游的分析中得到两种样本profile的差异统计检验结果，其中有p-value和p-adjust，可以根据p-value或p-adjust设置阈值，筛选出有统计学显著性的差异observation 根据这些observation的Id从原始profile中将这些差异observation的部分提取了得到subprofile 对subprofile进行可视化——箱线图（boxplot）与热图（heatmap） 需要提供的输入： 上游的分析中得到两种样本profile的差异统计检验结果 原始profile 样本的分组信息 脚本名：plotDiffSubprofile.R 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778# 实现功能说明：导入差异分析的结果与原始profile，提取原始profile中提取# 差异显著（通过第3个参数设置筛选的阈值）的集合绘制热图/箱线图## 考虑到可能直接得到的画图输出不满足用户的需求，所以提供了一个参数来控# 制是否输出画图用的数据，以便用户可以直接载人绘图数据，根据自己的需求# 灵活画图########################################################################## 必须参数说明：# - output file from wilcox stat# - profiles matrix file# - padj threshold# - min memmbers for subprofile# - sample group file, 2 columns, 1st column corresponding to sample, 2nd# column corresponding to group# - logic value, False or True, whether to output prepared data frame for# ggplot-boxplot and filted subprofile for heatmap, saved as raw txt and# Rdata(subprofile_melt--boxplot, subprofile_matrix,anno_col--heatmap) formats#########################################################################library(reshape2)library(ggplot2)library(pheatmap)Args &lt;- commandArgs(T)stat &lt;- read.table(Args[1],header=T,sep=&apos;\t&apos;)profile &lt;- read.table(Args[2],head=T,sep=&apos;\t&apos;)padj &lt;- as.numeric(Args[3])num_min &lt;- as.numeric(Args[4])group &lt;- read.table(Args[5],head=T,sep=&apos;\t&apos;)stat_filt &lt;- stat[stat$pAdj&lt;=padj,]# 若差异的集合太小则不进行后续的绘图操作if(dim(stat_filt)[1]&gt;=num_min)&#123; # 提取出差异的subprofile subprofile &lt;- profile[profile[,1] %in% rownames(stat_filt),] # 将数据框的短格式展开为长格式 subprofile_melt &lt;- melt(subprofile,id.vars=1,variable.name = &apos;sample&apos;,value.name = &apos;abundance&apos;) colnames(subprofile_melt) &lt;- c(&apos;ID&apos;,&apos;sample&apos;,&apos;abundance&apos;) # 添加分组信息 subprofile_melt$group &lt;- NULL group_factor &lt;- unique(group[,2]) # 获得所有组别及其表示符 for(i in group_factor)&#123; subprofile_melt$group[subprofile_melt$sample %in% sub(&apos;-&apos;,&apos;.&apos;,group[group[,2]==i,1])] &lt;- i &#125; subprofile_melt$group &lt;- as.factor(subprofile_melt$group) # 分组变量必须为factor # 画boxplot # png(paste(Args[2],&quot;.boxplotDiff.png&quot;,sep=&apos;&apos;)) p_box &lt;- ggplot(subprofile_melt) p_box + geom_boxplot(aes(x=ID,y=abundance,fill=group)) # dev.off() ggsave(paste(Args[2],&quot;.boxplotDiff.png&quot;,sep=&apos;&apos;)) # 画heatmap ## 整理绘图数据 subprofile_matrix &lt;- as.matrix(subprofile[,-1]) rownames(subprofile_matrix) &lt;- subprofile[,1] ## 准备heatmap的注释信息 anno_col &lt;- data.frame(group=as.factor(group[,2])) rownames(anno_col) &lt;- sub(&apos;-&apos;,&apos;.&apos;,group[,1]) png(paste(Args[2],&quot;.heatmapDiff.png&quot;,sep=&apos;&apos;)) pheatmap(subprofile_matrix,scale=&quot;row&quot;,cluster_rows=T,cluster_cols=T,annotation_col=anno_col,show_rownames = F) dev.off() # 若最后一个参数为True则写出数据 if(Args[6])&#123; write.table(subprofile_melt,paste(Args[2],&quot;.boxplotDiff.data&quot;,sep=&apos;&apos;),row.names=F,col.names=T,sep=&apos;\t&apos;,quote=F) # 画箱线图的数据 write.table(subprofile_matrix,paste(Args[2],&quot;.heatmapDiff.data&quot;,sep=&apos;&apos;),row.names=T,col.names=T,sep=&apos;\t&apos;,quote=F) # 画热图的数据 write.table(anno_col,paste(Args[2],&quot;.heatmapDiff.anno&quot;,sep=&apos;&apos;),row.names=T,col.names=T,sep=&apos;\t&apos;,quote=F) # 画热图的注释数据 save(subprofile_melt,subprofile_matrix,anno_col,file=paste(Args[2],&quot;.Rdata&quot;,sep=&apos;&apos;)) &#125;&#125;else&#123; print(&quot;Not enough memmbers of subprofile passing the padj threshold!&quot;)&#125; 4. 对大矩阵计算Jaccard Index（并行化+分治）在对一个大矩阵执行相关性计算或Jaccard Index的计算时，其实执行的是矩阵任意两行（这里假设要进行分析的对象是矩阵的每个行）之间的两两的计算，若这个矩阵的规模非常庞大，有n行时，计算的时间复杂度就是$O(n^2)$，这个时候可以采用并行化策略来加速这个进程（参考上文的 2. R中的并行化方法）： 1StatOut &lt;- parApply(cl, data, 1, fun, data) 这样就会实现将一个 n vs. n 的问题拆分成 n 个可以并行解决的 1 vs. n 的问题，当然通过设置线程数为$m,\,(m\le n)$，使得每次只并行执行m个 1 vs. n 的问题 然后再在函数fun内部再定义一个并行化计算来进一步并行化加速上面产生的 1 vs. n 的计算： 1234fun &lt;- function(vec, data)&#123; ... parApply(cl, data, 1, fun2, vec)&#125; 在这个函数内部实现了将一个 1 vs. n 的问题拆分成 n 个可以并行解决的 1 vs. 1 的问题 这样就实现了两步并行化，这样在保证硬件条件满足的情况下，的确能显著加快分析速度 但是并行化技术会带来一个问题是，虽然时间开销减少了，但是空间开销显著增加了 比如，第一次并行化，将一个 n vs. n 的问题拆分成 $\frac{n}{m}$ 次可以并行解决的 m个 1 vs. n 的问题，则需要在每一次并行化任务中拷贝一个 1 vs. n 的计算对象，即原始有n行的矩阵被拷贝了m次，则相应的缓存空间也增加了m倍，很显然内存的占用大大增加了 空间开销显著增加带来的后果就是，很容易导致运行内存不足程序运行中断的问题，那该怎么解决这个问题呢？ 可以采用分治方法（Divide-Conquer)，将原始大矩阵，按照行拆分成多个小的子块，对每个子块执行计算，从而得到每个子块的运算结果，最后再讲每个子块的结果进行合并： 脚本名：JaccardIndex.R 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111############################################################ 该脚本用于对Observed矩阵（行为Feature，列为Sample）# 对feature（即行）计算Jaccard Index，可以进行并行化计算# 考虑到原始矩阵可能过大（主要是行太多），如果直接对# 它执行并行化计算的话，非常容易导致内存溢出，为了解决# 这个问题，引入了分治方法：# 将原始矩阵按照行等分n份（平均每份有abs(N/n)，最后一# 份可能无法进行等分，最后一份为N%n），然后对这n份的每两# 份之间分别执行并行化计算，则总共有n^2次，得到n^2个子结# 果进行合并############################################################ 参数说明：# （1）Observed Matrix，每行表示一个feature，每列表示一个样本# （2）并行化使用的线程数# （3）切分的分数（默认不切块）library(parallel)Args &lt;- commandArgs(TRUE)inFile &lt;- Args[1]# 开启并行化if(!is.na(Args[2]))&#123; no_cores &lt;- as.integer(Args[2]) cl &lt;- makeCluster(no_cores)&#125;data &lt;- read.table(inFile,header=T,row.names=1)print(&quot;Load Observed Matrix Successfully&quot;)# 计算Jaccard IndexJaccardIndexSer &lt;- function(SourceVec, TargetMatrix)&#123; JaccardIndexOne &lt;- function(SourceVec, TargetVec)&#123; sum(SourceVec == TargetVec)/length(SourceVec) &#125; apply(TargetMatrix, 1, JaccardIndexOne, SourceVec)&#125;# 执行分治方法if(!is.na(Args[3]))&#123; n_row &lt;- nrow(data) nblock &lt;- Args[3] nrow_block &lt;- ceiling(n_row/nblock) StatOutList &lt;- vector(&quot;list&quot;, nblock) # 1. 开始进行分块计算 print(&quot;[Divide-Conquer]Start carry out Divide-Conquer for Large Observed Matrix&quot;) print(&quot;##################################################&quot;) for(i in 1:nblock)&#123; for(j in 1:nblock)&#123; nrow_start &lt;- (i-1)*nrow_block+1 nrow_end &lt;- i*nrow_block # 并行化计算 if(!is.na(Args[2]))&#123; print(paste(&quot;[Divide-Conquer]Start carry out statistic Jaccard Index parallel for block: &quot;,i,&quot;-&quot;,j,sep=&apos;&apos;)) StatOutList[[i]] &lt;- append(StatOutList[[i]], parApply(cl, data[nrow_start:nrow_end,], 1 , JaccardIndexSer, data)) print(paste(&quot;[Divide-Conquer]Finish run parallel for block: &quot;,i,&quot;-&quot;,j,sep=&apos;&apos;)) # 串行计算 &#125;else&#123; print(paste(&quot;[Divide-Conquer]Start carry out statistic Jaccard Index serially for block: &quot;,i,&quot;-&quot;,j,sep=&apos;&apos;)) StatOutList[[i]] &lt;- append(StatOutList[[i]], apply(data, 1 , JaccardIndexSer, data)) print(paste(&quot;[Divide-Conquer]Finish run serially for block: &quot;,i,&quot;-&quot;,j,sep=&apos;&apos;)) &#125; &#125; &#125; # 2. 结束分治方法的分块计算 if(!is.na(Args[2]))&#123; print(&quot;##################################################&quot;) print(&quot;[Divide-Conquer]Finish parallel running for statistic Jaccard Index!&quot;) stopCluster(cl) &#125;else&#123; print(&quot;##################################################&quot;) print(&quot;[Divide-Conquer]Finish serial running for statistic Jaccard Index!&quot;) &#125; # 3. 开始进行子块结果的合并 print(&quot;[Divide-Conquer]Start bind sub-block statout&quot;) StatOut &lt;- vector(&quot;list&quot;, nblock) # 先对列进行合并 for(i in 1:nblock)&#123; for(block in StatOutList[[i]])&#123; StatOut[[i]] &lt;- cbind(StatOut[[i]], block) &#125; &#125; # 再对行进行合并 StatOutMerge &lt;- data.frame() for(block in StatOut)&#123; StatOutMerge &lt;- rbind(StatOutMerge, block) &#125; StatOut &lt;- StatOutMerge# 不执行分治方法&#125;else&#123; # 并行化计算 if(!is.na(Args[2]))&#123; print(&quot;[Common]Start carry out statistic Jaccard Index parallel&quot;) StatOut &lt;- parApply(cl, data, 1 , JaccardIndexSer, data) print(&quot;[Common]Finish run parallel&quot;) stopCluster(cl) # 串行计算 &#125;else&#123; print(&quot;[Common]Start carry out statistic Jaccard Index serially&quot;) StatOut &lt;- apply(data, 1 , JaccardIndexSer, data) print(&quot;[Common]Finish run serially&quot;) &#125;&#125;save(StatOut,&apos;JaccardIndex.Rdata&apos;) 5. 合并两个矩阵在什么情节下，我们需要合并两个矩阵？ 对于两个批次的数据，我们分别用上面 1. 合并多个样本的定量结果成一个大矩阵 (profile matrix)得到了两个profile matrix，这两个matrix的行表示feature，列表示sample，样本一般不会有重复，而features大部分是重叠的，也有少部分是不重叠的，那么将这两个矩阵合并之后，这个新矩阵的行数为两个矩阵features的并集的大小，列数为两个矩阵列数的和 脚本名：MergeTwoMat.R 1234567891011121314151617181920212223242526Args &lt;- commandArgs(T)Matrix1 &lt;- read.table(Args[1], header=T, row.names=1)Matrix2 &lt;- read.table(Args[2], header=T, row.names=1)# 获取两个矩阵feature的并集MergeFeatureList &lt;- as.character(union(rownames(Matrix1),rownames(Matrix2)))# 获取两个矩阵的sample的并集（默认没有重叠）ColList &lt;- as.character(c(colnames(Matrix1), colnames(Matrix2)))# 初始化合并后的新矩阵，行数为feature并集大小，列数为sample并集大小MergeMatrix &lt;- matrix(rep(0,length(MergeFeatureList)*length(ColList)), nrow = length(MergeFeatureList), ncol = length(ColList))rownames(MergeMatrix) &lt;- MergeCloneListcolnames(MergeMatrix) &lt;- ColList# 对第一个矩阵中对应的列（即样本）进行更新index &lt;- match(rownames(Matrix1), MergeCloneList)for(col in colnames(Matrix1))&#123; MergeMatrix[index, col] &lt;- Matrix1[,col]&#125;# 对第二个矩阵中对应的列（即样本）进行更新index &lt;- match(rownames(Matrix2), MergeCloneList)for(col in colnames(Matrix2))&#123; MergeMatrix[index, col] &lt;- Matrix2[,col]&#125; 用法： 1$ Rscript MergeTwoMat.R &lt;matrix1&gt; &lt;matrix2&gt;]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>Bioinformatics</tag>
        <tag>Python</tag>
        <tag>R</tag>
        <tag>Perl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MeRIP/ChIP-seq分析流程]]></title>
    <url>%2F2019%2F02%2F17%2FMeRIP-seq%2F</url>
    <content type="text"><![CDATA[Schematic diagram of the MeRIP-seq protocol 由于m6A-seq数据分析的原理与过程和ChIP-seq十分相似，所以这里略过前面的质控，简单说明比对和peak calling步骤，具体内容可以参考ChIP-seq分析流程 m6A背景知识目前已知有100多种RNA修饰，涉及到mRNAs、tRNAs、rRNAs、small nuclear RNA (snRNAs) 以及 small nucleolar RNAs (snoRNAs)等。其中甲基化修饰是一种非常广泛的修饰，N6-methyl adenosine (m6A)是真核生物mRNA上最为广泛的甲基化修饰之一，并存在于多种多样的物种中。 腺嘌呤可以被编码器METTL3、METTL14和WTAP及其他组分组成的甲基转移酶复合体甲基化，甲基化的腺嘌呤可以被读码器（目前发现m6A读码器主要有四个，定位于细胞核内的YTHDC1以及定位在细胞质中的YTHDF1、YTHDF2、YTHDF3、YTHDC2）识别，同时m6A可以被擦除器FTO和ALKBH5这两个去甲基化酶催化去甲基化。 在哺乳动物mRNA中，m6A修饰存在于7000多个基因中，保守基序为RRACH (R = G, A; H = A, C, U)。m6A修饰富集在mRNA终止密码子附近。 比对参考基因组在 ChIP-seq 中一般用 BWA 或者 Bowtie 进行完全比对就可以了，但是在 MeRIP-seq 中，由于分析的 RNA ，那么就存在可变剪切，对于存在可变剪切的 mapping 用 Tophat 或者 Tophat 的升级工具 HISAT2 更合适 Tophat12345678910# build reference index## &lt;1&gt; build genome index$ bowtie2-build hg19.fa hg19## &lt;2&gt; build transcriptome index$ tophat -p 8 -G hg19.gtf --transcriptome-index=Ref/hg19/hg19_trans/know hg19# mapping$ tophat -p 8 --transcriptome-index=Ref/hg19/hg19_trans/know -o outdir hg19 reads1_1.fastq reads1_2.fastq# Only uniquely mapped reads with mapping quality score ≥20 were kept for the subsequent analysis for each sample$ samtools view -q 20 -O bam -o outdir/accepted_hits.highQual.bam outdir/accepted_hits.bam Tophat参数 -p Number of threads to use -G Supply TopHat with a set of gene model annotations and/or known transcripts, as a GTF 2.2 or GFF3 formatted file. –transcriptome-index TopHat should be first run with the -G/–GTF option together with the –transcriptome-index option pointing to a directory and a name prefix which will indicate where the transcriptome data files will be stored. Then subsequent TopHat runs using the same –transcriptome-index option value will directly use the transcriptome data created in the first run (no -G option needed after the first run). -o Sets the name of the directory in which TopHat will write all of its output samtools view 参数 -q only include reads with mapping quality &gt;= INT [0] HISAT212345678910# build reference index## using the python scripts included in the HISAT2 package, extract splice-site and exon information from the geneannotation fle$ extract_splice_sites.py chrX_data/genes/chrX.gtf &gt;chrX.ss$ extract_exons.py chrX_data/genes/chrX.gtf &gt;chrX.exon## build a HISAT2 index$ hisat2-build --ss chrX.ss --exon chrX.exon chrX_data/genome/chrX.fa chrX_tran# mapping$ hisat2 -p 10 --dta -x chrX_tran -1 reads1_1.fastq -2 reads1_2.fastq | samtools sort -@ 8 -O bam -o reads1.sort.bam 1&gt;map.log 2&gt;&amp;1 Usage: hisat2 [options]* -x &lt;ht2-idx&gt; {-1 &lt;m1&gt; -2 &lt;m2&gt; | -U &lt;r&gt;} [-S &lt;sam&gt;] -p Number of threads to use –dta reports alignments tailored for transcript assemblers -x Hisat2 index -1 The 1st input fastq file of paired-end reads -2 The 2nd input fastq file of paired-end reads -S File for SAM output (default: stdout) Peak calling MACS2参考ChIP-seq分析流程中的peak calling过程 PeakRanger12peakranger ccat --format bam SRR1042594.sorted.bam SRR1042593.sorted.bam \Xu_MUT_rep1_ccat_report --report --gene_annot_file hg19refGene.txt -q 0.05 -t 4 Peaks注释 CEAS哈佛刘小乐实验室出品的软件，可以跟MACS软件call到的peaks文件无缝连接，实现peaks的注释以及可视化分析 CEAS需要三种输入文件： Gene annotation table file (sqlite3) 可以到CEAS官网上下载：http://liulab.dfci.harvard.edu/CEAS/src/hg18.refGene.gz ，也可以自己构建：到UCSC上下载，然后用build_genomeBG脚本转换成split3格式 BED file with ChIP regions (TXT) 需要包含chromosomes, start, and end locations，这样文件可以由 peak-caller （如MACS2）得到 WIG file with ChiP enrichment signal (TXT) 如何得到wig文件可以参考samtools操作指南：以WIG文件输出测序深度 CEAS的使用方法很简单：123ceas --name=H3K36me3_ceas --pf-res=20 --gn-group-names=&apos;Top 10%,Bottom 10%&apos; \-g hg19.refGene -b ../paper_results/GSM1278641_Xu_MUT_rep1_BAF155_MUT.peaks.bed \-w ../rawData/SRR1042593.wig –name Experiment name. This will be used to name the output files. –pf-res Wig profiling resolution, DEFAULT: 50bp –gn-group-names The names of the gene groups in –gn-groups. The gene group names are separated by commas. (eg, –gn-group-names=’top 10%,bottom 10%’). -g Gene annotation table -b BED file of ChIP regions -w WIG file for either wig profiling or genome background annotation. Motif识别HOMER安装旧版本的HOMER比较复杂，因为旧版依赖于调用其他几个工具： blat Ghostscript weblogoDoes NOT work with version 3.0!!!! 新版HOMER安装很简单，主要是通过configureHomer.pl脚本来安装和管理HOMER123456789cd ~/biosoftmkdir homer &amp;&amp; cd homerwget http://homer.salk.edu/homer/configureHomer.pl# Installing the basic HOMER softwareperl configureHomer.pl -install# Download the hg19 version of the human genomeperl configureHomer.pl -install hg19 安装好后可以进行 Motif Identification1234567891011121314# 提取对应的列给HOMER作为输入文件# change # chr1 1454086 1454256 MACS_peak_1 59.88 #to # MACS_peak_1 chr1 1454086 1454256 +$ awk &apos;&#123;print $4&quot;\t&quot;$1&quot;\t&quot;$2&quot;\t&quot;$3&quot;\t+&quot;&#125;&apos; macs_peaks.bed &gt;homer_peaks.bed# MeRIP-seq 中 motif 的长度为6个 nt$ findMotifsGenome.pl homer_peaks.bed hg19 motifDir -size 200 -len 8,10,12# 自己指定background sequences，用bedtools shuffle构造随机的suffling peaks$ bedtools shuffle -i peaks.bed -g &lt;GENOME&gt; &gt;peaks_shuffle.bed# 用参数&quot;-bg&quot;指定background sequences$ findMotifsGenome.pl homer_peaks.bed hg19 motifDir -bg peaks_shuffle.bed -size 200 -len 8,10,12 Usage: findMotifsGenome.pl &lt;pos file&gt; &lt;genome&gt; &lt;output directory&gt; [additional options] 注意： &lt;genome&gt; 参数只需要写出genome的序号，不需要写出具体路径 bedtools shuffle中的genome文件的格式要求：123456&gt; For example, Human (hg19):&gt; chr1 249250621&gt; chr2 243199373&gt; ...&gt; chr18_gl000207_random 4262&gt; 可以使用 UCSC Genome Browser’s MySQL database 来获取 chromosome sizes 信息并构建genome文件12&gt; mysql --user=genome --host=genome-mysql.cse.ucsc.edu -A -e &quot;select chrom, size from hg19.chromInfo&quot; &gt;hg19.genome&gt; 最后得到的文件夹里面有一个详细的网页版报告 MEME算法原理 操作下载安装MEME 123456789cd ~/biosoftmkdir MEMEsuite &amp;&amp; cd MEMEsuite## http://meme-suite.org/doc/download.htmlwget http://meme-suite.org/meme-software/4.11.2/meme_4.11.2_1.tar.gztar zxvf meme_4.11.2_1.tar.gz cd meme_4.11.2/./configure --prefix=$HOME/my-bin/meme --with-url=&quot;http://meme-suite.org&quot;make make install 1234# 先提取peaks区域所对应的序列bedtools getfasta -fi input.fasta -bed input.bed -fo output.fasta# Motif identificationmeme output.fasta -dna -mod oops -pal Usage: meme &lt;dataset&gt; [optional arguments] &lt; dataset &gt; File containing sequences in FASTA format -dna Sequences use DNA alphabet -mod Distribution of motifs,3 options: oops | zoops | anr -pal Force palindromes (requires -dna) Differential binding analysisMerge peaks当ChIP-seq数据中有多分组，多样本以及多个重复时，需要进行样本间peaks的merge 12345678bedtools intersect -a Mcf7H3k27acUcdAlnRep1_peaks.filtered.bed -b Mcf7H3k27acUcdAlnRep2_peaks.filtered.bed -wa | cut -f1-3 | sort | uniq &gt; Mcf7Rep1_peaks.bedbedtools intersect -a Mcf7H3k27acUcdAlnRep1_peaks.filtered.bed -b Mcf7H3k27acUcdAlnRep2_peaks.filtered.bed -wb | cut -f1-3 | sort | uniq &gt; Mcf7Rep2_peaks.bedbedtools intersect -a Panc1H3k27acUcdAlnRep1_peaks.filtered.bed -b Panc1H3k27acUcdAlnRep2_peaks.filtered.bed -wa | cut -f1-3 | sort | uniq &gt; Panc1Rep1_peaks.bedbedtools intersect -a Panc1H3k27acUcdAlnRep1_peaks.filtered.bed -b Panc1H3k27acUcdAlnRep2_peaks.filtered.bed -wb | cut -f1-3 | sort | uniq &gt; Panc1Rep2_peaks.bedrm *filtered*cat *bed | sort -k1,1 -k2,2n | bedtools merge &gt; merge.bed Preparing ChIP-seq count table用bedtools 123456# Make a bed file adding peak id as the fourth colum$ awk &apos;&#123;$3=$3&quot;\t&quot;&quot;peak_&quot;NR&#125;1&apos; OFS=&quot;\t&quot; merge.bed &gt; bed_for_multicov.bed# 输入的bam文件要提前做好index，可同时提供多个bam文件$ samtools sort -@ 8 -O BAM -o input1.sort.bam input1.bam $ samtools index -@ 8 input1.sort.bam input1.sort.bam.bai # 注意：生成的索引文件的文件名必须为在原bam文件名后追加&quot;.bai&quot;，否则bedtools multicov无法识别$ bedtools multicov -bams input1.bam input2.bam ... -bed bed_for_multicov.bed &gt; counts_multicov.txt NR 表示awk开始执行程序后所读取的数据行数 OFS Out of Field Separator，输出字段分隔符 用featureCounts (subread工具包中的组件） 1234567891011# Make a saf(simplified annotation format) file for featureCount in the subread package,shown below:GeneID Chr Start End Strand497097 chr1 3204563 3207049 -497097 chr1 3411783 3411982 -497097 chr1 3660633 3661579 -...$ awk -F &quot;\t&quot; &apos;&#123;$1=&quot;peak_&quot;NR FS$1;$4=$4FS&quot;.&quot;&#125;1&apos; merge.bed &gt; subread.saf$ featureCounts -T 4 -a subread.saf -F SAF -o counts_subread.txt ../../data/*bam Usage: featureCounts [options] -a &lt;annotation_file&gt; -o &lt;output_file&gt; input_file1 [input_file2] ... -a Name of an annotation file -F Specify format of the provided annotation file. Acceptable formats include ‘GTF’ (or compatible GFF format) and ‘SAF’. ‘GTF’ by default -o Name of the output file including read counts -T Number of the threads Differential binding by DESeq2 对 contrast 构建一个 counts 矩阵，第一组的每个样本占据一列，紧接着的是第二组的样本，也是每个样本一列。 1234567891011# 做好表达矩阵count_table&lt;-read.delim(&quot;count_multicov.txt&quot;,header=F)count_matrix&lt;-as.matrix(count_table[,c(-1,-2,-3,-4)])rownames(count_matrix)&lt;-count_table$V4# 做好分组因子即可group_list&lt;-factor(c(&quot;control&quot;,&quot;control&quot;,&quot;control&quot;,&quot;treat&quot;,&quot;treat&quot;,&quot;treat&quot;))colData &lt;- data.frame(row.names=colnames(count_matrix), group_list=group_list))# 构建 DESeqDataSet 对象dds &lt;- DESeqDataSetFromMatrix(countData = count_matrix, colData = colData, design = ~ group_list) 接着计算每个样本的 library size 用于后续的标准化。library size 等于落在该样本所有peaks上的reads的总数，即 counts 矩阵中每列的加和。如果bFullLibrarySize设为TRUE，则会使用library的总reads数（根据原BAM/BED文件统计获得）。然后使用estimateDispersions估计统计分布，需要将参数fitType设为’local’ 12dds&lt;-estimateSizeFactors(dds)dds&lt;-estimateDispersions(dds,fitType=&quot;local&quot;) 对负二项分布进行显著性检验（Negative Binomial GLM fitting and Wald statistics） 123456789101112131415161718192021222324252627282930313233dds &lt;- nbinomWaldTest(dds)res &lt;- results(dds)res## log2 fold change (MLE): condition treated vs untreated ## Wald test p-value: condition treated vs untreated ## DataFrame with 9921 rows and 6 columns## baseMean log2FoldChange lfcSE stat pvalue## &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;## FBgn0000008 95.144292 0.002276428 0.2237292 0.01017493 0.99188172## FBgn0000014 1.056523 -0.495113878 2.1431096 -0.23102593 0.81729466## FBgn0000017 4352.553569 -0.239918945 0.1263378 -1.89902705 0.05756092## FBgn0000018 418.610484 -0.104673913 0.1484903 -0.70492106 0.48085936## FBgn0000024 6.406200 0.210848562 0.6895923 0.30575830 0.75978868## ... ... ... ... ... ...## FBgn0261570 3208.388610 0.29553289 0.1273514 2.32061001 0.0203079## FBgn0261572 6.197188 -0.95882276 0.7753130 -1.23669125 0.2162017## FBgn0261573 2240.979511 0.01271946 0.1133028 0.11226079 0.9106166## FBgn0261574 4857.680373 0.01539243 0.1925619 0.07993497 0.9362890## FBgn0261575 10.682520 0.16356865 0.9308661 0.17571663 0.8605166## padj## &lt;numeric&gt;## FBgn0000008 0.9972093## FBgn0000014 NA## FBgn0000017 0.2880108## FBgn0000018 0.8268644## FBgn0000024 0.9435005## ... ...## FBgn0261570 0.1442486## FBgn0261572 0.6078453## FBgn0261573 0.9826550## FBgn0261574 0.9881787## FBgn0261575 0.9679223 若存在多个分组需要进行两两比较，则需要提取指定的两个分组之间的比较结果1234## 提取你想要的差异分析结果，我们这里是treated组对untreated组进行比较res &lt;- results(dds2, contrast=c(&quot;group_list&quot;,&quot;treated&quot;,&quot;untreated&quot;))resOrdered &lt;- res[order(res$padj),]resOrdered=as.data.frame(resOrdered) 注意两个概念： Full library size: bam文件中reads的总数 Effective library size: 落在peaks区域的reads的总数 DESeq2中默认使用 Full library size bam (bFullLibrarySize =TRUE)，在ChIP-seq中使用 Effective library size 更合适，所有应该设置为bFullLibrarySize =FALSE 4. Differential binding peaks annotation在Differential binding analysis: 2. Preparing ChIP-seq count table得到的bed_for_multicov.bed中找DiffBind peaks的bed格式信息 123456# 用R进行取交集操作merge_bed&lt;-read.delim(&quot;m6A_seq/CallPeak/bed_for_multicov.bed&quot;,header=F)peaks_diffbind&lt;-read.delim(&quot;m6A_seq/CallPeak/res_diffBind.txt&quot;,header=T)peaks_diffbind_bed&lt;-merge_bed[merge_bed$V4 %in% rownames(peaks_diffbind),]# 保存文件write.table(peaks_diffbind_bed,&quot;m6A_seq/CallPeak/peaks_diffBind.bed&quot;,sep=&quot;\t&quot;,col.names=F,row.names=F,quote=F) 接着，用基因组注释文件GFF/GTF注释peaks 123456789$ bedtools intersect -wa -wb -a m6A_seq/CallPeak/peaks_diffBind.bed -b Ref/mm10/mm10_trans/Mus_musculus.GRCm38.91.gtf &gt;m6A_seq/CallPeak/peaks_diffbind.anno.bed$ head m6A_seq/CallPeak/peaks_diffbind.anno.bed## 1 7120312 7120526 peak_9 1 ensembl_havana exon 7120194 7120615 . + . gene_id &quot;ENSMUSG00000051285&quot;; gene_version &quot;17&quot;; transcript_id &quot;ENSMUST00000061280&quot;; transcript_version &quot;16&quot;; exon_number &quot;2&quot;; gene_name &quot;Pcmtd1&quot;; gene_source &quot;ensembl_havana&quot;; gene_biotype &quot;protein_coding&quot;; havana_gene &quot;OTTMUSG00000043373&quot;; havana_gene_version &quot;5&quot;; transcript_name &quot;Pcmtd1-201&quot;; transcript_source &quot;ensembl_havana&quot;; transcript_biotype &quot;protein_coding&quot;; tag &quot;CCDS&quot;; ccds_id &quot;CCDS35508&quot;; havana_transcript &quot;OTTMUST00000113805&quot;; havana_transcript_version &quot;2&quot;; exon_id &quot;ENSMUSE00000553965&quot;; exon_version &quot;2&quot;; tag &quot;basic&quot;; transcript_support_level &quot;1&quot;;## 1 7120312 7120526 peak_9 1 ensembl_havana CDS 7120309 7120615 . + 0 gene_id &quot;ENSMUSG00000051285&quot;; gene_version &quot;17&quot;; transcript_id &quot;ENSMUST00000061280&quot;; transcript_version &quot;16&quot;; exon_number &quot;2&quot;; gene_name &quot;Pcmtd1&quot;; gene_source &quot;ensembl_havana&quot;; gene_biotype &quot;protein_coding&quot;; havana_gene &quot;OTTMUSG00000043373&quot;; havana_gene_version &quot;5&quot;; transcript_name &quot;Pcmtd1-201&quot;; transcript_source &quot;ensembl_havana&quot;; transcript_biotype &quot;protein_coding&quot;; tag &quot;CCDS&quot;; ccds_id &quot;CCDS35508&quot;; havana_transcript &quot;OTTMUST00000113805&quot;; havana_transcript_version &quot;2&quot;; protein_id &quot;ENSMUSP00000059261&quot;; protein_version &quot;9&quot;; tag &quot;basic&quot;; transcript_support_level &quot;1&quot;;## 1 7120312 7120526 peak_9 1 ensembl_havana gene 7088920 7173628 . + . gene_id &quot;ENSMUSG00000051285&quot;; gene_version &quot;17&quot;; gene_name &quot;Pcmtd1&quot;; gene_source &quot;ensembl_havana&quot;; gene_biotype &quot;protein_coding&quot;; havana_gene &quot;OTTMUSG00000043373&quot;; havana_gene_version &quot;5&quot;;## 1 7120312 7120526 peak_9 1 ensembl_havana transcript 7088920 7173628 . + . gene_id &quot;ENSMUSG00000051285&quot;; gene_version &quot;17&quot;; transcript_id &quot;ENSMUST00000061280&quot;; transcript_version &quot;16&quot;; gene_name &quot;Pcmtd1&quot;; gene_source &quot;ensembl_havana&quot;; gene_biotype &quot;protein_coding&quot;; havana_gene &quot;OTTMUSG00000043373&quot;; havana_gene_version &quot;5&quot;; transcript_name &quot;Pcmtd1-201&quot;; transcript_source &quot;ensembl_havana&quot;; transcript_biotype &quot;protein_coding&quot;; tag &quot;CCDS&quot;; ccds_id &quot;CCDS35508&quot;; havana_transcript &quot;OTTMUST00000113805&quot;; havana_transcript_version &quot;2&quot;; tag &quot;basic&quot;; transcript_support_level &quot;1&quot;;## 1 7120312 7120526 peak_9 1 havana transcript 7088930 7169598 . + . gene_id &quot;ENSMUSG00000051285&quot;; gene_version &quot;17&quot;; transcript_id &quot;ENSMUST00000182114&quot;; transcript_version &quot;7&quot;; gene_name &quot;Pcmtd1&quot;; gene_source &quot;ensembl_havana&quot;; gene_biotype &quot;protein_coding&quot;; havana_gene &quot;OTTMUSG00000043373&quot;; havana_gene_version &quot;5&quot;; transcript_name &quot;Pcmtd1-202&quot;; transcript_source &quot;havana&quot;; transcript_biotype &quot;protein_coding&quot;; havana_transcript &quot;OTTMUST00000113813&quot;; havana_transcript_version &quot;2&quot;; tag &quot;cds_end_NF&quot;; tag &quot;mRNA_end_NF&quot;; transcript_support_level &quot;1&quot;; peaks注释基因的富集分析clusterProfiler: GO enrichment analysis多数人进行GO富集分析时喜欢使用DAVID，但是由于DAVID的最新版本是在2016年更新的，数据并不是最新的，所以不推荐使用DAVID。 推荐使用bioconductor包clusterProfiler 准备geneList 从DiffBind peaks annotation中获得的peaks_diffbind.anno.bed文件中提取geneID 123$ cut -f 13 m6A_seq/CallPeak/peaks_diffbind.anno.bed | \ awk &apos;BEGIN&#123;FS=&quot;;&quot;&#125; &#123;print $1&#125;&apos; | \ perl -ane &apos;chomp;$F[1]=~ s/\&quot;//g;print &quot;$F[1]\n&quot;&apos; | sort | uniq &gt;m6A_seq/CallPeak/peaks_diffbind_geneList.txt 1234567891011# 载入geneList，最终提供给enrichGO的gene list应该是一个geneID的向量gene_list &lt;- read.table(&quot;peaks_diffbind_geneList.txt&quot;,header=F)# ID转换，转换需要的信息来自于对应物种的数据包（orgDB），所以使用前需要提前安装好数据包eg = bitr(gene_list$V1, fromType=&quot;SYMBOL&quot;, toType=&quot;ENTREZID&quot;, OrgDb=&quot;org.Hs.eg.db&quot;)## ID转换支持的ID类型可以通过以下方式查看keytypes(org.Hs.eg.db)## 当ID转换涉及KEGG ID时需要使用特殊的函数bitr_kegg，只能在kegg，ncbi-geneid，ncbi-proteinid和uniprot之间进行转换eg2np &lt;- bitr_kegg(hg, fromType=&apos;kegg&apos;, toType=&apos;ncbi-proteinid&apos;, organism=&apos;hsa&apos;) GO over-representation test 123456789# 需要使用到相应物种的基因组注释数据包（org.Xx.eg.db），请提前安装好，下面以老鼠为例ego &lt;- enrichGO(gene = gene_list$V1, OrgDb = org.Mm.eg.db, keytype = &apos;ENSEMBL&apos;, ont = &quot;CC&quot;, pAdjustMethod = &quot;BH&quot;, pvalueCutoff = 0.01, qvalueCutoff = 0.05, readable = TRUE) 参数说明： gene: a vector of entrez gene id OrgDb: OrgDb keytype: keytype of input gene ont: One of “MF”, “BP”, and “CC” subontologies pvalueCutoff: Cutoff value of pvalue pAdjustMethod: one of “holm”, “hochberg”, “hommel”, “bonferroni”, “BH”, “BY”, “fdr”, “none” qvalueCutoff: qvalue cutoff readable: whether mapping gene ID to gene Name 123456789101112131415161718192021222324252627282930head(ego)## ID Description GeneRatio## GO:0005819 GO:0005819 spindle 25/198## GO:0000779 GO:0000779 condensed chromosome, centromeric region 15/198## GO:0000775 GO:0000775 chromosome, centromeric region 18/198## GO:0000776 GO:0000776 kinetochore 15/198## GO:0000793 GO:0000793 condensed chromosome 18/198## GO:0005876 GO:0005876 spindle microtubule 10/198## BgRatio pvalue p.adjust qvalue## GO:0005819 238/11745 2.090374e-13 5.518588e-11 4.950886e-11## GO:0000779 90/11745 2.241220e-11 2.958411e-09 2.654077e-09## GO:0000775 152/11745 7.936845e-11 6.984424e-09 6.265930e-09## GO:0000776 103/11745 1.649359e-10 8.919109e-09 8.001593e-09## GO:0000793 159/11745 1.689225e-10 8.919109e-09 8.001593e-09## GO:0005876 45/11745 2.821374e-09 1.241405e-07 1.113700e-07## geneID## GO:0005819 CDCA8/CDC20/KIF23/CENPE/ASPM/DLGAP5/SKA1/NUSAP1/TPX2/TACC3/NEK2/CDK1/MAD2L1/KIF18A/BIRC5/KIF11/TTK/AURKB/PRC1/KIFC1/KIF18B/KIF20A/AURKA/CCNB1/KIF4A## GO:0000779 CENPE/NDC80/HJURP/SKA1/NEK2/CENPM/CENPN/ERCC6L/MAD2L1/CDT1/BIRC5/NCAPG/AURKB/AURKA/CCNB1## GO:0000775 CDCA8/CENPE/NDC80/HJURP/SKA1/NEK2/CENPM/CENPN/ERCC6L/MAD2L1/KIF18A/CDT1/BIRC5/TTK/NCAPG/AURKB/AURKA/CCNB1## GO:0000776 CENPE/NDC80/HJURP/SKA1/NEK2/CENPM/CENPN/ERCC6L/MAD2L1/KIF18A/CDT1/BIRC5/TTK/AURKB/CCNB1## GO:0000793 CENPE/NDC80/TOP2A/NCAPH/HJURP/SKA1/NEK2/CENPM/CENPN/ERCC6L/MAD2L1/CDT1/BIRC5/NCAPG/AURKB/CHEK1/AURKA/CCNB1## GO:0005876 SKA1/NUSAP1/CDK1/KIF18A/KIF11/AURKB/PRC1/KIF18B/AURKA/KIF4A## Count## GO:0005819 25## GO:0000779 15## GO:0000775 18## GO:0000776 15## GO:0000793 18## GO:0005876 10 富集分析结果可视化 12345678# 绘制气泡图## 可以使用clusterProfiler中提供的绘图函数dotplotdotplot(ego)## 也可以使用ggplot2进行自定义绘图library(ggplot2)p&lt;-ggplot(ego)+geom_point(aes(x=) 12345# 绘制GOterm拓扑关系网，依赖topGO和Rgraphvizlibrary(topGO)library(Rgraphviz)# 画出来的拓扑图中节点的文字太小，看不清，这是一个问题plotGOgraph(ego) 参考资料： (1) Zhang C, Chen Y, Sun B, et al. m(6)A modulates haematopoietic stem and progenitor cell specification[J]. Nature, 2017, 549(7671):273. (2) BIG科研 | 细胞质内的m6A结合蛋白YTHDF3促进mRNA的翻译 (3) Pertea M, Kim D, Pertea G, et al. Transcript-level expression analysis of RNA-seq experiments with HISAT, StringTie, and Ballgown[J]. Nature Protocols, 2016, 11(9):1650. (4) ChIP-seq-pipeline (5) ChIPseq pipeline on jmzeng1314’s github (6) ChIPseq pipeline on crazyhottommy’s github (7) library size and normalization for ChIP-seq (8) Bioconductor tutorial: Analyzing RNA-seq data with DESeq2 (9) Bioconductor tutorial: clusterProfiler (10) 国科大-韩春生《生物信息学应用 - 模序搜索》ppt]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>Bioinformatics</tag>
        <tag>ChIPseq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP+MySQL实战：小鼠与人类lincRNA数据库]]></title>
    <url>%2F2019%2F02%2F17%2FInAction-PHP-MySQL-lincRNA-Database%2F</url>
    <content type="text"><![CDATA[Part1：lincRNA注释信息数据库实现目标 将lincRNA的gtf格式的注释信息写到MySQL数据库中 用户需要登录才具有数据库访问权限 提供数据库的检索功能 思路 从 UCSC genome browser 或 Ensemble 等数据库中下载人类（homo sapiens）和小鼠 （Mus musculus）的全基因组注释信息文件(GTF或者GFF3格式文件） 提取出其中的lincRNA的注释信息记录，然后将其写入MySQL数据库系统 写好PHP脚本与之前建好的MySQL数据库系统进行数据交互 准备需要写入数据库中的数据下载注释信息文件获取GTF文件的下载地址： 打开Ensemble数据库：http://asia.ensembl.org/index.html 以下列出了可供下载的基因组注释信息版本release-XX，目前的最新版本为release-91 得到下载链接，开始下载（下载至Linux服务器中） 12345# 下载GRCh38.91（人类）注释文件$ wget -c -P /Path/To/dir ftp://ftp.ensembl.org/pub/release-91/gtf/homo_sapiens/Homo_sapiens.GRCh38.91.gtf.gz# 下载GRCm38.91（小鼠）注释文件$ wget -c -P /Path/To/dir ftp://ftp.ensembl.org/pub/release-91/gtf/mus_musculus/Mus_musculus.GRCm38.91.gtf.gz 提取lincRNA部分记录首先解压前面下载的压缩文件 12$ gunzip /Path/To/dir/Homo_sapiens.GRCh38.91.gtf.gz$ gunzip /Path/To/dir/Mus_musculus.GRCm38.91.gtf.gz GTF格式说明 看一下我们的GTF文件的具体情况 1234Chrom Resource Feature Start End Score Strand Frame Attributes1 havana gene 3073253 3074322 . + . gene_id &quot;ENSMUSG00000102693&quot;; gene_version &quot;1&quot;; gene_name &quot;4933401J01Rik&quot;; gene_source &quot;havana&quot;; gene_biotype &quot;TEC&quot;; havana_gene &quot;OTTMUSG00000049935&quot;; havana_gene_version &quot;1&quot;;1 havana transcript 3073253 3074322 . + . gene_id &quot;ENSMUSG00000102693&quot;; gene_version &quot;1&quot;; transcript_id &quot;ENSMUST00000193812&quot;; transcript_version &quot;1&quot;; gene_name &quot;4933401J01Rik&quot;; gene_source &quot;havana&quot;; gene_biotype &quot;TEC&quot;; havana_gene &quot;OTTMUSG00000049935&quot;; havana_gene_version &quot;1&quot;; transcript_name &quot;4933401J01Rik-201&quot;; transcript_source &quot;havana&quot;; transcript_biotype &quot;TEC&quot;; havana_transcript &quot;OTTMUST00000127109&quot;; havana_transcript_version &quot;1&quot;; tag &quot;basic&quot;; transcript_support_level &quot;NA&quot;;1 havana exon 3073253 3074322 . + . gene_id &quot;ENSMUSG00000102693&quot;; gene_version &quot;1&quot;; transcript_id &quot;ENSMUST00000193812&quot;; transcript_version &quot;1&quot;; exon_number &quot;1&quot;; gene_name &quot;4933401J01Rik&quot;; gene_source &quot;havana&quot;; gene_biotype &quot;TEC&quot;; havana_gene &quot;OTTMUSG00000049935&quot;; havana_gene_version &quot;1&quot;; transcript_name &quot;4933401J01Rik-201&quot;; transcript_source &quot;havana&quot;; transcript_biotype &quot;TEC&quot;; havana_transcript &quot;OTTMUST00000127109&quot;; havana_transcript_version &quot;1&quot;; exon_id &quot;ENSMUSE00001343744&quot;; exon_version &quot;1&quot;; tag &quot;basic&quot;; transcript_support_level &quot;NA&quot;; 可以看到，最后一列的gene_biotype中保存的是基因类型，例如protein_coding，lincRNA，miRNA等，所以针对这一列来提取 用Perl单行命令提取lincRNA注释记录 1$ perl -ne &apos;chomp;next if (/^\#/);@gtf=split /\t/;if($gtf[8] =~ /gene_biotype\s\&quot;lincRNA\&quot;/) &#123;print &quot;$_\n&quot;;&#125;&apos; /Path/To/dir/Mus_musculus.GRCm38.91.gtf &gt;/Path/To/dir/lincRNA_GRCm38.91.gtf 想获取该数据库实战中的示例数据，请点 这里 提取出来的lincRNA注释记录为以下形式： 1234Chrom Resource Feature Start End Score Strand Frame Attributes1 havana gene 29554 31109 . + . gene_id &quot;ENSG00000243485&quot;; gene_version &quot;5&quot;; gene_name &quot;MIR1302-2HG&quot;; gene_source &quot;havana&quot;; gene_biotype &quot;lincRNA&quot;;1 havana transcript 29554 31097 . + . gene_id &quot;ENSG00000243485&quot;; gene_version &quot;5&quot;; transcript_id &quot;ENST00000473358&quot;; transcript_version &quot;1&quot;; gene_name &quot;MIR1302-2HG&quot;; gene_source &quot;havana&quot;; gene_biotype &quot;lincRNA&quot;; transcript_name &quot;MIR1302-2HG-202&quot;; transcript_source &quot;havana&quot;; transcript_biotype &quot;lincRNA&quot;; tag &quot;basic&quot;; transcript_support_level &quot;5&quot;;1 havana exon 29554 30039 . + . gene_id &quot;ENSG00000243485&quot;; gene_version &quot;5&quot;; transcript_id &quot;ENST00000473358&quot;; transcript_version &quot;1&quot;; exon_number &quot;1&quot;; gene_name &quot;MIR1302-2HG&quot;; gene_source &quot;havana&quot;; gene_biotype &quot;lincRNA&quot;; transcript_name &quot;MIR1302-2HG-202&quot;; transcript_source &quot;havana&quot;; transcript_biotype &quot;lincRNA&quot;; exon_id &quot;ENSE00001947070&quot;; exon_version &quot;1&quot;; tag &quot;basic&quot;; transcript_support_level &quot;5&quot;; 接下来，将以上格式的信息格式化成以下形式： 1234Chrom Biotype Feature Start End GeneId GeneName TranscriptId ExonNumber 1 lincRNA gene 29554 31109 ENSG00000243485 MIR1302-2HG - - 1 lincRNA transcript 29554 31097 ENSG00000243485 MIR1302-2HG ENST00000473358 - 1 lincRNA exon 29554 30039 ENSG00000243485 MIR1302-2HG ENST00000473358 1 利用Perl的正则匹配提取目标字符串进行文本格式化，想了解正则表达式，请点 这里 对于第三列Feature的不同，可以分别构造不同的正则表达式： gene： 原格式： 12&gt; gene_id &quot;ENSG00000243485&quot;; gene_version &quot;5&quot;; gene_name &quot;MIR1302-2HG&quot;; gene_source &quot;havana&quot;; gene_biotype &quot;lincRNA&quot;;&gt; 正则表达式： 12&gt; /gene_id\s\&quot;(\w+)\&quot;;.&#123;18,19&#125;\sgene_name\s\&quot;([\w-\.]+)\&quot;/&gt; transcript： 原格式： 12&gt; gene_id &quot;ENSG00000243485&quot;; gene_version &quot;5&quot;; transcript_id &quot;ENST00000473358&quot;; transcript_version &quot;1&quot;; gene_name &quot;MIR1302-2HG&quot;; gene_source &quot;havana&quot;; gene_biotype &quot;lincRNA&quot;; transcript_name &quot;MIR1302-2HG-202&quot;; transcript_source &quot;havana&quot;; transcript_biotype &quot;lincRNA&quot;; tag &quot;basic&quot;; transcript_support_level &quot;5&quot;;&gt; 正则表达式： 12&gt; /gene_id\s\&quot;(\w+)\&quot;;.&#123;18,19&#125;\stranscript_id\s\&quot;(\w+)\&quot;;.&#123;24,25&#125;\sgene_name\s\&quot;([\w-\.]+)\&quot;/&gt; exon： 原格式： 12&gt; gene_id &quot;ENSG00000243485&quot;; gene_version &quot;5&quot;; transcript_id &quot;ENST00000473358&quot;; transcript_version &quot;1&quot;; exon_number &quot;1&quot;; gene_name &quot;MIR1302-2HG&quot;; gene_source &quot;havana&quot;; gene_biotype &quot;lincRNA&quot;; transcript_name &quot;MIR1302-2HG-202&quot;; transcript_source &quot;havana&quot;; transcript_biotype &quot;lincRNA&quot;; exon_id &quot;ENSE00001947070&quot;; exon_version &quot;1&quot;; tag &quot;basic&quot;; transcript_support_level &quot;5&quot;;&gt; 正则表达式： 12&gt; /gene_id\s\&quot;(\w+)\&quot;;.&#123;18,19&#125;\stranscript_id\s\&quot;(\w+)\&quot;;.&#123;24,25&#125;\sexon_number\s\&quot;(\d+)\&quot;;\sgene_name\s\&quot;([\w-\.]+)\&quot;/&gt; 1234567891011121314151617181920# 正则匹配的正则表达式可能有点复杂，这里为了方便阅读分为多行且加上缩进，在实际命令行中请写成一行$ perl -ne &apos;chomp;next if (/^\#/);@linc=split /\t/;if($linc[2] =~ /gene/)&#123; if ($linc[8] =~ /gene_id\s\&quot;(\w+)\&quot;;.&#123;18,19&#125;\sgene_name\s\&quot;([\w-\.]+)\&quot;/)&#123; print &quot;$linc[0]\tlincRNA\t$linc[2]\t$linc[3]\t$linc[4]\t$1\t$2\t-\t-\n&quot;; &#125;&#125;elsif($linc[2] =~ /transcript/)&#123; if($linc[8] =~ /gene_id\s\&quot;(\w+)\&quot;;.&#123;18,19&#125;\stranscript_id\s\&quot;(\w+)\&quot;;.&#123;24,25&#125;\sgene_name\s\&quot;([\w-\.]+)\&quot;/) &#123; print &quot;$linc[0]\tlincRNA\t$linc[2]\t$linc[3]\t$linc[4]\t$1\t$3\t$2\t-\n&quot;; &#125;&#125;else&#123; if($linc[8] =~ /gene_id\s\&quot;(\w+)\&quot;;.&#123;18,19&#125;\stranscript_id\s\&quot;(\w+)\&quot;;.&#123;24,25&#125;\sexon_number\s\&quot;(\d+)\&quot;;\sgene_name\s\&quot;([\w-\.]+)\&quot;/) &#123; print &quot;$linc[0]\tlincRNA\t$linc[2]\t$linc[3]\t$linc[4]\t$1\t$4\t$2\t$3\n&quot;; &#125;&#125;&apos;/Path/To/dir/lincRNA_GRCm38.91.gtf &gt;/Path/To/dir/lincRNA_GRCm38.91.gtf.format 将数据写入数据库中以下以将lincRNA_GRCh38.91.gtf.format的数据导入lincRNA_h表中为例 方法一：使用MySQLiMySQLi：PHP中用于与MySQL数据库系统进行数据库交互的扩展 使用MySQLi的预处理与参数绑定方法，进行数据的批量导入 预备知识： 想了解 MySQLi 连接数据库的方法，请点 这里 想了解 MySQLi 的预处理方法，请点 这里 想了解 PHP 的文件处理方法，请点 这里 1. 首先，创建好表格 1234567891011121314151617181920# 登录MySQL$ mysql -u username -p# 选定要操作的数据库，即打算将表格创建在哪个数据库底下，例如我们要操作的数据库为lincRNAdbmysql&gt; use lincRNAdb;# 创建表格，表格名为lincRNA_hmysql&gt; CREATE TABLE lincRNA_h ( `id` int(11) NOT NULL AUTO_INCREMENT, `chrom` varchar(255) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;染色体&apos;, `biotype` varchar(255) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;基因类型&apos;, `feature` varchar(255) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;结构单元&apos;, `start` int(11) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;起始位置&apos;, `end` int(11) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;终止位置&apos;, `geneid` varchar(255) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;基因ENsembleId&apos;, `genename` varchar(255) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;基因名&apos;, `transcriptid` varchar(255) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;转录本ENsembleId&apos;, `exon` int(11) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;转录本exon序号&apos;, PRIMARY KEY (`id`) ) ; 编辑php脚本lincRNA_Ano_data_batch_import.php 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&lt;?php$host=&quot;localhost&quot;;// 请设置好正确的用户名和密码，注意必须为具有数据库写权限的用户$username=&quot;root&quot;;$password=&quot;password&quot;;$dbname=&quot;testdb&quot;;// 连接数据库$conn=new mysqli($host,$username,$password,$dbname);// 检查连接if($conn-&gt;connect_error)&#123; die(&quot;连接失败：&quot;.$conn-&gt;connect_error);&#125;else&#123; echo &quot;连接成功！\n&quot;; echo &quot;正在批量导入数据，请耐心等待...\n&quot;;&#125;// 预处理及绑定$stmt=$conn_prepare(&quot;INSERT INTO $argv[1] (chrom,biotype,feature,start,end,geneid,genename,transcriptid,exon) VALUES (?,?,?,?,?,?,?,?,?)&quot;) ;$stmt-&gt;bind_param(&quot;sssiisssi&quot;,$chrom,$biotype,$feature,$start,$end,$geneid,$genename,$transcriptid,$exon);// 打开文件$ file=fopen(&quot;$argv[2]&quot;,&apos;r&apos;) or exit(&quot;Unable to open file!&quot;);// 逐行读取文件，并写入数据库while(!feof($file))&#123; $data=explode(&quot;\t&quot;,fgets($file)); // 以tab为间隔标识将字符串打散 // 设置参数 $chrom=$data[0]; $biotype=$data[1]; $feature=$data[2]; $start=$data[3]; $end=$data[4]; $geneid=$data[5]; $genename=$data[6]; $transcriptid=$data[7]; $exon=$data[8]; // 执行 $stmt-&gt;execute();&#125;echo &quot;新记录插入成功&quot;;fcolse($file);$stmt-&gt;close();$conn-&gt;close();?&gt; 执行php脚本，完成数据的批量导入 1$ php -f lincRNA_Ano_data_batch_import.php &lt;tablename&gt; &lt;data-to-import&gt; 想了解如何在 Linux 命令行中使用和执行 PHP 代码，请点 这里 方法二：使用sql脚本准备sql脚本要往MySQL数据库中写入数据，需要用INSERT INTO 语句 12INSERT INTO table_name (column1,column2,column3,...)VALUES (value1,value2,value3,...); 一般情况下我们往数据库中写入少数几条数据时，直接在MySQL的交互环境中执行INSERT INTO 语句即可，但是当要批量导入时，再采用在MySQL的交互环境中执行INSERT INTO 语句，明显是低效而不现实的，这个时候我们可以写一个sql脚本，来执行数据库的批量操作。 简单来说sql脚本就是把多个INSERT INTO 语句写到一个文件里 sql脚本的文件起始部分（为人类和小鼠的数据分别准备一个sql文件，创建的对应表名分别为 lincRNA_h 和 lincRNA_m ，以下以人类的为例）： 1234567891011121314151617181920212223242526SET NAMES utf8;SET FOREIGN_KEY_CHECKS = 0;-- ------------------------------ Table structure for `lincRNA_h`-- ----------------------------DROP TABLE IF EXISTS `lincRNA_h`;CREATE TABLE lincRNA_h ( `id` int(11) NOT NULL AUTO_INCREMENT, `chrom` varchar(255) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;染色体&apos;, `biotype` varchar(255) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;基因类型&apos;, `feature` varchar(255) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;结构单元&apos;, `start` int(11) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;起始位置&apos;, `end` int(11) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;终止位置&apos;, `geneid` varchar(255) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;基因ENsembleId&apos;, `genename` varchar(255) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;基因名&apos;, `transcriptid` varchar(255) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;转录本ENsembleId&apos;, `exon` int(11) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;转录本exon序号&apos;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8;-- ------------------------------ Records of `lincRNA_h`-- ---------------------------- 然后，用perl单行命令实现以下转换： 11 lincRNA gene 29554 31109 ENSG00000243485 MIR1302-2HG - - 1INSERT INTO lincRNA_h （chrom,biotype,feature,start,end,geneid,genename,transcriptid,exon) VALUES (&quot;1&quot;,&quot;lincRNA&quot;,&quot;gene&quot;,&quot;29554&quot;,&quot;31109&quot;,&quot;ENSG00000243485&quot;,&quot;MIR1302-2HG&quot;,&quot;-&quot;,&quot;-&quot;); 并将转换后的结果追加到上一步已经写好起始部分的sql脚本的后面 1$ perl -ane &apos;chomp;print &quot;INSERT INTO lincRNA_h (chrom,biotype,feature,start,end,geneid,genename,transcriptid,exon) VALUES (\&quot;$F[0]\&quot;,\&quot;$F[1]\&quot;,\&quot;$F[2]\&quot;,\&quot;$F[3]\&quot;,\&quot;$F[4]\&quot;,\&quot;$F[5]\&quot;,\&quot;$F[6]\&quot;,\&quot;$F[7]\&quot;,\&quot;$F[8]\&quot;);\n&quot;&apos; /Path/To/lincRNA_GRCh38.91.gtf.format &gt;&gt;/Path/To/lincRNA_h.sql 写入数据登录，进入MySQL的交互环境，才能进行以下操作 123456789101112131415161718# 输入密码，登录MySQL$ mysql -u username -p# 查看已有的数据库mysql&gt; show databases;# 选择要进行操作的数据库，比如我们要操作的数据库为lincRNAdb## 注意：请确认你具有该数据库的写入权限，否则无法写入mysql&gt; use lincRNAdb;# 执行sql脚本，向数据库中批量导入数据mysql&gt; source /Path/To/lincRNA_h.sql;# 查看当前使用的数据库中已有表格，确保数据已成功导入：是否有lincRNA_h表格？mysql&gt; show tables;# 查看表格中的数据是否正确，为了避免将表中的数据全部打印出来，请使用where子句，只打印出前100条记录mysql&gt; select * from lincRNA_h where id&lt;100; 编辑php脚本注册创建注册页面 表单部分的代码（想了解表单的相关知识，请点 这里）： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&lt;form action=&quot;registeraction.php&quot; method=&quot;post&quot;&gt;&lt;table border=&quot;0&quot;&gt;&lt;tr&gt; &lt;td&gt;用户名：&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;text&quot; id=&quot;id_name&quot; name=&quot;username&quot; required=&quot;required&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td&gt;密&amp;nbsp;&amp;nbsp;&amp;nbsp;码：&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;password&quot; id=&quot;password&quot; name=&quot;password&quot; required=&quot;required&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td&gt;重复密码：&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;password&quot; id=&quot;re_password&quot; name=&quot;re_password&quot; required=&quot;required&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td&gt;性别：&lt;/td&gt; &lt;td&gt; &lt;input type=&quot;radio&quot; id=&quot;sex&quot; name=&quot;sex&quot; value=&quot;男&quot;&gt;男 &lt;input type=&quot;radio&quot; id=&quot;sex&quot; name=&quot;sex&quot; value=&quot;女&quot;&gt;女 &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td&gt;QQ：&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;text&quot; id=&quot;qq&quot; name=&quot;qq&quot; required=&quot;required&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td&gt;Email：&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;email&quot; id=&quot;email&quot; name=&quot;email&quot; required=&quot;required&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td&gt;电话：&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;text&quot; id=&quot;phone&quot; name=&quot;phone&quot; required=&quot;required&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td&gt;地址：&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;text&quot; id=&quot;address&quot; name=&quot;address&quot; required=&quot;required&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td colspan=&quot;2&quot; align=&quot;center&quot; style=&quot;color:red;font-size:10px;&quot;&gt;&lt;!--提示信息--&gt;&lt;?php$err=isset($_GET[&quot;err&quot;])?$_GET[&quot;err&quot;]:&quot;&quot;;switch($err) &#123; case 1: echo &quot;用户名已存在！&quot;; break; case 2: echo &quot;密码与重复密码不一致！&quot;; break; case 3: echo &quot;注册成功！&quot;; break;&#125;?&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt; &lt;input type=&quot;submit&quot; id=&quot;register&quot; name=&quot;register&quot; value=&quot;注册&quot;&gt; &lt;input type=&quot;reset&quot; id=&quot;reset&quot; name=&quot;reset&quot; value=&quot;重置&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;如果已有账号，快去&lt;a href=&quot;login.php&quot;&gt;登录&lt;/a&gt;吧！&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/form&gt; 表单信息提交的对象为同一目录下的registeraction.php脚本 注册信息处理脚本registeraction.php脚本 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;?php// 声明变量$username = isset($_POST[&apos;username&apos;])?$_POST[&apos;username&apos;]:&quot;&quot;;$password = isset($_POST[&apos;password&apos;])?$_POST[&apos;password&apos;]:&quot;&quot;;$re_password = isset($_POST[&apos;re_password&apos;])?$_POST[&apos;re_password&apos;]:&quot;&quot;;$sex = isset($_POST[&apos;sex&apos;])?$_POST[&apos;sex&apos;]:&quot;&quot;;$qq = isset($_POST[&apos;qq&apos;])?$_POST[&apos;qq&apos;]:&quot;&quot;;$email = isset($_POST[&apos;email&apos;])?$_POST[&apos;email&apos;]:&quot;&quot;;$phone = isset($_POST[&apos;phone&apos;])?$_POST[&apos;phone&apos;]:&quot;&quot;;$address = isset($_POST[&apos;address&apos;])?$_POST[&apos;address&apos;]:&quot;&quot;;if($password == $re_password) &#123; // 建立连接，需要以root的身份登录MySQL，因为只有root用户才具有数据库的写权限 // 使用面向对象的MySQLi方法 $conn = new mysqli(&apos;localhost&apos;,&apos;root&apos;,&apos;&apos;,&apos;php&apos;); // 第三项需填入root用户的密码，若设置为免密登录，则可以为空 // 准备SQL语句,查询用户名 $sql_select=&quot;SELECT username FROM User WHERE username = &apos;$username&apos;&quot;; // 执行SQL语句 $result = $conn-&gt;query($sql_select); $row = $result-&gt;fetch_array(); // 判断用户名是否已存在 if($username == $row[&apos;username&apos;]) &#123; //用户名已存在，显示提示信息 header(&quot;Location:register.php?err=1&quot;); &#125; else &#123; //用户名不存在，插入数据 //准备SQL语句 $sql_insert = &quot;INSERT INTO User(username,password,sex,qq,email,phone,address) VALUES(&apos;$username&apos;,&apos;$password&apos;,&apos;$sex&apos;,&apos;$qq&apos;,&apos;$email&apos;,&apos;$phone&apos;,&apos;$address&apos;)&quot;; //执行SQL语句 $conn-&gt;query($sql_insert); header(&quot;Location:register.php?err=3&quot;); &#125; //关闭数据库 $conn-&gt;close($conn);&#125; else &#123; header(&quot;Location:register.php?err=2&quot;);&#125;?&gt; 登录登录页面 12345678910111213141516171819202122232425262728293031323334353637383940&lt;form id=&quot;loginform&quot; action=&quot;loginaction.php&quot; method=&quot;post&quot;&gt;&lt;table border=&quot;0&quot;&gt;&lt;tr&gt; &lt;td&gt;用户名：&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;text&quot; id=&quot;name&quot; name=&quot;username&quot; required=&quot;required&quot; value=&quot;&lt;?php echo isset($_COOKIE[&quot;wang&quot;])?$_COOKIE[&quot;wang&quot;]:&quot;&quot;;?&gt;&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td&gt;密&amp;nbsp;&amp;nbsp;&amp;nbsp;码：&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;password&quot; id=&quot;password&quot; name=&quot;password&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td colspan=&quot;2&quot;&gt;&lt;input type=&quot;checkbox&quot; name=&quot;remember&quot; value=&quot;on&quot;&gt;&lt;small&gt;记住我&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td colspan=&quot;2&quot; align=&quot;center&quot; style=&quot;color:red;font-size:10px;&quot;&gt;&lt;!--提示信息--&gt;&lt;?php$err=isset($_GET[&quot;err&quot;])?$_GET[&quot;err&quot;]:&quot;&quot;;switch($err) &#123;case 1: echo &quot;用户名或密码错误！&quot;; break;case 2: echo &quot;用户名或密码不能为空！&quot;; break;&#125;?&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt; &lt;input type=&quot;submit&quot; id=&quot;login&quot; name=&quot;login&quot; value=&quot;登录&quot;&gt; &lt;input type=&quot;reset&quot; id=&quot;reset&quot; name=&quot;reset&quot; value=&quot;重置&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;还没有账号，快去&lt;a href=&quot;register.php&quot;&gt;注册&lt;/a&gt;吧！&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/form&gt; 登录脚本方法一： 该方法为，用root用户权限从User表中提取相应的用户名和密码信息，与用户填写的用户名和密码进行比较，来判断用户用户是否拥有数据库的访问权限 该方法需要拥有root用户的账户密码，若没有，请使用第二种方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;?php// 声明变量$username = isset($_POST[&apos;username&apos;])?$_POST[&apos;username&apos;]:&quot;&quot;;$password = isset($_POST[&apos;password&apos;])?$_POST[&apos;password&apos;]:&quot;&quot;;$remember = isset($_POST[&apos;remember&apos;])?$_POST[&apos;remember&apos;]:&quot;&quot;;// 判断用户名和密码是否为空if(!empty($username)&amp;&amp;!empty($password)) &#123; // 建立连接 // 使用面向对象的方法 $conn = new mysqli(&apos;localhost&apos;,&apos;root&apos;,&apos;&apos;,&apos;php&apos;); // 准备SQL语句 $sql_select = &quot;SELECT user,password FROM User WHERE user = &apos;$username&apos; AND password = &apos;$password&apos;&quot;; //执行SQL语句 $result = $conn-&gt;query($sql_select); $row = $result-&gt;fetch_assoc(); //判断用户名或密码是否正确 if($username==$row[&apos;username&apos;]&amp;&amp;$password==$row[&apos;password&apos;]) &#123; //选中“记住我” if($remember==&quot;on&quot;) &#123; //创建cookie setcookie(&quot;wang&quot;, $username, time()+7*24*3600); &#125; //开启session session_start(); //创建session $_SESSION[&apos;user&apos;]=$username; $_SESSION[&apos;password&apos;]=$password; //写入日志 $ip = $_SERVER[&apos;REMOTE_ADDR&apos;]; $date = date(&apos;Y-m-d H:m:s&apos;); $info = sprintf(&quot;当前访问用户：%s,IP地址：%s,时间：%s \n&quot;,$username, $ip, $date); $sql_logs = &quot;INSERT INTO Logs(username,ip,date) VALUES(&apos;$username&apos;,&apos;$ip&apos;,&apos;$date&apos;)&quot;; //日志写入文件，如实现此功能，需要创建文件目录logs $f = fopen(&apos;./logs/&apos;.date(&apos;Ymd&apos;).&apos;.log&apos;,&apos;a+&apos;); fwrite($f,$info); fclose($f); //跳转到loginsucc.php页面 header(&quot;Location:loginsucc.php&quot;); //关闭数据库 $conn-&gt;close(); &#125;else &#123; //用户名或密码错误，赋值err为1 header(&quot;Location:login.php?err=1&quot;); &#125;&#125;else &#123; //用户名或密码为空，赋值err为2 header(&quot;Location:login.php?err=2&quot;);&#125;?&gt; 方法二： 该方法通过使用用户填写的用户名和密码直接尝试登陆MySQL，通过登录结果来判断用户是否拥有数据库的使用权限 12345678910111213141516171819202122232425262728293031323334353637&lt;?php// 声明变量$username = isset($_POST[&apos;username&apos;])?$_POST[&apos;username&apos;]:&quot;&quot;;$password = isset($_POST[&apos;password&apos;])?$_POST[&apos;password&apos;]:&quot;&quot;;$remember = isset($_POST[&apos;remember&apos;])?$_POST[&apos;remember&apos;]:&quot;&quot;;// 判断用户名和密码是否为空if(!empty($username)&amp;&amp;!empty($password)) &#123; // 建立连接 // 使用面向对象的方法，使用用户填写的用户名和密码直接尝试登陆MySQL $conn = new mysqli(&apos;localhost&apos;,$username,$password,&apos;testdb&apos;); if ($conn-&gt;connect_error) &#123; // 连接失败，用户名或密码错误，赋值err为1 header(&quot;Location:login.php?err=1&quot;); &#125;else&#123; //选中“记住我” if($remember==&quot;on&quot;) &#123; //创建cookie setcookie(&quot;wang&quot;, $username, time()+7*24*3600); &#125; //开启session session_start(); //创建session $_SESSION[&apos;user&apos;]=$username; $_SESSION[&apos;password&apos;]=$password; //跳转到loginsucc.php页面 header(&quot;Location:loginsucc.php&quot;); //关闭数据库 $conn-&gt;close(); &#125;&#125;else &#123; //用户名或密码为空，赋值err为2 header(&quot;Location:login.php?err=2&quot;);&#125;?&gt; 登录成功 通过判断全局变量$_SESSION[&#39;user&#39;]是否定义来判断是否成功登录 1234567891011121314151617181920212223242526&lt;?php//开启sessionsession_start();//声明变量$username= isset($_SESSION[&apos;user&apos;])?$_SESSION[&apos;user&apos;]:&quot;&quot;;//判断session是否为空if(!empty($username))&#123;?&gt; &lt;h1&gt;登录成功！&lt;/h1&gt; 欢迎您！&lt;?php echo $username;?&gt; // 继续检索数据库 &lt;br/&gt; &lt;a href=&quot;databaseQuery_Ano.php&quot;&gt;检索数据库&lt;/a&gt; &lt;br/&gt; &lt;a href=&quot;logout.php&quot;&gt;退出&lt;/a&gt;&lt;?php&#125;else &#123; //未登录，无权访问 ?&gt; &lt;h1&gt;你无权访问！！！&lt;/h1&gt;&lt;?php&#125;?&gt; 退出登录123456789&lt;?php//开启sessionsession_start();//撤销sessionsession_unset();session_destroy();//跳转到login.phpheader(&quot;Location:login.php&quot;);?&gt; 检索数据库检索数据库，需要用到数据库用户的用户名和密码，用于在之前的登录过程中，我们已经将用户名和密码存储在 $_SESSION 中，所以可以通过 $_SESSION[&#39;user&#39;] 和 $_SESSION[&#39;password&#39;] 获得 这里将表单部分与PHP命令写在一个PHP脚本中，脚本命名为databaseQuery_Ano.php 表单部分 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980&lt;form action=&quot;#&quot; method=&quot;post&quot;&gt;&lt;table&gt;&lt;tr&gt; &lt;td align=&quot;left&quot;&gt;物&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp种：&lt;/td&gt; &lt;td colspan=&quot;2&quot; align=&quot;left&quot;&gt; &lt;select name=&quot;specie&quot;&gt; &lt;option value=&quot;&quot;&gt;选择一个物种:&lt;/option&gt; &lt;option value=&quot;human&quot;&gt;Homo Sapiens&lt;/option&gt; &lt;option value=&quot;mouse&quot;&gt;Mus musculus&lt;/option&gt; &lt;/select&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td align=&quot;left&quot;&gt;基因名/基因ID:&lt;/td&gt; &lt;td colspan=&quot;2&quot; align=&quot;left&quot;&gt;&lt;input type=&quot;text&quot; name=&quot;geneName&quot; value=&quot;MIR1302-2HG&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td align=&quot;left&quot;&gt;基&amp;nbsp;&amp;nbsp;&amp;nbsp因&amp;nbsp;&amp;nbsp;&amp;nbsp位&amp;nbsp;&amp;nbsp;&amp;nbsp置：&lt;/td&gt; &lt;td colspan=&quot;2&quot; align=&quot;left&quot;&gt; &lt;select name=&quot;chrom&quot;&gt; &lt;option value=&quot;&quot;&gt;选择一条染色体：&lt;/option&gt; &lt;option value=&quot;1&quot;&gt;chr1&lt;/option&gt; &lt;option value=&quot;2&quot;&gt;chr2&lt;/option&gt; &lt;option value=&quot;3&quot;&gt;chr3&lt;/option&gt; &lt;option value=&quot;4&quot;&gt;chr4&lt;/option&gt; &lt;option value=&quot;5&quot;&gt;chr5&lt;/option&gt; &lt;option value=&quot;6&quot;&gt;chr6&lt;/option&gt; &lt;option value=&quot;7&quot;&gt;chr7&lt;/option&gt; &lt;option value=&quot;8&quot;&gt;chr8&lt;/option&gt; &lt;option value=&quot;9&quot;&gt;chr9&lt;/option&gt; &lt;option value=&quot;10&quot;&gt;chr10&lt;/option&gt; &lt;option value=&quot;11&quot;&gt;chr11&lt;/option&gt; &lt;option value=&quot;12&quot;&gt;chr12&lt;/option&gt; &lt;option value=&quot;13&quot;&gt;chr13&lt;/option&gt; &lt;option value=&quot;14&quot;&gt;chr14&lt;/option&gt; &lt;option value=&quot;15&quot;&gt;chr15&lt;/option&gt; &lt;option value=&quot;16&quot;&gt;chr16&lt;/option&gt; &lt;option value=&quot;17&quot;&gt;chr17&lt;/option&gt; &lt;option value=&quot;18&quot;&gt;chr18&lt;/option&gt; &lt;option value=&quot;19&quot;&gt;chr19&lt;/option&gt; &lt;option value=&quot;20&quot;&gt;chr20&lt;/option&gt; &lt;option value=&quot;21&quot;&gt;chr21&lt;/option&gt; &lt;option value=&quot;22&quot;&gt;chr22&lt;/option&gt; &lt;option value=&quot;X&quot;&gt;chrX&lt;/option&gt; &lt;option value=&quot;Y&quot;&gt;chrY&lt;/option&gt; &lt;/select&gt; &lt;/td&gt; &lt;td&gt; 区间：&lt;input type=&quot;text&quot; name=&quot;start&quot;&gt; - &lt;input type=&quot;text&quot; name=&quot;end&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td colspan=&quot;2&quot; align=&quot;center&quot; style=&quot;color:red;font-size:10px;&quot;&gt;&lt;!--提示信息--&gt;&lt;?php$err=isset($_GET[&quot;err&quot;])?$_GET[&quot;err&quot;]:&quot;&quot;;switch($err) &#123;case 1: echo &quot;表单填写错误:物种必须选，基因名和基因位置两项至少选一项填写！&quot;; break;case 2: echo &quot;表单填写错误：区间起始和终止位置需为整数，且起始位置小于终止位置！&quot;; break;case 3: echo &quot;未登陆，无权访问！&lt;br&gt;&quot;; echo &quot;快去&lt;a href=&apos;login.php&apos;&gt;登录&lt;/a&gt;吧&quot;; break;case 4: echo &quot;Update或Delete操作异常！未在数据库中找到对应记录！&quot;; break;&#125;?&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td align=&quot;center&quot;&gt;&lt;input type=&quot;submit&quot; name=&quot;submit&quot; value=&quot;Submit&quot;&gt;&lt;/td&gt; &lt;td align=&quot;center&quot;&gt;&lt;input type=&quot;reset&quot; value=&quot;Reset&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/form&gt; PHP脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081&lt;?php//开启sessionsession_start();// 获取用户名和密码$username=isset($_SESSION[&apos;user&apos;])?$_SESSION[&apos;user&apos;]:&quot;&quot;;$password=isset($_SESSION[&apos;password&apos;])?$_SESSION[&apos;password&apos;]:&quot;&quot;;// 获取表单提交数据$specie=isset($_POST[&apos;specie&apos;])?$_POST[&apos;specie&apos;]:&quot;&quot;;$geneName=isset($_POST[&apos;geneName&apos;])?$_POST[&apos;geneName&apos;]:&quot;&quot;;$chrom=isset($_POST[&apos;chrom&apos;])?$_POST[&apos;chrom&apos;]:&quot;&quot;;$start=isset($_POST[&apos;start&apos;])?$_POST[&apos;start&apos;]:&quot;&quot;;$end=isset($_POST[&apos;end&apos;])?$_POST[&apos;end&apos;]:&quot;&quot;;$submit=isset($_POST[&apos;submit&apos;])?$_POST[&apos;submit&apos;]:&quot;&quot;;if($submit)&#123; // 判断提交的检索信息是否满足要求：物种必须选，基因名和基因位置两项至少选一项填写 if(empty($specie)||(empty($geneName)&amp;&amp;empty($chrom)))&#123; header(&quot;Location:databaseQuery.php?err=1&quot;); // 判断用户名和密码是否已经设置 &#125;elseif(!empty($username)&amp;&amp;!empty($password))&#123; // 连接数据库 $conn=new mysqli(&apos;localhost&apos;,$username,$password,&apos;testdb&apos;); if($conn-&gt;connect_error)&#123; header(&quot;Location:databaseQuery_Ano.php?err=3&quot;); &#125; // 设置要检索的数据库表格名 $table=&quot;&quot;; if($specie==&quot;human&quot;)&#123; $table=&quot;lincRNA_h&quot;; &#125;else&#123; $table=&quot;lincRNA_m&quot;; &#125; // 创建SQL查询语句 $sql=&quot;SELECT * from $table where&quot;; // 根据实际表单填写情况追加查询条件 $bool_and=false; if(!empty($geneName))&#123; $sql .= &quot; genename=&apos;$geneName&apos; or geneid=&apos;$geneName&apos;&quot;; $bool_and=true; &#125; if(!empty($chrom))&#123; if($bool_and)&#123; $sql .= &quot; and&quot;; &#125; $sql .= &quot; chrom=&apos;$chrom&apos;&quot;; if(!empty($start)&amp;&amp;!empty($end))&#123; // 判断提交的起始点与终止点是否正确，即是否为整数，且起始点小于终止点 if(is_int(intval($start))&amp;&amp;is_int(intval($end))&amp;&amp;intval($start)&lt;intval($end))&#123; $sql .= &quot; and start&gt;=$start and end&lt;=$end&quot;; &#125;else&#123; header(&quot;Location:databaseQuery_Ano.php?err=2&quot;); &#125; &#125; &#125; $sql .= &quot;;&quot;; // 执行查询语句 $result=$conn-&gt;query($sql); if($result-&gt;num_rows &gt; 0)&#123; // 输出查询结果 echo &quot;&lt;table &gt;&lt;tr&gt;&lt;th&gt;chrom&lt;/th&gt;&lt;th&gt;Biotype&lt;/th&gt;&lt;th&gt;Feature&lt;/th&gt;&lt;th&gt;Start&lt;/th&gt;&lt;th&gt;End&lt;/th&gt;&lt;th&gt;GeneId&lt;/th&gt;&lt;th&gt;GeneName&lt;/th&gt;&lt;th&gt;TranscriptId&lt;/th&gt;&lt;th&gt;ExonNumber&lt;/th&gt;&lt;th&gt;操作&lt;/th&gt;&lt;/tr&gt;&quot;; while($row = $result-&gt;fetch_array())&#123; echo &quot;&lt;tr&gt;&lt;td&gt;&quot;.$row[&apos;chrom&apos;].&quot;&lt;/td&gt;&lt;td&gt;&quot;.$row[&apos;biotype&apos;].&quot;&lt;/td&gt;&lt;td&gt;&quot;.$row[&apos;feature&apos;].&quot;&lt;/td&gt;&lt;td&gt;&quot;.$row[&apos;start&apos;].&quot;&lt;/td&gt;&lt;td&gt;&quot;.$row[&apos;end&apos;].&quot;&lt;/td&gt;&lt;td&gt;&quot;.$row[&apos;geneid&apos;].&quot;&lt;/td&gt;&lt;td&gt;&quot;.$row[&apos;genename&apos;].&quot;&lt;/td&gt;&lt;td&gt;&quot;.$row[&apos;transcriptid&apos;].&quot;&lt;/td&gt;&lt;td&gt;&quot;.$row[&apos;exon&apos;].&quot;&lt;/td&gt;&lt;td&gt;&lt;a style=\&quot;padding:2px;\&quot; href=\&quot;update_form.php?id=&quot;.$row[&apos;id&apos;].&quot;&amp;dbname=&quot;.$table.&quot;\&quot;&gt;Update&lt;/a&gt;&lt;/tr&gt;&quot;; &#125; &#125;else&#123; echo &quot;未检索到满足条件的记录!&quot;; &#125; &#125;else&#123; // 跳转到当前页面，并为err赋值1 header(&quot;Location:databaseQuery_Ano.php?err=3&quot;); &#125;&#125;?&gt; Part2：内容与功能升级实现目标 增加 blast 功能：将用户提交的序列与 lncRNA 的序列库进行 blast 比对 创建 lncRNA 表达数据库，除了 part1 中提供的基础检索功能外，还要提供绘图功能 增加数据库的删除、追加（单条记录追加和批量追加）与编辑功能 思路1. 增加 blast 功能 目标序列的指定方式： 用户在输入框中输入 用户提交目标序列的 fasta 文件 创建lncRNA的 blast 序列库（即lncRNA的 blast 索引)： 根据 part1 《准备需要写入数据库中的数据》 中得到的文件lincRNA_GRCm38.91.gtf.format和lincRNA_GRCh38.91.gtf.format，从中提取unique的lincRNA的Ensembl gene id 利用lincRNA的Ensembl gene id，从Ensembl的BioMart中下载lincRNA对应的序列 对这些序列建立blast索引 2. 创建 lncRNA 表达数据库 从公共数据库（如GEO）中直接下载RNA-seq表达谱文件，或者下载原始fastq文件，自己跑RNA-seq分析流程来获取表达谱，然后提取其中的lincRNA部分，进行差异表达分析后将差异表达结果写进MySQL数据库中，同时保留表达谱文件，以供后期绘图时访问 表达数据来源：从GEO数据库中下载某个RNA-seq实验的表达谱文件 差异表达分析：使用DESeq2进行差异表达分析，DESeq2使用方法请点 这里 将差异表达结果写进MySQL数据库中：过程与 part1 《将数据写入数据库中——方法一：使用MySQLi》一样 绘图：即在检索获得差异表达结果列表后，每条记录后提供一个绘图按钮，用户点击后即可绘制出该基因在不同samples的表达柱状图，可通过调用R脚本实现 3. 增加数据库的删除、追加与编辑功能 这部分应该是整个part2中最容易实现的部分 删除 —— 注意：在真正删除之前，请要求用户进行再次确认 删除一条记录：用SQL的DELETE语句 删除多条记录：在每条记录前有一个复选框，选中多条记录前的复选框，然后点击页面某个角落的“Delete”按钮，提交删除操作请求 追加 追加一条记录 批量追加：可以让用户提交文件来实现批量追加，不过真正进行追加操作之前，务必进行文件格式的检查 编辑 —— 用UPDATE语句 添加blast功能获得chrX上lincRNA序列 利用Gene stable ID批量下载序列 第一次写作业时，在mysql数据库中添加了lncRNA的位置信息、ID等。这次从之前建立数据得到的lincRNA_GRCh38.91.gtf.format和lincRNA_GRCm38.91.gtf.format文件中提取Gene stable ID，批量下载FASTA格式的序列信息。 得到lnRNA的ID 先以lincRNA_GRCh38.91.gtf.format（人类）为例 1. 通过less lincRNA_GRCh38.91.gtf.format查看文件 实例如下，第6列的就是基因ID（ENSG00000243485开始） 123456789101112131415161718192021221 lincRNA gene 29554 31109 ENSG00000243485 MIR1302-2HG - -1 lincRNA transcript 29554 31097 ENSG00000243485 MIR1302-2HG ENST00000473358 -1 lincRNA exon 29554 30039 ENSG00000243485 MIR1302-2HG ENST00000473358 11 lincRNA exon 30564 30667 ENSG00000243485 MIR1302-2HG ENST00000473358 21 lincRNA exon 30976 31097 ENSG00000243485 MIR1302-2HG ENST00000473358 31 lincRNA transcript 30267 31109 ENSG00000243485 MIR1302-2HG ENST00000469289 -1 lincRNA exon 30267 30667 ENSG00000243485 MIR1302-2HG ENST00000469289 11 lincRNA exon 30976 31109 ENSG00000243485 MIR1302-2HG ENST00000469289 21 lincRNA gene 34554 36081 ENSG00000237613 FAM138A - -1 lincRNA transcript 34554 36081 ENSG00000237613 FAM138A ENST00000417324 -1 lincRNA exon 35721 36081 ENSG00000237613 FAM138A ENST00000417324 11 lincRNA exon 35277 35481 ENSG00000237613 FAM138A ENST00000417324 21 lincRNA exon 34554 35174 ENSG00000237613 FAM138A ENST00000417324 31 lincRNA transcript 35245 36073 ENSG00000237613 FAM138A ENST00000461467 -1 lincRNA exon 35721 36073 ENSG00000237613 FAM138A ENST00000461467 11 lincRNA exon 35245 35481 ENSG00000237613 FAM138A ENST00000461467 21 lincRNA gene 89295 133723 ENSG00000238009 AL627309.1 - -1 lincRNA transcript 89295 120932 ENSG00000238009 AL627309.1 ENST00000466430 -1 lincRNA exon 120775 120932 ENSG00000238009 AL627309.1 ENST00000466430 11 lincRNA exon 112700 112804 ENSG00000238009 AL627309.1 ENST00000466430 21 lincRNA exon 92091 92240 ENSG00000238009 AL627309.1 ENST00000466430 31 lincRNA exon 89295 91629 ENSG00000238009 AL627309.1 ENST00000466430 4 2. 用cut -f 6 lincRNA_GRCh38.91.gtf.format|sort|uniq &gt; lincRNA_GRCh38.91.geneid.txt命令截取第6列ID信息，并输出到out文件中 3. 打开ensembl的biomart，按照图片调整参数，最后上传ID信息文件即可。 （由于全基因组数据太大，所以我在REGION中只选择了x染色体） 4. 点击results,跳转页面点GO生成FASTA序列。 5. 小鼠数据如法炮制，只是在第一张照片处把Human参数改成Mouse。 构建BLAST索引 删除课堂教学用的数据 每人配额只有2G，所以清空一下磁盘，方便做作业 123456#查看文件大小du -h#删除文件夹rm -rf filename-r ：循环删掉文件夹中的文件-f ：删除文件时不提示 构建BLAST索引 在biomart下载完FASTA格式序列后，会得到mart_export.txt文件，修改文件名为lincRNA_chrX.fa将其导入服务器。 调用BLAST命令构建索引 1makeblastdb -in /Path/To/lincRNA_chrX.fa -input_type fasta -dbtype nucl -title lncRNA_h -parse_seqids -out /Path/To/lncRNA_h -logfile File_h 参数介绍 -in:FASTA文件位置 -input_type:输入格式 -dbtype:数据库格式 -title:索引名称 -parse_seqids:不知道啥意思 -out:输出文件名（title感觉没啥用，最后blast调用索引都是调用输出文件） -logfile:软件日志 命令完成，打开软件日志File_h查看： 1234567Building a new DB, current time: 05/26/2018 15:17:13New DB name: /home/201728016715029/blastdb/lncRNA_hNew DB title: lncRNA_hSequence type: NucleotideKeep MBits: TMaximum file size: 1000000000BAdding sequences from FASTA; added 246 sequences in 0.00809789 seconds. 成功！ 执行BLAST 提交BLAST检索申请，脚本保存为blastQuery.php 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;form action=&quot;localblast.php&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt;&lt;p&gt;输入你的序列&lt;/p&gt;&lt;textarea style=&quot;width:500px;height:200px;&quot; name=&quot;seq&quot;&gt;&gt;ENSG00000226530|ENST00000418775CACGCTCCTCCAGGGCGCAGAGAGCATCAAAGCATTATCACCGACCCAACACACACCGGCGCTGCGTAGTTTCTGCAGCAGGTGTGGGGTGAAGAGTTGCCACACAGCTGTCTGACACCGGGTGTACGGCGTCACCCCTCACTTCCTGGACAAGGCAACATTTTCCTAAAGGTCAAAGGAGAAAAATCCTATCATCTGAGGTGCTGAACAAAGAATTCAATAAATTCAATCAATCTGATGCACTCTGCAGTCGTTCTAAGTTCTTGTCTACTAAAATCCTCGACTACTACACATCCACAATCGCTTCTGTGAAACTCTTGGGACCAAATGGTTCAGAATTTGTCTCCCATACATACATAATATCTATGTCAAGGCCTGTAGTAATACTGTGGAATCTAATACAATGATATTTCTGTAGTGAAACATACAAATGTTCACTCCAAGATCTAAAGATTACAAATAGCATCATGTCAGTTCAAATCAGGTTTTATGGATAAATGAGTCAGGAAAAGTATGTTTTTCAAAGCATTTTGAATGTTAGACTTGTGCACAAGGGATTGTGAGTCTGAATCATATTTCCGTCAAC&lt;/textarea&gt;&lt;table&gt;&lt;tr&gt; &lt;td&gt;提交你的序列文件&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td&gt; &lt;label for=&quot;file&quot;&gt;文件名：&lt;/label&gt; &lt;input type=&quot;file&quot; style=&quot;width:800px;&quot; name=&quot;file&quot; id=&quot;file&quot;&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td&gt;&lt;input type=&quot;submit&quot; value=&quot;Submit&quot;&gt;&lt;input type=&quot;submit&quot; value=&quot;Submit&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;!-- 提示信息 --&gt;&lt;tr&gt; &lt;td style=&quot;color:red;font-size:10px;&quot;&gt;&lt;?php$err=isset($_GET[&quot;err&quot;])?$_GET[&quot;err&quot;]:&quot;&quot;;switch($err) &#123; case 1: echo &quot;请在输入序列或提交序列文件！&quot;; break; case 2: echo &quot;无法提交输入序列！&quot;; break; case 3: echo &quot;未登陆，无权操作！请&lt;a href=\&quot;login.php\&quot;&gt;登陆&lt;/a&gt;&quot;; break;&#125;?&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/form&gt; 执行BLAST操作，保存为localhost.php 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;?php//开启sessionsession_start();// 获取用户名和密码$username=isset($_SESSION[&apos;user&apos;])?$_SESSION[&apos;user&apos;]:&quot;&quot;;$password=isset($_SESSION[&apos;password&apos;])?$_SESSION[&apos;password&apos;]:&quot;&quot;;// 通过POST方法获取表单提交信息$seq=isset($_POST[&apos;seq&apos;])?$_POST[&apos;seq&apos;]:&quot;&quot;;// 若既未输入序列也未提交序列文件，则返回BLAST申请界面，并显示错误信息if ($_FILES[&quot;file&quot;][&quot;error&quot;] &gt; 0 &amp;&amp; empty($seq))&#123; header(&quot;Location:blastQuery.php?err=1&quot;);&#125;// 若输入了序列，则将序列写入upload文件夹下的文本中$seqfile=&quot;upload/query.fa&quot;;if(!empty($seq))&#123; if(!($file=fopen($seqfile, &quot;w&quot;)))&#123; header(&quot;Location:blastQuery.php?err=2&quot;); &#125; fwrite($file,$seq); fclose($file);&#125;if(!empty($username)&amp;&amp;!empty($password))&#123; // 连接数据库 $conn=new mysqli(&apos;localhost&apos;,$username,$password,&apos;testdb&apos;); if($conn-&gt;connect_error)&#123; header(&quot;Location:blastQuery.php?err=3&quot;); &#125; // 执行BLAST，优先用输入序列进行BLAST if(!empty($seq))&#123; system(&apos;/home/Ben/software/blast/ncbi-blast-2.6.0+/bin/blastn -query upload/query.fa -db blastdb/lncRNA_h -html -evalue 1 -num_descriptions 10 -num_alignments 10 -out blast.out&apos;); &#125;else&#123; system(&apos;/home/Ben/software/blast/ncbi-blast-2.6.0+/bin/blastn -query upload/&apos;.$_FILES[&quot;file&quot;][&quot;name&quot;].&apos; -db blastdb/lncRNA_h -html -evalue 1 -num_descriptions 10 -num_alignments 10 -out blast.out&apos;); &#125; // 打印BLAST结果 $file_out=fopen(&quot;blast.out&quot;,&quot;r&quot;) or exit(&quot;Unable to open file!&quot;); while(!feof($file_out))&#123; echo fgets($file_out); &#125;&#125;else&#123; header(&quot;Location:blastQuery.php?err=3&quot;);&#125;?&gt; 添加lincRNA表达谱获取在两个样本组的表达谱数据集简单介绍与下载该笔记中使用的数据集为 Nature Protocols 文章中用到的数据集 sample id Sex population ERR188245 Female GBR ERR188428 Female GBR ERR188337 Female GBR ERR188401 Male GBR ERR188257 Male GBR ERR188383 Male GBR ERR204916 Female YRI ERR188234 Female YRI ERR188273 Female YRI ERR188454 Male YRI ERR188104 Male YRI ERR188044 Male YRI 文章作者已经将数据集打包好了，下载链接：ftp://ftp.ccb.jhu.edu/pub/RNAseq_protocol/chrX_data.tar.gz 12345# 下载$ nohup wget -c ftp://ftp.ccb.jhu.edu/pub/RNAseq_protocol/chrX_data.tar.gz &gt;download.log 2&gt;&amp;1 &amp;# 解压$ tar zxvf chrX_data.tar.gz 解压后可以看到数据集的组成123456789|---chrX_data |---genes |---chrX.gtf |---genome |---chrX.fa |---indexes |---chrX_tran.[1-7].ht2 |---samples |---ERR*_chrX_[12].fastq.gz 跑RNA-seq分析流程RNA-seq分析流程，请参考 这里 一般第一步是创建hisat2索引，由于数据集中已经提供了（在chrX_data/indexes文件夹下），所以跳过这一步 hisat2比对 在比对前，先准备好一个保存样本Id的文本samplelist.txt： 123456789101112ERR188245ERR188428ERR188337ERR188401ERR188257ERR188383ERR204916ERR188234ERR188273ERR188454ERR188104ERR188044 开始执行批量比对 1234567$ mkdir map$ nohup cat samplelist.txt | while read i;do hisat2 -p 10 --dta -x indexes/chrX_tran -1 samples/$&#123;i&#125;_chrX_1.fastq.gz -2 samples/$&#123;i&#125;_chrX_2.fastq.gz | \ samtools sort -@ 8 -O bam -o map/$&#123;i&#125;_chrX.sort.bam 1&gt;map/$&#123;i&#125;_map.log 2&gt;&amp;1done &amp; stringtie转录本拼接 12345678910111213$ mkdir asm# 拼接$ cat cat samplelist.txt | while read i;do stringtie -p 16 -G genes/chrX.gtf -o asm/$&#123;i&#125;_chrX.gtf -l $i map/$&#123;i&#125;_chrX.sort.bam 1&gt;asm/$&#123;i&#125;_strg_assm.log 2&gt;&amp;1done# 准备mergelist.txt文件$ awk &apos;&#123;print &quot;asm/&quot;$0&quot;_chrX.gtf&quot;&#125;&apos; samplelist.txt &gt;mergelist.txt# merge多样本的转录本拼接结果$ stringtie --merge -p 16 -G genes/chrX.gtf -o asm/merge.gtf mergelist.txt 1&gt;asm/strg_merge.log 2&gt;&amp;1 stringtie定量 以read count进行定量，作为DESeq2或edgeR的输入 1234567891011121314151617$ mkdir quant# 转录本定量$ cat samplelist.txt | while read i;do stringtie -e -p 16 -G asm/merge.gtf -o quant/$&#123;i&#125;_chrX_quant.gtf map/$&#123;i&#125;_chrX.sort.bam 1&gt;quant/$&#123;i&#125;_chrX_quant.log 2&gt;&amp;1done# 整合多样本的转录本定量结果## 先准备sample_lst.txt文件，格式如下： ERR188021 &lt;PATH_TO_ERR188021.gtf&gt; ERR188023 &lt;PATH_TO_ERR188023.gtf&gt; ...$ perl -ne &apos;chomp;print &quot;$_\tquant/$&#123;_&#125;_chrX_quant.gtf\n&quot;&apos; samplelist.txt &gt;samplelist.quant.txt## 执行定量结果整合$ python2 /Path/To/prepDE.py -i sample_lst.txt 最后会在工作目录下产生两个read count定量文件： gene定量：gene_count_matrix.csv transcript定量：transcript_count_matrix.csv 想下载这两个文件，请点 这里 差异表达分析：DESeq2 DESeq2要求输入的表达矩阵是read counts 构建 DESeqDataSet 对象 123456789# 构建表达矩阵count_table&lt;-read.csv(&quot;gene_count_matrix.csv&quot;,row.names=&quot;gene_id&quot;)count_matrix&lt;-as.matrix(count_table)# 构建分组矩阵phenodata&lt;-read.csv(&quot;geuvadis_phenodata.csv&quot;,row.names=&quot;ids&quot;)# 构建 DESeqDataSet 对象dds &lt;- DESeqDataSetFromMatrix(countData = count_matrix, colData = phenodata, design = ~ population) 差异表达分析 12345dds &lt;- DESeq(dds)res &lt;- results(dds)resOrdered &lt;- res[order(res$padj),]diffResult &lt;- as.data.frame(resOrdered)write.table(diffResult,&quot;gene_diffResult.txt&quot;,sep=&quot;\t&quot;,quote=F) 这一步最终得到差异表达分析结果文件gene_diffResult.txt，想下载这两个文件，请点 这里 将差异表达结果写进MySQL过程与 part1 方法一：使用MySQLi 相同 12345678910111213# 创建表格DiffExp_resultmysql&gt; CREATE TABLE DiffExp_result ( `id` int(11) NOT NULL AUTO_INCREMENT, `RefSeq` varchar(255) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;RefSeq id&apos;, `baseMean` DOUBLE(40,20) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;the base mean over all rows&apos;, `log2FoldChange` DOUBLE(40,20) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;log2 fold change (MAP): treatment OHT vs Control&apos;, `lfcSE` DOUBLE(40,20) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;standard error: treatment OHT vs Control&apos;, `stat` DOUBLE(40,20) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;Wald statistic: treatment OHT vs Control&apos;, `pvalue` FLOAT(20) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;Wald test p-value: treatment OHT vs Control&apos;, `padj` FLOAT(20) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;BH adjusted p-values&apos;, PRIMARY KEY (`id`) ) ; 数据类型 描述 TINYINT(size) -128 到 127 常规。0 到 255 无符号*。在括号中规定最大位数。 SMALLINT(size) -32768 到 32767 常规。0 到 65535 无符号*。在括号中规定最大位数。 MEDIUMINT(size) -8388608 到 8388607 普通。0 to 16777215 无符号*。在括号中规定最大位数。 INT(size) -2147483648 到 2147483647 常规。0 到 4294967295 无符号*。在括号中规定最大位数。 BIGINT(size) -9223372036854775808 到 9223372036854775807 常规。0 到 18446744073709551615 无符号*。在括号中规定最大位数。 FLOAT(size,d) 带有浮动小数点的小数字。在括号中规定最大位数。在 d 参数中规定小数点右侧的最大位数。 DOUBLE(size,d) 带有浮动小数点的大数字。在括号中规定最大位数。在 d 参数中规定小数点右侧的最大位数。 DECIMAL(size,d) 作为字符串存储的 DOUBLE 类型，允许固定的小数点。 基于 part1 中的php脚本data_batch_import.php稍作修改即可复用 需要改动的位置如下图： 直接手动修改PHP脚本，保存为lincRNA_DiffExp_data_batch_import.php 以下只给出修改部分的代码 1234567891011121314151617181920212223// 预处理及绑定$stmt=$conn-&gt;prepare(&quot;INSERT INTO DiffExp_result (RefSeq,baseMean,log2FoldChange,lfcSE,stat,pvalue,padj) VALUES (?,?,?,?,?,?,?)&quot;);$stmt-&gt;bind_param(&quot;sdddddd&quot;,$RefSeq,$baseMean,$log2FoldChange,$lfcSE,$stat,$pvalue,$padj);// 打开文件，通过$argv[1]进行脚本传参$file=fopen(&quot;$argv[1]&quot;,&apos;r&apos;) or exit(&quot;Unable to open file!&quot;);// 逐行读取文件，并写入数据库fgets($file); // 跳过第一行表头while(!feof($file))&#123; $data=explode(&quot;\t&quot;,fgets($file)); // 以tab为间隔标识将字符串打散 // 设置参数 $RefSeq=$data[0]; $baseMean=$data[1]; $log2FoldChange=$data[2]; $lfcSE=$data[3]; $stat=$data[4]; $pvalue=$data[5]; $padj=$data[6]; // 执行 $stmt-&gt;execute();&#125; 最后执行php脚本，完成数据的批量导入 1$ php -f lincRNA_DiffExp_data_batch_import.php &lt;data-to-import&gt; 检索表达谱数据库过程类似于 part1 检索数据库 表单部分，提供以下检索筛选条件 log2FoldChange 一般认为其绝对值大于1，差异显著 p-value 一般认为其值小于等于0.05，差异显著 q-value 一般认为其值小于等于0.05，差异显著 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;form action=&quot;#&quot; method=&quot;post&quot;&gt;&lt;table style=&quot;text-align:center; border:0;&quot;&gt;&lt;tr&gt; &lt;td&gt;|log2FoldChange| &gt;=&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;text&quot; name=&quot;log2FoldChange&quot; value=&quot;1&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td&gt;p-value &lt;=&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;text&quot; name=&quot;p_value&quot; value=&quot;0.05&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td&gt;q-value &lt;=&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;text&quot; name=&quot;q_value&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td align=&quot;center&quot;&gt;&lt;input type=&quot;submit&quot; name=&quot;submit&quot; value=&quot;Submit&quot;&gt;&lt;/td&gt; &lt;td align=&quot;center&quot;&gt;&lt;input type=&quot;reset&quot; value=&quot;Reset&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;!-- 错误提示信息 --&gt;&lt;tr&gt; &lt;td style=&quot;color:red; font-size:10px;&quot;&gt;&lt;?php$err=isset($_GET[&quot;err&quot;])?$_GET[&quot;err&quot;]:&quot;&quot;;switch($err) &#123;case 1: echo &quot;表单填写错误：至少要填写一项！&quot;; break;case 2: echo &quot;表单填写错误：填写信息必须是数字！&quot;; break;case 3: echo &quot;未登陆，无权访问！&quot;; echo &quot;快去&lt;a href=&apos;login.php&apos;&gt;登录&lt;/a&gt;吧！&quot;; break;case 4: echo &quot;Update或Delete操作异常！未在数据库中找到对应记录！&quot;; break;&#125;?&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/form&gt; php脚本部分 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&lt;?php//开启sessionsession_start();// 获取用户名和密码$username=isset($_SESSION[&apos;user&apos;])?$_SESSION[&apos;user&apos;]:&quot;&quot;;$password=isset($_SESSION[&apos;password&apos;])?$_SESSION[&apos;password&apos;]:&quot;&quot;;// 获取表单提交数据$log2FoldChange=isset($_POST[&apos;log2FoldChange&apos;])?$_POST[&apos;log2FoldChange&apos;]:&quot;&quot;;$p_value=isset($_POST[&apos;p_value&apos;])?$_POST[&apos;p_value&apos;]:&quot;&quot;;$q_value=isset($_POST[&apos;q_value&apos;])?$_POST[&apos;q_value&apos;]:&quot;&quot;;$submit=isset($_POST[&apos;submit&apos;])?$_POST[&apos;submit&apos;]:&quot;&quot;;if($submit)&#123; // 判断提交的检索信息是否满足要求：三个提交的变量,至少一个不为空，且若不为空则必须是数值 if(empty($log2FoldChange)&amp;&amp;empty($p_value)&amp;&amp;empty($q_value))&#123; header(&quot;Location:databaseQuery_DiffExp.php?err=1&quot;); &#125;elseif((empty($log2FoldChange)||is_numeric($log2FoldChange))&amp;&amp;(empty($p_value)||is_numeric($p_value))&amp;&amp;(empty($q_value)||is_numeric($q_value)))&#123; // 连接数据库 $conn=new mysqli(&apos;localhost&apos;,$username,$password,&apos;testdb&apos;); if($conn-&gt;connect_error)&#123; header(&quot;Location:databaseQuery_DiffExp.php?err=3&quot;); &#125; // 创建SQL查询语句 $sql=&quot;SELECT * from DiffExp_result where&quot;; // 根据实际表单填写情况追加查询条件 $bool_and=false; if(!empty($log2FoldChange))&#123; $sql .= &quot; log2FoldChange &gt;= $log2FoldChange or log2FoldChange &lt;= -$log2FoldChange&quot;; $bool_and=true; &#125; if(!empty($p_value))&#123; if($bool_and)&#123; $sql .= &quot; and&quot;; &#125; $sql .= &quot; pvalue &lt;= $p_value&quot;; $bool_and=true; &#125; if(!empty($q_value))&#123; if($bool_and)&#123; $sql .= &quot; and&quot;; &#125; $sql .= &quot; padj &lt;= $q_value&quot;; &#125; $sql .= &quot;;&quot;; // 执行查询语句 $result=$conn-&gt;query($sql); if($result-&gt;num_rows &gt; 0)&#123; // 输出查询结果 echo &quot;&lt;table&gt;&lt;tr&gt;&lt;th&gt;RefSeq&lt;/th&gt;&lt;th&gt;baseMean&lt;/th&gt;&lt;th&gt;log2FoldChange&lt;/th&gt;&lt;th&gt;lfcSE&lt;/th&gt;&lt;th&gt;stat&lt;/th&gt;&lt;th&gt;pvalue&lt;/th&gt;&lt;th&gt;padj&lt;/th&gt;&lt;th&gt;操作&lt;/th&gt;&lt;/tr&gt;&quot;; while($row = $result-&gt;fetch_array())&#123; echo &quot;&lt;tr&gt;&lt;td&gt;&quot;.$row[&apos;RefSeq&apos;].&quot;&lt;/td&gt;&lt;td&gt;&quot;.$row[&apos;baseMean&apos;].&quot;&lt;/td&gt;&lt;td&gt;&quot;.$row[&apos;log2FoldChange&apos;].&quot;&lt;/td&gt;&lt;td&gt;&quot;.$row[&apos;lfcSE&apos;].&quot;&lt;/td&gt;&lt;td&gt;&quot;.$row[&apos;stat&apos;].&quot;&lt;/td&gt;&lt;td&gt;&quot;.$row[&apos;pvalue&apos;].&quot;&lt;/td&gt;&lt;td&gt;&quot;.$row[&apos;padj&apos;].&quot;&lt;/td&gt;&lt;td&gt;&lt;a href=\&quot;update_form.php?id=&quot;.$row[&apos;id&apos;].&quot;&amp;dbname=DiffExp_result\&quot; style=\&quot;padding:2px;\&quot;&gt;Update&lt;/a&gt;&lt;a href=\&quot;barplox.php?RefSeq=&quot;.$row[&apos;RefSeq&apos;].&quot;\&quot; style=\&quot;padding:2px;\&quot;&gt;Barplox&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;\n&quot;; &#125; &#125;else&#123; echo &quot;未检索到满足条件的记录!&quot;; &#125; &#125;else&#123; header(&quot;Location:databaseQuery_DiffExp.php?err=2&quot;); &#125;&#125;?&gt; 画图：单个基因表达谱实现思路： 用户在检索页面的记录栏的最后一项点击Barplot以后，向绘图脚本(barplot.php)传递对应记录的RefSeq 绘图脚本(barplot.php)根据传递过来的RefSeq，调用绘图用的底层R脚本(barplot.R)，调用的同时传递参数——RefSeq值 barplot.R实现的功能：读取表达谱文件（跑RNA-seq分析流程：stringtie定量 产生的gene_count_matrix.csv文件），根据传递过来的RefSeq值匹配出对应的基因的表达谱，然后用ggplot把图画出来，并将图保存为图像文件 绘图脚本(barplot.php)将图片用&lt;img&gt;标签呈现给用户 绘图脚本(barplot.php) 123456789101112131415161718192021222324&lt;?php//开启sessionsession_start();// 获取用户名和密码$username=isset($_SESSION[&apos;user&apos;])?$_SESSION[&apos;user&apos;]:&quot;&quot;;$password=isset($_SESSION[&apos;password&apos;])?$_SESSION[&apos;password&apos;]:&quot;&quot;;// 用GET方法获取提交的数据$RefSeq=isset($_GET[&apos;RefSeq&apos;])?$_GET[&apos;RefSeq&apos;]:&quot;&quot;;// 登录数据库，检查用户是否有操作权限$conn=new mysqli(&apos;localhost&apos;,$username,$password,&apos;testdb&apos;);if($conn-&gt;connect_error)&#123; header(&quot;Location:databaseQuery_DiffExp.php?err=3&quot;);&#125;else&#123; system(&apos;Rscript barplot.R &apos;.$RefSeq); // 检查文件是否存在 if(file_exists(&apos;barplot.jpg&apos;))&#123; echo &quot;&lt;p align=\&quot;center\&quot;&gt;&lt;img src=./barplot.jpg width=600 &gt;&lt;/p&gt;&quot;; &#125;else&#123; echo &quot;&lt;p style=\&quot;color:red; text-align:center; font-size:50px;\&quot;&gt;绘图失败！&lt;/p&gt;&quot;; &#125;&#125;?&gt; 底层R脚本(barplot.R) 注意： 该脚本底层依赖ggplot2包，若未安装请提前安装好，推荐以下两种安装方法： 直接在R的交互环境下，使用install.packages(&quot;ggplot2&quot;) 使用conda进行安装 12345678910111213library(ggplot2)# 获取传入参数args&lt;-commandArgs(T)# 载入全表达谱profile &lt;- read.csv(&quot;gene_count_matrix.csv&quot;)profile_matrix &lt;- as.matrix(profile[,-1])rownames(profile_matrix)&lt;-profile$gene_id# 获取目标基因的表达谱geneExp &lt;- data.frame(Sample=colnames(profile_matrix),Expression=profile_matrix[args[1],])#画图并导出jpeg(&quot;barplot.jpg&quot;)ggplot(geneExp)+geom_bar(aes(x=sample,y=expression),stat=&quot;identity&quot;)+theme(axis.text.x = element_text(angle = 60, hjust = 0.5, vjust = 0.5))dev.off() 添加批量提交数据功能基本的实现思路为： 提供文件提交的输入框，同时让用户指定向哪个数据库（实际上是不同的表格）提交数据。将用户提交的文件保存为服务器某个目录下的临时文件，然后用 part1 将数据写入数据库中：方法一：使用MySQLi的脚本lincRNA_Ano_data_batch_import.php 或者，part2 将差异表达结果写进MySQL的脚本lincRNA_DiffExp_data_batch_import.php 将临时文件的数据追加导入相应的表格中 预备知识： 文件上传，请点 这里 要实现文件上传，首先要在php脚本所在的文件夹下创建一个权限全开的upload文件夹 12$ mkdir upload$ chmod 777 upload 接着开始编辑php脚本，保存为batch_submit_data.php 表单部分 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;form action=&quot;batch_submit_data.php&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt;&lt;table&gt;&lt;tr&gt; &lt;td&gt;选择提交数据库：&lt;/td&gt; &lt;td&gt; &lt;select name=&quot;dbname&quot;&gt; &lt;option value=&quot;&quot;&gt;选择数据库:&lt;/option&gt; &lt;option value=&quot;ano&quot;&gt;基因结构注释&lt;/option&gt; &lt;option value=&quot;diffexp&quot;&gt;基因表达谱&lt;/option&gt; &lt;/select&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td&gt;选择物种（基因结构注释）：&lt;/td&gt; &lt;td&gt; &lt;select name=&quot;specie&quot;&gt; &lt;option value=&quot;&quot;&gt;选择一个物种:&lt;/option&gt; &lt;option value=&quot;human&quot;&gt;Homo Sapiens&lt;/option&gt; &lt;option value=&quot;mouse&quot;&gt;Mus musculus&lt;/option&gt; &lt;/select&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td&gt;选择上传数据文件：&lt;/td&gt; &lt;td&gt; &lt;label for=&quot;file&quot;&gt;文件名：&lt;/label&gt; &lt;input type=&quot;file&quot; name=&quot;file&quot; id=&quot;file&quot;&gt;&lt;br&gt; &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td align=&quot;center&quot;&gt;&lt;input type=&quot;submit&quot; name=&quot;submit&quot; value=&quot;提交&quot;&gt;&lt;/td&gt; &lt;td align=&quot;center&quot;&gt;&lt;input type=&quot;reset&quot; value=&quot;Reset&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;!-- 报错信息 --&gt;&lt;tr&gt; &lt;td style=&quot;color:red; font-size:10px;&quot;&gt;&lt;?php$err=isset($_GET[&quot;err&quot;])?$_GET[&quot;err&quot;]:&quot;&quot;;switch($err) &#123;case 1: echo &quot;表单填写错误：必须选择一个数据库！&quot;; break;case 2: echo &quot;上传文件格式不符合要求！请参照&lt;a href=\&quot;formatGuid.html\&quot;&gt;格式要求&lt;/a&gt;进行修改！&quot;; break;case 3: echo &quot;未登陆，无权访问！&quot;; echo &quot;快去&lt;a href=&apos;login.php&apos;&gt;登录&lt;/a&gt;吧！&quot;; break;&#125;?&gt;&lt;/form&gt; php脚本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111&lt;?php//开启sessionsession_start();// 获取用户名和密码$username=isset($_SESSION[&apos;user&apos;])?$_SESSION[&apos;user&apos;]:&quot;&quot;;$password=isset($_SESSION[&apos;password&apos;])?$_SESSION[&apos;password&apos;]:&quot;&quot;;// 创建Ano_check函数，进行上传文本的文件格式检查function Ano_check($fname)&#123; $file=fopen(&quot;upload/$fname&quot;,&quot;r&quot;) or exit(&quot;Unable to open file!&quot;); $bool_mark=1; while(!feof($file))&#123; $data=explode(&quot;\t&quot;,fgets($file)); // 检查第一列，是否为1-22或XY if(!preg_match(&quot;\d&#123;1,2&#125;|[XY]&quot;,$data[0]))&#123; $bool_mark=0; break; &#125; // 检查第二三列，是否都是英文字母 if(!preg_match(&quot;[a-zA-Z]&quot;,$data[1])||!preg_match(&quot;[a-zA-Z]&quot;,$data[2]))&#123; $bool_mark=0; break; &#125; // 检查第四五列，即start和end，是否是数字 if(!is_numeric($data[3])||!is_numeric($data[4]))&#123; $bool_mark=0; break; &#125; // 检查第六八列，即GeneId和TranscriptId,是否以ENSG或ENSG打头 if(!preg_match(&quot;^ENSG&quot;,$data[5])||!preg_match(&quot;^(ENST)|-&quot;,$data[5]))&#123; $bool_mark=0; break; &#125; &#125; return $bool_mark;&#125;// 创建DiffExp_check函数，进行上传文本的文件格式检查function DiffExp_check($fname)&#123; $file=fopen(&quot;upload/$fname&quot;,&quot;r&quot;) or exit(&quot;Unable to open file!&quot;); $bool_mark=1; while(!feof($file))&#123; $data=explode(&quot;\t&quot;,fgets($file)); // 检查第一列，RefSeq，是否以大写字母打头 if(!preg_match(&quot;^[A-Z]&#123;2,&#125;&quot;,$data[0]))&#123; $bool_mark=0; break; &#125; // 检查其他列，是否都是数字 if(!is_numeric($data[1])||!is_numeric($data[2])||!is_numeric($data[3])||!is_numeric($data[4])||!is_numeric($data[5])||!is_numeric($data[6]))&#123; $bool_mark=0; break; &#125; &#125; return $bool_mark;&#125;// 获取表单提交数据$dbname=isset($_POST[&apos;dbname&apos;])?$_POST[&apos;dbname&apos;]:&quot;&quot;;$specie=isset($_POST[&apos;specie&apos;])?$_POST[&apos;specie&apos;]:&quot;&quot;;$submit=isset($_POST[&apos;submit&apos;])?$_POST[&apos;submit&apos;]:&quot;&quot;;if($submit)&#123; // 检查文件上传是否成功 if ($_FILES[&quot;file&quot;][&quot;error&quot;] &gt; 0) &#123; echo &quot;错误：&quot; . $_FILES[&quot;file&quot;][&quot;error&quot;] . &quot;&lt;br&gt;&quot;; &#125; // 进行文件格式检查 if(empty($dbname))&#123; header(&quot;Location:batch_submit_data.php?err=1&quot;); &#125; elseif($dbname==&quot;ano&quot;)&#123; $check_stat=Ano_check($_FILE[&apos;file&apos;][&apos;name&apos;]); &#125;else&#123; $check_stat=DiffExp_check($_FILE[&apos;file&apos;][&apos;name&apos;]); &#125; // 判断文件格式是否符合要求 if(!$check_stat)&#123; header(&quot;Location:batch_submit_data.php?err=2&quot;); // 判断用户名和密码是否已经设置 &#125;elseif(!empty($username)&amp;&amp;!empty($password))&#123; // 连接数据库 $conn=new mysqli(&apos;localhost&apos;,$username,$password,&apos;testdb&apos;); if($conn-&gt;connect_error)&#123; header(&quot;batch_submit_data.php?err=3&quot;); &#125; // 设置数据库表格名，并执行批量操作 if($dbname==&quot;ano&quot;)&#123; if($specie==&quot;human&quot;)&#123; $table=&quot;lincRNA_h&quot;; &#125;else&#123; $table=&quot;lincRNA_m&quot;; &#125; system(&quot;php -f lincRNA_Ano_data_batch_import.php &quot;.$table.&quot; upload/&quot;.$_FILE[&apos;file&apos;][&apos;name&apos;].&quot; &amp;&quot;); &#125;else&#123; system(&quot;php -f lincRNA_DiffExp_data_batch_import.php upload/&quot;.$_FILE[&apos;file&apos;][&apos;name&apos;].&quot; &amp;&quot;); &#125; // 调用之前的php脚本进行批量提交操作 &#125;else&#123; header(&quot;Location:batch_submit_data.php?err=3&quot;); &#125;&#125;?&gt; 添加Update功能实现思路： 用户在检索页面的记录栏的最后一项点击update以后，跳转到update表单填写页面(update_form.php脚本)，同时向该php脚本传递对应记录的id update表单填写页面(update_form.php脚本)根据传递过来的id，把对应记录的信息以表单形式呈现，同时表单的每个输入框可以收集用户的修改信息 用户修改后点击”submit”，执行update操作(updata_operate_Ano.php或updata_operate_DiffExp.php脚本)，同时将操作状态告知用户：是不是成功了？ 脚本update_form.php 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283&lt;?php//开启sessionsession_start();// 获取用户名和密码$username=isset($_SESSION[&apos;user&apos;])?$_SESSION[&apos;user&apos;]:&quot;&quot;;$password=isset($_SESSION[&apos;password&apos;])?$_SESSION[&apos;password&apos;]:&quot;&quot;;// 获取通过GET方法向该脚本传递的变量$id=isset($_GET[&apos;id&apos;])?$_GET[&apos;id&apos;]:&quot;&quot;;$dbname=isset($_GET[&apos;dbname&apos;])?$_GET[&apos;dbname&apos;]:&quot;&quot;;$statu=isset($_GET[&apos;statu&apos;])?$_GET[&apos;statu&apos;]:&quot;&quot;;// 检查用户是否登录if(!empty($username)&amp;&amp;!empty($password))&#123; // 连接数据库 $conn=new mysqli(&apos;localhost&apos;,$username,$password,&apos;testdb&apos;); if($conn-&gt;connect_error)&#123; if($dbname==&quot;lincRNA_h&quot;||$dbname==&quot;lincRNA_m&quot;)&#123; header(&quot;Location:databaseQuery_Ano.php?err=3&quot;); &#125;else&#123; header(&quot;Location:databaseQuery_DiffExp.php?err=3&quot;); &#125; &#125; // 根据id或dbname，从数据库中检索出相应的记录 // 构造SQL查询语句 $sql=&quot;SELECT * from $dbname where id=$id&quot;; // 执行查询命令 $result = $conn-&gt;query($sql); // 选择对应的updata操作脚本名 if($dbname==&quot;lincRNA_h&quot;||$dbname==&quot;lincRNA_m&quot;)&#123; $phpscript=&quot;update_operate_Ano.php&quot;; &#125;else&#123; $phpscript=&quot;update_operate_DiffExp.php&quot;; &#125; // 将查询结果以表单形式呈现 if($result-&gt;num_rows &gt; 0)&#123; echo &quot;&lt;form action=\&quot;&quot;.$phpscript.&quot;?id=&quot;.$id.&quot;&amp;dbname=&quot;.$dbname.&quot;\&quot; method=\&quot;post\&quot;&gt;\n&quot;; echo &quot;&lt;table&gt;\n&lt;tr&gt;\n\t&lt;th&gt;Feature&lt;/th&gt;&lt;th&gt;Value&lt;/th&gt;\n&lt;/tr&gt;\n&quot;; $row = $result-&gt;fetch_array(); // 遍历关联数组的每一个元素，并将它们填到表单里 foreach($row as $key=&gt;$key_value)&#123; if(is_numeric($key)||$key==&quot;id&quot;)&#123; continue; &#125; echo &quot;&lt;tr&gt;\n&quot;; echo &quot;\t&lt;td&gt;&quot;.$key.&quot;&lt;/td&gt;\n&quot;; echo &quot;\t&lt;td&gt;&lt;input type=\&quot;text\&quot; name=\&quot;&quot;.$key.&quot;\&quot; value=\&quot;&quot;.$key_value.&quot;\&quot;&gt;&lt;/td&gt;\n&lt;/tr&gt;&quot;; &#125; // 表单的submit和reset按钮 echo &quot;&lt;tr&gt;\n\t&lt;td align=\&quot;center\&quot;&gt;&lt;input type=\&quot;submit\&quot; name=\&quot;submit\&quot; value=\&quot;Submit\&quot;&gt;&lt;/td&gt;\n&quot;; echo &quot;\t&lt;td align=\&quot;center\&quot;&gt;&lt;input type=\&quot;reset\&quot; value=\&quot;Reset\&quot;&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&quot;; // 提示任务执行状态：成功或失败 echo &quot;&lt;tr&gt;\n\t&lt;td style=\&quot;color:red; font-size:10px;\&quot;&gt;&quot;; switch($statu) &#123; case 1: echo &quot;该记录已修改成功！&quot;; case 2: echo &quot;该记录修改失败！&quot;; case 3: echo &quot;修改数据类型不符合格式要求，请重新填写！&quot;; case 4: echo &quot;未登陆，没有修改权限！请&lt;a href=\&quot;login.php\&quot;&gt;登陆&lt;/a&gt;&quot;; &#125; echo &quot;&lt;/td&gt;\n&lt;/tr&gt;\n&quot;; echo &quot;&lt;/table&gt;\n&lt;/form&gt;\n&quot;; &#125;else&#123; if($dbname==&quot;lincRNA_h&quot;||$dbname==&quot;lincRNA_m&quot;)&#123; header(&quot;Location:databaseQuery_Ano.php?err=4&quot;); &#125;else&#123; header(&quot;Location:databaseQuery_DiffExp.php?err=4&quot;); &#125; &#125;&#125;else&#123; if($dbname==&quot;lincRNA_h&quot;||$dbname==&quot;lincRNA_m&quot;)&#123; header(&quot;Location:databaseQuery_Ano.php?err=3&quot;); &#125;else&#123; header(&quot;Location:databaseQuery_DiffExp.php?err=3&quot;); &#125;&#125;?&gt; 脚本update_operate_Ano.php 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566&lt;?php//开启sessionsession_start();// 获取用户名和密码$username=isset($_SESSION[&apos;user&apos;])?$_SESSION[&apos;user&apos;]:&quot;&quot;;$password=isset($_SESSION[&apos;password&apos;])?$_SESSION[&apos;password&apos;]:&quot;&quot;;// 获取通过GET方法向该脚本传递的变量$id=isset($_GET[&apos;id&apos;])?$_GET[&apos;id&apos;]:&quot;&quot;;$dbname=isset($_GET[&apos;dbname&apos;])?$_GET[&apos;dbname&apos;]:&quot;&quot;;// 获取通过POST方法向该脚本传递的变量$chrom=isset($_POST[&apos;chrom&apos;])?$_POST[&apos;chrom&apos;]:&quot;&quot;;$biotype=isset($_POST[&apos;biotype&apos;])?$_POST[&apos;biotype&apos;]:&quot;&quot;;$feature=isset($_POST[&apos;feature&apos;])?$_POST[&apos;feature&apos;]:&quot;&quot;;$start=isset($_POST[&apos;start&apos;])?$_POST[&apos;start&apos;]:&quot;&quot;;$end=isset($_POST[&apos;end&apos;])?$_POST[&apos;end&apos;]:&quot;&quot;;$geneid=isset($_POST[&apos;geneid&apos;])?$_POST[&apos;geneid&apos;]:&quot;&quot;;$genename=isset($_POST[&apos;genename&apos;])?$_POST[&apos;genename&apos;]:&quot;&quot;;$transcriptid=isset($_POST[&apos;transcriptid&apos;])?$_POST[&apos;transcriptid&apos;]:&quot;&quot;;$exon=isset($_POST[&apos;exon&apos;])?$_POST[&apos;exon&apos;]:&quot;&quot;;// 进行提交数据的格式检查$bool_mark=1;// 检查chrom，是否为1-22或XYif(!preg_match(&quot;\d&#123;1,2&#125;|[XY]&quot;,$chrom))&#123; $bool_mark=0;&#125;// 检查biotype和feature，是否都是英文字母if(!preg_match(&quot;[a-zA-Z]&quot;,$biotype)||!preg_match(&quot;[a-zA-Z]&quot;,$feature))&#123; $bool_mark=0;&#125;// 检查start、end和exon，是否是数字if(!is_numeric($start)||!is_numeric($end)||!is_numeric($exon))&#123; $bool_mark=0;&#125;// 检查GeneId和TranscriptId,是否以ENSG或ENSG打头if(!preg_match(&quot;^ENSG&quot;,$geneid)||!preg_match(&quot;^(ENST)|-&quot;,$transcriptid))&#123; $bool_mark=0;&#125;// 若用户提交的修改申请不符合格式要求，则返回update表单填写页面，并提示错误信息if(!$bool_mark)&#123; header(&quot;Location:update_form.php?statu=3&quot;);&#125;else&#123; // 判断用户名和密码是否已经设置 if(!empty($username)&amp;&amp;!empty($password))&#123; // 连接数据库 $conn=new mysqli(&apos;localhost&apos;,$username,$password,&apos;testdb&apos;); if($conn-&gt;connect_error)&#123; header(&quot;Location:update_form.php?statu=4&quot;); &#125; // 创建SQL的UPDATE语句 $sql=&quot;UPDATE $dbname SET chrom=&apos;$chrom&apos;,biotype=&apos;$biotype&apos;,feature=&apos;$feature&apos;,start=$start,end=$end,geneid=&apos;$geneid&apos;,genename=&apos;$genename&apos;,transcriptid=&apos;$transcriptid&apos;,exon=$exon where id=$id;&quot;; // 执行SQL语句 if($conn-&gt;query($sql)===TRUE)&#123; header(&quot;Location:update_form.php?statu=1&quot;); &#125;else&#123; header(&quot;Location:update_form.php?statu=2&quot;); &#125; &#125;else&#123; header(&quot;Location:update_form.php?statu=3&quot;); &#125;&#125;?&gt; 脚本updata_operate_DiffExp.php 12345678&lt;?php//开启sessionsession_start();// 获取用户名和密码$username=isset($_SESSION[&apos;user&apos;])?$_SESSION[&apos;user&apos;]:&quot;&quot;;$password=isset($_SESSION[&apos;password&apos;])?$_SESSION[&apos;password&apos;]:&quot;&quot;;// 获取通过GET方法向该脚本传递的变量 番外篇：前端优化——html+css登录界面优化实现效果： 山寨主流网站的登录界面，即将页面分为左右两半，左侧放置图片，右侧放置登录信息输入框 12345678910111213141516171819202122232425262728&lt;head&gt;&lt;title&gt;登录&lt;/title&gt;&lt;style&gt;/* 设置背景色：碧绿色 */body&#123; background-color:#7FFFAA; margin:50px;&#125;form &#123; background-color:white; width:100%; margin:auto; padding:70px 0;&#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div&gt; &lt;div style=&quot;float:left; width:50%; height:100%;&quot;&gt; &lt;img src=./picture/lncRNAdb-logo.jpg width=100% /&gt; &lt;/div&gt; &lt;div style=&quot;float:right; width:50%; height:100%;&quot;&gt; . . . &lt;/div&gt;&lt;/div&gt;&lt;/body&gt; 导航条菜单的制作实现效果： 将页面分为左右部分，左侧放置垂直导航栏，右侧放置主面板 使用外部css制定页面渲染风格，css文件保存为private.css 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/* 导航栏样式 *//* 页面左侧主导航栏（垂直导航栏），占据页面左侧10%的空间，且位置固定 */.nav_vertical &#123; margin:0; padding:0; width:10%; list-style-type:none; background-color: #f1f1f1; position: fixed; height: 100%;&#125;.nav_vertical li a&#123; display:block; padding: 8px 16px; text-decoration:none; color: #000; /* 字体颜色：灰色 */&#125;.nav_vertical li a:hover&#123; /* 鼠标悬停时链接块颜色：橙色 */ background-color:#F60; color:#fff&#125;.nav_vertical li a.active&#123; background-color: #4CAF50; /* 激活的链接块颜色：绿色 */ color:#fff;&#125;/* 页面右侧子导航栏（水平导航栏），用于在进行数据库检索时，在不同数据库间切换 */.nav_horizontal &#123; margin-left:10px; padding:0; list-style-type:none;&#125;.nav_horizontal li &#123; display:inline;&#125;.nav_horizontal li a&#123; padding: 8px 16px; text-decoration:none; background-color: #f1f1f1; color: #000; /* 字体颜色：灰色 */&#125;.nav_horizontal li a:hover&#123; /* 鼠标悬停时链接块颜色：橙色 */ background-color:#F60; color:#fff&#125;.nav_horizontal li a.active&#123; background-color: #4CAF50; /* 激活的链接块颜色：绿色 */ color:#fff;&#125;/* 链接样式 */a:link &#123;color:#0000FF;text-decoration:none;&#125; /* 蓝色 */a:visited &#123;color:#778899;text-decoration:none;&#125; /* 灰色 */a:hover &#123;color:#000000;text-decoration:none;&#125; /* 黑色 */a:active &#123;color:#FF704D;text-decoration:none;&#125;/* 表格样式 *//* 检索表单部分表格样式 */.result &#123;border-collapse:collapse;&#125;table,th,td &#123;border:1px solid black;&#125;th &#123;width:100%;height:50px;&#125; 则在每个php脚本前加入以下代码： 1234567891011121314151617181920212223&lt;head&gt;&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;private.css&quot;&gt;&lt;/head&gt;&lt;body&gt;&lt;ul class=&quot;nav_vertical&quot;&gt; &lt;li&gt;&lt;a class=&quot;active&quot; href=&quot;loginsucc.php&quot;&gt;主页&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;databaseQuery_Ano.php&quot;&gt;检索&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;batch_submit_data.php&quot;&gt;提交数据&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#&quot;&gt;BLAST&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;help.html&quot;&gt;帮助&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;logout.php&quot;&gt;退出&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div style=&quot;margin-left:10%; padding:10px; height:100%;&quot;&gt;&lt;!-- 若为检索部分，请添加以下部分代码 --&gt;&lt;!-- &lt;ul class=&quot;nav_horizontal&quot;&gt; &lt;li&gt;&lt;a class=&quot;active&quot; href=&quot;databaseQuery_Ano.php&quot;&gt;基因结构注释&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;databaseQuery_DiffExp.php&quot;&gt;基因表达谱&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt; --&gt;...&lt;/div&gt;&lt;/body&gt; 参考资料： (1) PHP+MySQLi实现注册和登陆 (2) Pertea M, Kim D, Pertea G M, et al. Transcript-level expression analysis of RNA-seq experiments with HISAT, StringTie and Ballgown[J]. Nature Protocols, 2016, 11(9):1650. (3) Analysis pipeline for RNA-seq (4) W3School：SQL 数据类型]]></content>
      <categories>
        <category>前后端技术</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>MySQL</tag>
        <tag>CSS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前端小技巧：加载并解析Markdown文档]]></title>
    <url>%2F2019%2F02%2F09%2FLoad-and-resolve-MarkdownDoc%2F</url>
    <content type="text"><![CDATA[问题描述与分析作为程序猿，应该多多少少都用过Markdown，或者至少读过别人用Markdown语法写的一些文档，比如在GitHub有一个你要用的开源程序，而你又是第一回用它，那么你一般会在这个仓库的Readme里读一读开发者提供的工具说明和使用的相关信息，这部分文档一般就是用Markdown的语法写的 此处以李恒大神的BWA的Readme为例 简单来说，Markdown就是简化阉割过的HTML，优点是语法简单高效，缺点就是HTML中一些稍微高级复杂一点的效果，比如文本居中，Markdown就无法实现，所以Markdown一般被用来写对页面排版要求不高，以文字为主的笔记和文档 如果你一开始用Markdown写了文档，想要把它放到你的网页上去，并以解析后的形式呈现 左边是原始Markdown文档，右边是解析后的网页效果 那么你有两种实现途径： 将Markdown文档以HTML形式导出，在网页上加载之前导出的HTML文件即可 原始网页获取Markdown文档，用js将Markdown文本解析成HTML文本，动态修改原始网页对应标签节点的内容，就实现了Markdown文本的解析和展示 第一种方法： 技术难度最小，也很容易理解，但是操作起来比较繁琐：你的Markdown文档一般是要经常修改更新的，那么每修改一次，你就需要重新导出，然后将之前导出的那一版HTML替换掉 第二种方法： 技术难度上要高一些，但是目前已经有可以调用的JS框架来实现Markdown2HTML的解析 最大的优点就是，每一次修改更新很方便，只需要对Markdown文档就可以了 下面我们对第二种方法的实现过程进行详细的说明 用AJAX获取Markdown文档上一部分已经提到，我们需要先让原始网页请求服务器中的Markdown文档 这一步需要使用AJAX（中文音译为阿甲克斯），这里先对AJAX作一个简单的科普： AJAX = Asynchronous JavaScript and XML（异步的 JavaScript 和 XML），AJAX 是一种用于创建快速动态网页的技术 首先要说清楚什么是动态网页。与动态网页相对的是静态网页，也就是大家都知道的用HTML写的网页，这种网页的一个鲜明的特点就是它的内容一旦确定就无法改变，在互联网发展的最早期的时候大家用的都是这种静态网页。动态网页要求页面的内容能进行更新。那么怎么让原先的静态网页动起来成为动态网页呢？也有两种方法： 重载整个网页面来更新页面内容 上一个页面状态与下一个页面状态，它们之间有相同不变的部分，也有发生了改变的部分，那么我只需要把那些需要发生变化的部分进行修改就行了。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新 第一种方法的不经济显而易见，不管各个页面节点前后是否发生了改变，统一都进行刷新 AJAX就是采用第二种页面刷新方法：通过在后台与服务器进行少量数据交换，向服务器请求数据，解析请求的数据，然后对原页面需要刷新的部分进行刷新 AJAX工作原理 12345678910111213141516171819202122232425262728293031// 获取表单变量var f=form;// 创建XMLHttpRequest对象var xmlhttp;if (window.XMLHttpRequest)&#123; // IE7+, Firefox, Chrome, Opera, Safari 浏览器执行代码 xmlhttp=new XMLHttpRequest();&#125;else&#123; // IE6, IE5 浏览器执行代码 xmlhttp=new ActiveXObject(&quot;Microsoft.XMLHTTP&quot;);&#125;// 得到服务器响应后，对得到的Markdown文档进行解析xmlhttp.onreadystatechange=function()&#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; // 这里调用了marked框架中的marked函数实现Markdown2HTML的解析 document.getElementById(&quot;content&quot;).innerHTML=marked(xmlhttp.responseText); &#125;&#125;// 向服务器发送请求，获取你需要的Markdown文档xmlhttp.open(&quot;GET&quot;,f.q.value,true);xmlhttp.send();&#125; 对上面的脚本继续简单的说明： 这个是AJAX的基本语法框架 完成三步操作： 创建http请求对象，告诉服务器我要哪个Markdown文档 向服务器发起http请求 收到服务器的反馈数据，得到请求的Markdown文档，进行数据的解析，实现Markdown2HTML 那么这里有几个问题需要解答： 1. 怎么指定我想要的Markdown文档呢？ 看上面的代码部分的最后一小部分，有这样一行命令： 12&gt; xmlhttp.open(&quot;GET&quot;,f.q.value,true);&gt; 这个函数的第二个参数f.q.value，指定了你要的Markdown文档的地址，所以如果想要指定Markdown文档就需要对这个变量进行设定 怎么设定这个变量？ 用表单传参来实现，这里的f就是表单变量，表单部分怎么实现请点 这里 用表单指定Markdown文档12345678910&lt;form name=&quot;form&quot; action=&quot;&quot; method=&quot;post&quot;&gt; &lt;select name=&quot;q&quot;&gt; &lt;option class=&quot;active&quot; alue=&quot;&quot;&gt;选择一个笔记:&lt;/option&gt; &lt;option value=&quot;A.md&quot;&gt;第一个Markdown文档&lt;/option&gt; &lt;option value=&quot;B.md&quot;&gt;第二个Markdown文档&lt;/option&gt; &lt;option value=&quot;C.md&quot;&gt;第三个Markdown文档&lt;/option&gt; &lt;option value=&quot;Evalue-TranscriptomeData.md&quot;&gt;第四个Markdown文档&lt;/option&gt; &lt;/select&gt; &lt;input type=&quot;button&quot; value=&quot;显示&quot; onclick=&quot;showMarkdown()&quot;&gt;&lt;/form&gt; 表单的显示效果如下： 点击表单中的“显示”按钮后会执行showMarkdown( )函数，即 用AJAX获取Markdown文档 部分的那个函数，并且将表单选择的信息通过form变量传递给了showMarkdown( )函数中的f变量 这样就通过表单设定了用户指定的Markdown文档 引用marked框架解析Markdown文档这里采用的是GitHub上的名为marked的JS框架，链接：https://github.com/markedjs/marked 要想使用这个框架，需要在html脚本的头信息中引用这个框架： 123456789&lt;html&gt;&lt;head&gt; ... &lt;script src=&quot;https://cdn.jsdelivr.net/npm/marked/marked.min.js&quot;&gt;&lt;/script&gt; ...&lt;/head&gt;... 引用这个框架后就可以使用里面定义的marked( )函数进行Markdown文本解析了 附：完整的实现脚本12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; . . . // 在这里对marked框架进行引用 &lt;script src=&quot;https://cdn.jsdelivr.net/npm/marked/marked.min.js&quot;&gt;&lt;/script&gt; . . . // 在这里编写AJAX代码 &lt;script&gt; function showMarkdown()&#123; // 获取表单变量 var f=form; // 创建XMLHttpRequest对象 var xmlhttp; if (window.XMLHttpRequest) &#123; // IE7+, Firefox, Chrome, Opera, Safari 浏览器执行代码 xmlhttp=new XMLHttpRequest(); &#125; else &#123; // IE6, IE5 浏览器执行代码 xmlhttp=new ActiveXObject(&quot;Microsoft.XMLHTTP&quot;); &#125; // 得到服务器响应后，对得到的Markdown文档进行解析 xmlhttp.onreadystatechange=function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; // 这里调用了marked框架中的marked函数实现Markdown2HTML的解析 document.getElementById(&quot;content&quot;).innerHTML=marked(xmlhttp.responseText); &#125; &#125; // 向服务器发送请求，获取你需要的Markdown文档 xmlhttp.open(&quot;GET&quot;,f.q.value,true); xmlhttp.send(); &#125;&#125; &lt;/script&gt; . . .&lt;/head&gt;&lt;body&gt; . . . // 在这里编辑表单代码 &lt;div class=&quot;container&quot; style=&quot;margin-top:30px&quot;&gt; &lt;form name=&quot;form&quot; action=&quot;&quot; method=&quot;post&quot;&gt; &lt;select name=&quot;q&quot;&gt; &lt;option class=&quot;active&quot; alue=&quot;&quot;&gt;选择一个笔记:&lt;/option&gt; &lt;option value=&quot;A.md&quot;&gt;第一个Markdown文档&lt;/option&gt; &lt;option value=&quot;B.md&quot;&gt;第二个Markdown文档&lt;/option&gt; &lt;option value=&quot;C.md&quot;&gt;第三个Markdown文档&lt;/option&gt; &lt;option value=&quot;Evalue-TranscriptomeData.md&quot;&gt;第四个Markdown文档&lt;/option&gt; &lt;/select&gt; &lt;input type=&quot;button&quot; value=&quot;显示&quot; onclick=&quot;showMarkdown()&quot;&gt; &lt;/form&gt; . . . // 在这里编辑空的div节点，用于后面展示解析后的内容 &lt;div id=&quot;content&quot;&gt;&lt;/div&gt; &lt;/div&gt; . . .&lt;/body&gt;&lt;/html&gt; 参考资料： (1) 本人github笔记：AJAX学习笔记 (2) marked说明文档]]></content>
      <categories>
        <category>前后端技术</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
        <tag>AJAX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Perl、Shell和Python中传参与输出帮助文档]]></title>
    <url>%2F2019%2F02%2F09%2FHelpdoc-for-different-languages%2F</url>
    <content type="text"><![CDATA[基于本人对多种编程语言的粗浅了解，不论是哪种编程语言它的参数传递方式主要分为下面两类： 直接传递（以Perl为例进行说明） 在调用脚本时，直接传递参数，如：./script.pl a b c 然后在脚本中用@ARGV变量获取这些参数 getopt方法 这种方法是大多数专业程序中都会用到的方法，使用的时候在参数名前加连接符，后面接上参数的实际赋值，例如：-a 1 不过getopt方法的参数解析方式略显复杂，下面会在具体的语言中进行逐一说明 直接传递的传参方式的优点是编写和使用起来很方便，但缺点很明显，参数的顺序是固定的，不能随意改变，每回使用时都需要确定各个参数分别是什么，而且一般采用这种传参方式的人是不会编写帮助文档的，所以一旦忘了只能查看源代码 getopt方法的优点是，传参方式灵活，而且采用这种传参方式的程序员一般都会在程序中添加帮助文档，因此这种传参方式对用户是非常友好的，但是对于程序员来说，则意味着他或她不得不多写好几行代码——所以一个好的程序员头顶凉凉是可以理解的~ 以下我们只介绍第二种传参方法 PerlPerl中getopt传参Perl中的这个功能需要通过调用Getopt::Long模块实现 1use Getopt::Long; 然后使用GetOptions函数承接传递的参数： 1234567my ($var1,$var2,$var3,$var4); # 若使用&quot;use strict&quot;模式，则需要提前定义变量GetOptions( &quot;i:s&quot;=&gt;\$var1, &quot;o:s&quot;=&gt;\$var2, &quot;n:i&quot;=&gt;\$var3, &quot;m:i&quot;=&gt;\$var4 ); 这样，你就可以通过以下的方式进行灵活的Perl脚本参数传递了： 1$ perl perlscript.pl -i var1 -o var2 ... Perl中输出帮助文档可以使用POD文档实现在Perl中输出帮助文档，想了解更多关于POD文档的知识，请点 这里 12345678910111213=head1 part1 doc in part1=head2 part2 doc in part2...=cut # pod文档结束的标志 注意：每个=标签上下必须隔一行，否则就会错误解析。 用pod2doc $0可以将程序中的文档打印出来，不过一般用在程序内部，当程序参数设定错误时打印pod文档： 1die `pod2doc $0` if (...); 实现实例123456789101112131415161718192021222324252627282930313233343536#!/usr/bin/perluse strict;use warnings;use Getopt::Long;use POSIX;# 帮助文档=head1 Description This script is used to split fasta file, which is too large with thosands of sequence=head1 Usage $0 -i &lt;input&gt; -o &lt;output_dir&gt; [-n &lt;seq_num_per_file&gt;] [-m &lt;output_file_num&gt;] =head1 Parameters -i [str] Input raw fasta file -o [str] Output file to which directory -n [int] Sequence number per file, alternate chose paramerter &quot;-n&quot; or &quot;-m&quot;, if set &quot;-n&quot; and &quot;-m&quot; at the same time, only take &quot;-n&quot; parameter -m [int] Output file number (default:100)=cutmy ($input,$output_dir,$seq_num,$file_num);GetOptions( &quot;i:s&quot;=&gt;\$input, &quot;o:s&quot;=&gt;\$output_dir, &quot;n:i&quot;=&gt;\$seq_num, &quot;m:i&quot;=&gt;\$file_num );die `pod2text $0` if ((!$input) or (!$output_dir));... ShellShell中的getopt传参Shell中的这个功能可以通过getopts函数实现 1getopts [option[:]] [DESCPRITION] VARIABLE option：表示为某个脚本可以使用的选项 &quot;:&quot;：如果某个选项（option）后面出现了冒号（”:”），则表示这个选项后面可以接参数（即一段描述信息DESCPRITION） VARIABLE：表示将某个选项保存在变量VARIABLE中 1234567891011121314151617while getopts &quot;:a:b:c:&quot; optdo case $opt in a) echo &quot;参数a的值$OPTARG&quot; ;; b) echo &quot;参数b的值$OPTARG&quot; ;; c) echo &quot;参数c的值$OPTARG&quot; ;; ?) echo &quot;未知参数&quot; exit 1;; esacdone Shell中输出帮助文档在Shell中编辑一个helpdoc( )的函数即可实现输出帮助文档 12345678910111213141516171819helpdoc()&#123; cat &lt;&lt;EOFDescription: . . .Usage: $0 -a &lt;argv1&gt; -b &lt;argv2&gt; -c &lt;argv3&gt; ...Option: . . .EOF&#125; 将你想要打印出来的帮助信息写在cat &lt;&lt;EOF和EOF之间 之所以使用EOF来编写是因为，只是一种所见即所得的文本编辑形式（这个不好解释，请自行百度） 当你要打印帮助文档时，直接调用执行helpdoc( )函数即可 123456# 当没有指定参数时，即参数个数为0时，输出帮助文档并退出程序执行if [ $# = 0 ]then helpdoc() exit 1fi 实现实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687helpdoc()&#123; cat &lt;&lt;EOFDescription: This shellscript is used to run the pipeline to call snp using GATK4 - Data merge: merge multi-BAM files coresponding to specified strain - Data pre-processing: Mark Duplicates + Base (Quality Score) Recalibration - Call variants per-sample - Filter Variants: hard-filteringUsage: $0 -S &lt;strain name&gt; -R &lt;bwa index&gt; -k &lt;known-site&gt; -i &lt;intervals list&gt;Option: -S strain name, if exist character &quot;/&quot;, place &quot;/&quot; with &quot;_&quot; (Required) -R the path of bwa index (Required) -k known-sites variants VCF file -i intervals list file,must be sorted (Required)EOF&#125;workdir=&quot;/work_dir&quot;# 若无指定任何参数则输出帮助文档if [ $# = 0 ]then helpdoc exit 1fiwhile getopts &quot;hS:k:R:i:&quot; optdo case $opt in h) helpdoc exit 0 ;; S) strain=$OPTARG # 检测输入的strain名是否合格：是否含有非法字符&quot;/&quot; if [[ $strain =~ &quot;/&quot; ]] then echo &quot;Error in specifing strain name, if exist character \&quot;/\&quot;, place \&quot;/\&quot; with \&quot;_\&quot;&quot; helpdoc exit 1 fi if [ ! -d $workdir/SAM/$strain ] then echo &quot;There is no such folder coresponding to $strain&quot; helpdoc exit 1 fi ;; R) index=$OPTARG ;; k) vcf=$OPTARG if [ ! -f $vcf ] then echo &quot;No such file: $vcf&quot; helpdoc exit 1 fi ;; i) intervals=$OPTARG if [ ! -f $bed ] then echo &quot;No such file: $intervals&quot; helpdoc exit 1 fi ;; ?) echo &quot;Unknown option: $opt&quot; helpdoc exit 1 ;; esacdone... PythonPython中的getopt传参Python中的这种功能需要通过getopt模块实现 1import getopt Python脚本获得成对的参数名和参数值后，会分别把它们保存在一个字典变量中，参数名为key，参数值为value 1opts,args = getopt.getopt(argv,&quot;hi:o:t:n:&quot;,[&quot;ifile=&quot;,&quot;ofile=&quot;,&quot;time=&quot;]) getopt函数的使用说明： argv：使用argv过滤掉第一个参数（它是执行脚本的名字，不应算作参数的一部分） &quot;hi:o:t:n:&quot;：使用短格式分析串，当一个选项只是表示开关状态时，即后面不带附加参数时，在分析串中写入选项字符。当选项后面是带一个附加参数时，在分析串中写入选项字符同时后面加一个”:” 号 [&quot;ifile=&quot;,&quot;ofile=&quot;,&quot;time=&quot;]： 使用长格式分析串列表，长格式串也可以有开关状态，即后面不跟”=” 号。如果跟一个等号则表示后面还应有一个参数 然后通过条件判断的方法对参数进行解析： 1234567891011for opt,arg in opts: if opt in (&quot;-h&quot;,&quot;--help&quot;): print(helpdoc) sys.exit() elif opt in (&quot;-i&quot;,&quot;--ifile&quot;): infile = arg elif opt in (&quot;-t&quot;,&quot;--time&quot;): sleep_time = int(arg) . . . Python中的argparse传参该示例例来自黄树嘉大佬的github项目 cmdbtools 123456789101112131415161718192021222324252627282930313233343536import argparse //导入命令行解析的库文件// 为了别人执行代码的时候用--help看出来怎么使用这些代码argparser = argparse.ArgumentParser(description=&apos;Manage authentication for CMDB API and do querying from command line.&apos;) // 添加子命令commands = argparser.add_subparsers(dest=&apos;command&apos;, title=&apos;Commands&apos;)// 分别定义各个子命令，并未各个子命令设置参数// 1. 定义子命令loginlogin_command = commands.add_parser(&apos;login&apos;, help=&apos;Authorize access to CMDB API.&apos;)login_command.add_argument(&apos;-k&apos;, &apos;--token&apos;, type=str, required=True, dest=&apos;token&apos;,help=&apos;CMDB API access key(Token).&apos;) // 给子命令添加&apos;-k&apos;参数// 2. 定义子命令logoutlogout_command = commands.add_parser(&apos;logout&apos;, help=&apos;Logout CMDB.&apos;)// 3. 定义子命令tokentoken_command = commands.add_parser(&apos;print-access-token&apos;, help=&apos;Display access token for CMDB API.&apos;)// 4. 定义子命令annotateannotate_command = commands.add_parser(&apos;annotate&apos;, help=&apos;Annotate input VCF.&apos;, description=&apos;Input VCF file. Multi-allelic variant records in input VCF must be split into multiple bi-allelic variant records.&apos;)annotate_command.add_argument(&apos;-i&apos;, &apos;--vcffile&apos;, metavar=&apos;VCF_FILE&apos;, type=str, required=True, dest=&apos;in_vcffile&apos;,help=&apos;input VCF file.&apos;)// 5. 定义子命令query_variantquery_variant_command = commands.add_parser(&apos;query-variant&apos;, help=&apos;Query variant by variant identifier or by chromosome name and chromosomal position.&apos;, description=&apos;Query variant by identifier chromosome name and chromosomal position.&apos;)query_variant_command.add_argument(&apos;-c&apos;, &apos;--chromosome&apos;, metavar=&apos;name&apos;, type=str, dest=&apos;chromosome&apos;,help=&apos;Chromosome name.&apos;, default=None)query_variant_command.add_argument(&apos;-p&apos;, &apos;--position&apos;, metavar=&apos;genome-position&apos;, type=int, dest=&apos;position&apos;,help=&apos;Genome position.&apos;, default=None)query_variant_command.add_argument(&apos;-l&apos;, &apos;--positions&apos;, metavar=&apos;File-contain-a-list-of-genome-positions&apos;, type=str, dest=&apos;positions&apos;, help=&apos;Genome positions list in a file. One for each line. You can input single &apos; &apos;position by -c and -p or using -l for multiple poisitions in a single file, &apos; &apos;could be .gz file&apos;, default=None) 输出的主命令的帮助文档如下： 1234567891011121314151617$ cmdbtools --helpusage: cmdbtools [-h] &#123;login,logout,print-access-token,annotate,query-variant&#125; ...Manage authentication for CMDB API and do querying from command line.optional arguments: -h, --help show this help message and exitCommands: &#123;login,logout,print-access-token,annotate,query-variant&#125; login Authorize access to CMDB API. logout Logout CMDB. print-access-token Display access token for CMDB API. annotate Annotate input VCF. query-variant Query variant by variant identifier or by chromosome name and chromosomal position. Python中输出帮助文档在Python中创建一个字符串变量helpdoc即可实现输出帮助文档 123456789101112131415161718192021helpdoc = &apos;&apos;&apos;Description ...Usage python pyscript.py -i/--ifile &lt;input file&gt; -o/--ofile &lt;output file&gt; -t/--time &lt;int&gt; ...Parameters -h/--help Print helpdoc -i/--ifile Input file, including only one column with sampleId -o/--ofile Output file, including two columns, the 1st column is sampleId, the 2nd column is attribute information -t/--time Time for interval (seconds, default 5s) ...&apos;&apos;&apos; 在需要时将这个变量打印出来即可： 12345678try: opts,args = getopt.getopt(argv,&quot;hi:o:t:n:&quot;,[&quot;ifile=&quot;,&quot;ofile=&quot;,&quot;time=&quot;]) if len(opts) == 0: print(&quot;Options Error!\n\n&quot;+helpdoc) sys.exit(2)except getopt.GetoptError: print(&quot;Options Error!\n\n&quot;+helpdoc) sys.exit(2) 实现实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import getopt...if __name__ == &apos;__main__&apos;: ... helpdoc = &apos;&apos;&apos;Description This script is used to grab SRA sample attributes information based on SampleIdUsage python webspider_ncbiBiosample.py -i/--ifile &lt;input file&gt; -o/--ofile &lt;output file&gt; -t/--time &lt;int&gt; -n/--requests-number &lt;int&gt;Parameters -h/--help Print helpdoc -i/--ifile Input file, including only one column with sampleId -o/--ofile Output file, including two columns, the 1st column is sampleId, the 2nd column is attribute information -t/--time Time for interval (seconds, default 5s) -n/--requests-number Setting the requests number between interval (default 10)&apos;&apos;&apos; # 获取命令行参数 try: opts,args = getopt.getopt(argv,&quot;hi:o:t:n:&quot;,[&quot;ifile=&quot;,&quot;ofile=&quot;,&quot;time=&quot;]) if len(opts) == 0: print(&quot;Options Error!\n\n&quot;+helpdoc) sys.exit(2) except getopt.GetoptError: print(&quot;Options Error!\n\n&quot;+helpdoc) sys.exit(2) # 设置参数 for opt,arg in opts: if opt in (&quot;-h&quot;,&quot;--help&quot;): print(helpdoc) sys.exit() elif opt in (&quot;-i&quot;,&quot;--ifile&quot;): infile = arg elif opt in (&quot;-o&quot;,&quot;--ofile&quot;): outfile = arg # 若指定的输出文件已经存在，让用户决定覆盖该文件，还是直接退出程序 if os.path.exists(outfile): keyin = input(&quot;The output file you specified exists, rewrite it?([y]/n: &quot;) if keyin in (&quot;y&quot;,&quot;Y&quot;,&quot;&quot;): os.remove(outfile) elif keyin in (&quot;n&quot;,&quot;N&quot;): print(&quot;The output file existed!\n&quot;) sys.exit(2) else: print(&quot;Input error!\n&quot;) sys.exit(2) elif opt in (&quot;-t&quot;,&quot;--time&quot;): sleep_time = int(arg) elif opt in (&quot;-n&quot;,&quot;--requests-number&quot;): requestNum = int(arg) . . . 参考资料： (1) 本人github笔记：Perl进阶笔记 (2) 本人github笔记：实用小脚本 (3) 本人github笔记：Linux (Raspbian) 操作进阶——Shell编程 (4) Python 命令行参数和getopt模块详解]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Perl</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多套RNA-seq分析流程]]></title>
    <url>%2F2019%2F02%2F08%2FRNA-seq%2F</url>
    <content type="text"><![CDATA[1. 测序数据下载参见： https://github.com/Ming-Lian/Memo/blob/master/ChIP-seq-pipeline.md#get-data 2. 比对与定量2.1. Salmon流程不需要比对，直接对转录水平进行定量 2.1.1. 创建索引1$ salmon index -t Arabidopsis_thaliana.TAIR10.28.cdna.all.fa.gz -i athal_index_salmon 2.1.2. 定量salmon quant 有两种运行模式： Salmon’s quasi-mapping-based mode： using raw reads 输入以下命令查看该模式下的help文档 12&gt; salmon quant --help-reads&gt; Salmon’s alignment-based mode： makes use of already-aligned reads (in BAM/SAM format) 当使用参数-a时，启用该模式，否则默认使用quasi-mapping-based mode 输入以下命令查看该模式下的help文档 12&gt; salmon quant --help-alignment&gt; 12345678910111213#! /bin/bashindex=~/rna_seq_practice_2/data/ref/athal_index_salmon ## 指定索引文件夹quant=~/rna_seq_practice_2/work/quant_salmon # 指定定量文件输出文件夹for fn in ERR1698&#123;194..209&#125;;do sample=`basename $&#123;fn&#125;` # basename命令用于去掉路径信息，返回纯粹的文件名，如果指定的文件的扩展名，则将扩展名也一并去掉。 echo &quot;Processin sample $&#123;sampe&#125;&quot; salmon quant -i $index -l A \ -1 $&#123;sample&#125;_1.fastq.gz \ -2 $&#123;sample&#125;_2.fastq.gz \ -p 5 -o $quant/$&#123;sample&#125;_quantdone# quant_salmon.sh salmon quant 参数： -i Salmon index -l Format string describing the library type. -l A tells salmon that it should automatically determine the library type of the sequencing reads (e.g. stranded vs. unstranded etc.) -1 File containing the #1 mates -2 File containing the #2 mates -p The number of threads to use -o Output quantification file. 2.2. subread流程2.2.1. 创建索引12$ gunzip Arabidopsis_thaliana.TAIR10.28.dna.genome.fa.gz$ subread-buildindex -o athal_index_subread Arabidopsis_thaliana.TAIR10.28.dna.genome.fa 2.2.2. 比对1234567891011121314#! /bin/bashindex=~/rna_seq_practice_2/data/ref/athal_index_subreadmap=~/rna_seq_practice_2/work/mapfor fn in ERR1698&#123;194..209&#125;;do sample=`basename $&#123;fn&#125;` echo &quot;Processin sample $&#123;sampe&#125;&quot; subjunc -i $index \ -r $&#123;sample&#125;_1.fastq.gz \ -R $&#123;sample&#125;_2.fastq.gz \ -T 5 -o $map/$&#123;sample&#125;_subjunc.bam# 比对的sam自动转为bam，但是并不按照参考基因组坐标排序done# map_subjunc.sh 2.2.3. 定量123456featureCounts=~/anaconda2/bin# gff3=~/rna_seq_practice_2/data/ref/Arabidopsis_thaliana.TAIR10.28.gff3.gzgtf=~/rna_seq_practice_2/data/ref/Arabidopsis_thaliana.TAIR10.28.gtfcount=~/rna_seq_practice_2/work/quant_subjuncnohup $featureCounts/featureCounts -T 5 -p -t exon -g gene_name -a $gtf -o $count/counts.txt *.bam &amp;nohup $featureCounts/featureCounts -T 5 -p -t exon -g gene_id -a $gtf -o $count/counts_id.txt *.bam &amp; 2.3. hisat2-stringtie流程2.3.1. hisat2创建索引1234567# build reference index## using the python scripts included in the HISAT2 package, extract splice-site and exon information from the geneannotation fle$ extract_splice_sites.py chrX_data/genes/chrX.gtf &gt;chrX.ss$ extract_exons.py chrX_data/genes/chrX.gtf &gt;chrX.exon## build a HISAT2 index$ hisat2-build --ss chrX.ss --exon chrX.exon chrX_data/genome/chrX.fa chrX_tran 2.3.2. hisat2比对1$ hisat2 -p 10 --dta -x chrX_tran -1 reads1_1.fastq -2 reads1_2.fastq | samtools sort -@ 8 -O bam -o reads1.sort.bam 1&gt;map.log 2&gt;&amp;1 Usage: hisat2 [options]* -x &lt;ht2-idx&gt; {-1 &lt;m1&gt; -2 &lt;m2&gt; | -U &lt;r&gt;} [-S &lt;sam&gt;] -p Number of threads to use –dta reports alignments tailored for transcript assemblers -x Hisat2 index -1 The 1st input fastq file of paired-end reads -2 The 2nd input fastq file of paired-end reads -S File for SAM output (default: stdout) 2.3.3. stringtie转录本拼接123$ stringtie -p 16 -G Ref/hg19/grch37_tran/Homo_sapiens.GRCh37.75.gtf -o Asm/read1.gtf -l prefix Map/read1.bam 1&gt;Asm/read1_strg_assm.log 2&gt;&amp;1# 样本间转录本合并$ stringtie --merge -p 16 -G Ref/hg19/grch37_tran/Homo_sapiens.GRCh37.75.gtf -o Asm/merge.gtf Asm/mergelist.txt 1&gt;Asm/strg_merge.log 2&gt;&amp;1 Transcript merge usage mode: stringtie --merge [Options] { gtf_list | strg1.gtf ...} -p number of threads (CPUs) to use -G reference annotation to include in the merging (GTF/GFF3) -o output path/file name for the assembled transcripts GTF -l name prefix for output transcripts (default: STRG) 2.3.4. stringtie定量 以ballgown格式输出 12$ stringtie -e -B -p 16 -G Asm/merge.gtf -o quant/read1/read1.gtf \ Map/read1.bam 1&gt;quant/read1/read1_strg_quant.log 2&gt;&amp;1 -e only estimate the abundance of given reference transcripts (requires -G) -B enable output of Ballgown table files which will be created in the same directory as the output GTF (requires -G, -o recommended) 以read count进行定量，作为DESeq2或edgeR的输入 123$ stringtie -e -p 16 -G Asm/merge.gtf -o quant/read1/read1.gtf \ Map/read1.bam 1&gt;quant/read1/read1_strg_quant.log 2&gt;&amp;1$ python prepDE.py -i sample_lst.txt 注意： - stringtie的用法与上面相同，除了少了一个参数`-B` - `prepDE.py`脚本需要到stringtie官网下载：http://ccb.jhu.edu/software/stringtie/dl/prepDE.py ，注意该脚本是用**python2**编写的 - `prepDE.py` 会以csv格式，分别输出基因和转录本的count matrices prepDE.py参数 &gt; - -i the parent directory of the sample sub-directories or a textfile listing the paths to GTF files [default: ballgown] &gt; - -g where to output the gene count matrix [default: gene_count_matrix.csv] &gt; - -t where to output the transcript count matrix [default: transcript_count_matrix.csv] samplelist textfile 格式如下： 12345678910ERR188021 &lt;PATH_TO_ERR188021.gtf&gt;ERR188023 &lt;PATH_TO_ERR188023.gtf&gt;ERR188024 &lt;PATH_TO_ERR188024.gtf&gt;ERR188025 &lt;PATH_TO_ERR188025.gtf&gt;ERR188027 &lt;PATH_TO_ERR188027.gtf&gt;ERR188028 &lt;PATH_TO_ERR188028.gtf&gt;ERR188030 &lt;PATH_TO_ERR188030.gtf&gt;ERR188033 &lt;PATH_TO_ERR188033.gtf&gt;ERR188034 &lt;PATH_TO_ERR188034.gtf&gt;ERR188037 &lt;PATH_TO_ERR188037.gtf&gt; 2.4. RSEM流程RSEM属于Alignment-based transcript quantification的转录本定量工具的一种，也就是先比对后定量 RSEM最早被广泛应用于无参转录组的定量分析，因为无参转录组需要对reads进行拼接，然后将reads比对至拼接的转录本上，再通过定量获得其表达丰度 RSEM是在2010年发表的，最新更新是在2016年，下载地址则 http://deweylab.github.io/RSEM/，下载后编译下即可 12345$ make$ make install # 默认安装在/usr/local/bin目录下# 自定义安装路径# 若要修改安装路径需要设置DESRDIR和prefix参数，默认情况下DESRDIR=&apos;&apos;，prefix=/usr/local，最终的安装目录为 $DESTDIR/$prefix/binmake install DESRDIR=/Path/To/Destdir prefix= RSEM整体上来说是属于定量软件，但其支持调用其他比对软件，如Bowtie，Bowtie2和STAR，来将reads比对至转录本上，所以必须至少预先安装上述3款比对软件中的一种，且要将它们的安装路径加入到环境变量PATH中 2.4.1. 创建索引这步可以理解为是对转录本建索引，RSEM支持两种方式： 提供参考基因组fa序列和GTF注释文件 RSEM则会通过GTF注释文件从参考基因组序列中提取出各个转录本的序列，然后利用Bowtie2 or STAR等软件来建索引 12345&gt; $ ~/biosoft/rsem/RSEM-1.3.0/rsem-prepare-reference \&gt; -gtf ~/annotation/hg38/gencode.v27.annotation.gtf \&gt; --bowtie2 \&gt; ~/reference/genome/hg38/GRCh38.p10.genome.fa ~/reference/index/RSEM/hg38/hg38&gt; 直接提供转录本序列，然后再建索引 无参转录组一般是这样的 参数与上面的情况相似，只是不需要设置-gtf参数，若用户不指定-gtf参数，则RSEM会将后面提供的参考序列作为转录本 如果还想有基于基因水平的定量结果，则需再加–transcript-to-gene-map参数，用于导入转录本和基因的对应关系的文件（一列基因ID，一列对应的转录本ID） 在~/reference/index/RSEM/hg38/目录下包含有从参考基因组提取出来的转录本的序列文件以及bowtie2的索引文件（以.bt2结尾）等 2.4.2. 转录本定量用RSEM的rsem-calculate-expression命令来对reads进行bowtie2比对以及表达水平的定量 12345678$ ~/biosoft/rsem/RSEM-1.3.0/rsem-calculate-expression \ -p 6 \ --bowtie2 \ --append-names \ --output-genome-bam \ --paired-end sample_R1.fastq sample_R2.fastq \ ~/reference/index/RSEM/hg38/ \ RSEM/sample 该命令有以下三种命令行形式： 123456# 1. Single-end，比对与定量rsem-calculate-expression [options] upstream_read_file(s) reference_name sample_name# 2. Paired-end，比对与定量rsem-calculate-expression [options] --paired-end upstream_read_file(s) downstream_read_file(s) reference_name sample_name# 3. 用户提供SAM/BAM/CRAM格式的比对文件，只进行定量rsem-calculate-expression [options] --alignments [--paired-end] input reference_name sample_name 参数说明： –paired-end 表明是双端测序数据 –bowtie2 指定bowtie2来用于reads比对 –append-names 用于在结果中附加上gene name和transcript name –output-genome-bam 结果中输出基于基因水平的bam文件（默认只有转录水平的bam文件） –estimate-rspd 文档中的例子还加了这个参数，用于estimate the read start position distribution，看是否有positional biases RSEM通过调用bowtie2来完成mapping，其调用bowtie2时使用的默认参数为： 12345678910111213141516$ bowtie2 \ -q \ --phred33 \ --sensitive \ --dpad 0 \ --gbar 99999999 \ # 不支持有gap的比对 --mp 1,1 \ --np 1 \ --score-min L,0,-0.1 \ -I 1 -X 1000 --no-mixed \ --no-discordant \ # 不支持paired reads discordant的比对 -p 6 \ -k 200 \ # 输出最佳的200个比对结果 -x /home/anlan/reference/index/RSEM/hg38/ \ -1 SRR6269049.clean_1.fastq \ -2 SRR6269049.clean_2.fastq | samtools view -S -b -o RSEM/SRR6269049.temp/SRR6269049.bam 也可以自己用bowtie2进行mapping将mapping得到的文件作为RSEM定量的输入，但是必须保证： 不支持有gap的比对 不支持paired reads discordant的比对 输出最佳的200个比对结果 否则RSEM会报错！ RSEM对multiple mapping结果的处理是这款工具的亮点： 对于RSEM如何处理multimapping reads，RSEM官方教程特意讲了一点 以Ccl6基因的3个转录本的比对结果为例，其第2个转录本全长与第1个转录本重叠，所以在这重叠区域会产生大量的multimapping reads，黑色表示uniquely aligned reads，红色表示multi-mapping reads RSEM的处理方法是这样的：RSEM先尽量让第1个转录本在其重叠区域的reads（包括unique reads和multimapping reads）分布趋于平滑？因此再将在区域的剩下multimapping reads算在第2个转录本上；至于第3个转录本，其没有reads比对上，所以就空的；总体上如下图所示 3. 差异表达分析3.1. 使用DESeq2进行差异分析DESeq2要求输入的表达矩阵是read counts 构建 DESeqDataSet 对象 12345678910# 构建表达矩阵count_table&lt;-read.csv(&quot;/path/to/gene_count_matrix.csv&quot;,row.names=&quot;gene_id&quot;)count_matrix&lt;-as.matrix(count_table)# 构建分组矩阵group_list&lt;-factor(rep(c(&quot;control&quot;,&quot;treat&quot;),c(3,3)))colData &lt;- data.frame(row.names=colnames(count_matrix), group_list=group_list)# 构建 DESeqDataSet 对象dds &lt;- DESeqDataSetFromMatrix(countData = count_matrix, colData = colData, design = ~ group_list) 表达矩阵的格式为： SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 SRR1039521 ENSG00000000003 679 448 873 408 1138 1047 770 572 ENSG00000000005 0 0 0 0 0 0 0 0 ENSG00000000419 467 515 621 365 587 799 417 508 ENSG00000000457 260 211 263 164 245 331 233 229 ENSG00000000460 60 55 40 35 78 63 76 60 ENSG00000000938 0 0 2 0 1 0 0 0 ENSG00000000971 3251 3679 6177 4252 6721 11027 5176 7995 ENSG00000001036 1433 1062 1733 881 1424 1439 1359 1109 ENSG00000001084 519 380 595 493 820 714 696 704 ENSG00000001167 394 236 464 175 658 584 360 269 ENSG00000001460 172 168 264 118 241 210 155 177 ENSG00000001461 2112 1867 5137 2657 2735 2751 2467 2905 ENSG00000001497 524 488 638 357 676 806 493 475 ENSG00000001561 71 51 211 156 23 38 134 172 分组矩阵的格式为： dex SampleName cell SRR1039508 untrt GSM1275862 N61311 SRR1039509 trt GSM1275863 N61311 SRR1039512 untrt GSM1275866 N052611 SRR1039513 trt GSM1275867 N052611 SRR1039516 untrt GSM1275870 N080611 SRR1039517 trt GSM1275871 N080611 SRR1039520 untrt GSM1275874 N061011 SRR1039521 trt GSM1275875 N061011 差异表达分析 1234567891011## 数据过滤# dds &lt;- dds[ rowSums(counts(dds)) &gt; 1 ,]dds &lt;- DESeq(dds)# plotDispEsts(dds, main=&quot;Dispersion plot&quot;)# rld &lt;- rlogTransformation(dds)# xprMatrix_rlog=assay(rld)## 若有多组，可以用参数contrast提取指定两组的比较结果res &lt;- results(dds, contrast=c(&quot;group_list&quot;,&apos;control&apos;,&apos;treat&apos;))## 将比较结果依据p-adjust进行排序resOrdered &lt;- res[order(res$padj),] res_Day1_Day0=as.data.frame(resOrdered) 3.2. 使用Ballgown进行差异分析紧接着stringtie的定量结果进行分析 stringtie的定量结果提供多种表达值的表示方法，有read counts, RPKM/FPKM, TPM 。其中read counts是原始reads计算，RPKM/FPKM 和 TPM 都是基因表达值的归一化后的，因为本身的某些缺点，主流科学家强烈要求它就被TPM取代了。 下面对 RPKM/FPKM 和 TPM 的说明摘抄自健明大大的简书：https://www.jianshu.com/p/e9d5d7206124 ，说得通俗易懂 TPM是什么？ 我不喜欢看公式，直接说事情，我有一个基因A，它在这个样本的转录组数据中被测序而且mapping到基因组了 5000个的reads，而这个基因A长度是10K，我们总测序文库是50M，所以这个基因A的RPKM值是 5000除以10，再除以50，为10. 就是把基因的reads数量根据基因长度和样本测序文库来normalization 。 那么它的TPM值是多少呢？ 这个时候这些信息已经不够了，需要知道该样本其它基因的RPKM值是多少，加上该样本有3个基因，另外两个基因的RPKM值是5和35，那么我们的基因A的RPKM值为10需要换算成TPM值就是 1,000,000 *10/(5+10+35)=200,000， 看起来是不是有点大呀！其实主要是因为我们假设的基因太少了，一般个体里面都有两万多个基因的，总和会大大的增加，这样TPM值跟RPKM值差别不会这么恐怖的。 载入R包 1234require(ballgown)require(dplyr)require(genefilter)setwd(&quot;/share/disk5/lianm&quot;) 载入stringtie输出的表达数据，并设置表型信息（即分组信息） 12bg_Z1Z4&lt;-ballgown(dataDir=&quot;SingleCell_process/Cleandata/Expression&quot;,samplePattern=“Z[14]T&quot;,meas=&quot;FPKM&quot;)pData(bg_Z1Z4)&lt;-data.frame(id=sampleNames(bg_Z1Z4),group=c(rep(1,num_group1),rep(0,num_group2))) 过滤低丰度的基因 1bg_Z1Z4_filt&lt;-subset(bg_Z1Z4,&quot;rowVars(texpr(bg_Z1Z4))&gt;1&quot;,genomesubset=T) 差异表达基因分析 1234result_genes&lt;-stattest(bg_Z1Z4_filt,feature=&quot;gene&quot;,covariate=&quot;group&quot;,getFC=T)result_genes&lt;-data.frame(geneNames=geneNames(bg_Z1Z4_filt)[match(result_genes$id,geneIDs(bg_Z1Z4_filt))],geneIDs=geneIDs(bg_Z1Z4_filt)[match(result_genes$id,geneIDs(bg_Z1Z4_filt))],result_genes)result_genes_sort&lt;-arrange(result_genes,pval)write.csv(result_genes_sort,file=paste(&quot;SingleCell_process/Cleandata/Expression/&quot;,name_group1,&quot;_VS_&quot;,name_group2,&quot;_geneDiff_results.csv&quot;,sep=&quot;&quot;),row.names=F) 分组设置对差异表达分析的影响： FC = group_1/group_0，所以分组标签互换后FC会变为原来的倒数 差异转录本分析 1234result_trans&lt;-stattest(bg_Z1Z4_filt,feature=&quot;transcript&quot;,covariate=&quot;group&quot;,getFC=T)result_trans&lt;-data.frame(geneNames=geneNames(bg_Z1Z4_filt),geneIDs=geneIDs(bg_Z1Z4_filt),result_trans)result_trans_sort&lt;-arrange(result_transs,pval)write.csv(result_trans_sort,file=paste(&quot;SingleCell_process/Cleandata/Expression/&quot;,name_group1,&quot;_VS_&quot;,name_group2,&quot;_transDiff_results.csv&quot;,sep=&quot;&quot;),row.names=F) 4. 几点思考 为什么要进行转录本拼接？不是都有参考转录组了吗？ 无参考基因组：对于无参考转录组的物种来说，往往需要通过RNA-Seq的数据自己拼出参考转录组，然后再进行下游的数据分析。所以，对于无参分析来说，往往需要自己拼装转录本，生成自己的参考转录组信息 注释完备的基因组：例如human，不同的细胞条件可能不同，一些永生化的细胞系往往都具有“癌症”的特征，这些细胞的转录组，基因组或多或少都有结构的变异，以及转录本的差异。 参考资料： (1) 【PANDA姐的转录组入门】拟南芥实战项目-定量、差异表达、功能分析 (2) 一步法差异分析 (3) Pertea M, Kim D, Pertea G M, et al. Transcript-level expression analysis of RNA-seq experiments with HISAT, StringTie and Ballgown.[J]. Nature Protocols, 2016, 11(9):1650. (4) Alignment-based的转录本定量-RSEM (5) RSEM官方教程 (6) 知乎《高通量测序技术》专栏：生物信息学100个基础问题 —— 第25题 GTF/GFF的注释是怎么来的，应该从哪里下载？]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>Bioinformatics</tag>
        <tag>RNAseq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RNA-seq中的那些统计学问题]]></title>
    <url>%2F2019%2F02%2F08%2FStat-on-RNAseq%2F</url>
    <content type="text"><![CDATA[数据预处理过滤低表达的基因 所有数据集将包括表达的基因和不表达的基因的组合。 虽然检查在一种条件下表达但不在另一种条件下表达的基因是有意义的，但是一些基因在所有样品中都未表达 标准化 由于不同文库测序深度不同，比较前当然要进行均一化！用总reads进行均一化可能最简单，其基于以下两个基本假设： 绝大多数的gene表达量不变； 高表达量的gene表达量不发生改变； 但在转录组中，通常一小部分极高丰度基因往往会贡献很多reads，如果这些“位高权重”的基因还是差异表达的，则会影响所有其它基因分配到的reads数，而且，两个样本总mRNA量完全相同的前提假设也过于理想了。那如何比较呢，各个方家使出浑身解数，有用中位数的，有用75分位数的，有用几何平均数的，有用TMM(trimmed mean of Mvalues)的等等，总之要找一个更稳定的参考值。 House-keeping gene(s)矫正的思路很简单，就是在变化的样本中寻找不变的量 那么在不同RNA-seq样本中，那些是不变的量呢？一个很容易想到的就是管家基因 (House-keeping gene(s)) 那么 Human 常用的 House-keeping gene 怎么确定？ 目前大家用的比较多的一个human housekeeping gene list 来源于下面这篇文章，是2013年发表在 Cell系列的 Trends in Genetics 部分的一篇文章 spike-in使用Housekeeping gene的办法来进行相对定量，这种办法在一定程度上能够解决我们遇到的问题。但其实这种办法有一个非常强的先验假设：housekeeping gene的表达量不怎么发生变化。其实housekeeping gene list有几千个，这几千个基因有一定程度上的变化是有可能的 spike-in方法：在RNA-Seq建库的过程中掺入一些预先知道序列信息以及序列绝对数量的内参。这样在进行RNA-Seq测序的时候就可以通过不同样本之间内参（spike-in）的量来做一条标准曲线，就可以非常准确地对不同样本之间的表达量进行矫正 比较常用的spike-in类型：ERCC Control RNA ERCC = External RNA Controls Consortium ERCC就是一个专门为了定制一套spike-in RNA而成立的组织，这个组织早在2003年的时候就已经宣告成立。主要的工作就是设计了一套非常好用的spike-in RNA，方便microarray，以及RNA-Seq进行内参定量 CPMCPM(count-per-million) TCS (Total Count Scaling)简单来说，就是找出多个样本中library size为中位数的样本，作为参考样本，将所有的样本的library size按比例缩放到参考样本的水平 选择一个library size为中位数的sample，以它为baseline，计算出其它sample对于baseline的normalization factor，即一个缩放因子： 然后基于该缩放因子对特定的sample中的每个基因的read count进行标准化（缩放）： Quantile简单来说，就是排序后求平均，然后再回序 在R里面，推荐用preprocessCore 包来做quantile normalization，不需要自己造轮子啦！但是需要明白什么时候该用quantile normalization，什么时候不应该用，就复杂很多了 Median of Ratio (DESeq2)该方法基于的假设是，即使处在不同条件下的不同个样本，大多数基因的表达是不存在差异的，实际存在差异的基因只占很小的部分那么我们只需要将这些稳定的部分找出来，作为标准化的内参，依据内参算出各个样本的标准化因子 （1）对每个基因计算几何平均数，得到一个假设的参考样本(pseudo-reference sample) gene sampleA sampleB pseudo-reference sample EF2A 1489 906 sqrt(1489 * 906) = 1161.5 ABCD1 22 13 sqrt(22 * 13) = 17.7 … … … … （2）对每个样本的每个基因对于参考样本计算Fold Change gene sampleA sampleB pseudo-reference sample ratio of sampleA/ref ratio of sampleB/ref EF2A 1489 906 1161.5 1489/1161.5 = 1.28 906/1161.5 = 0.78 ABCD1 22 13 16.9 22/16.9 = 1.30 13/16.9 = 0.77 MEFV 793 410 570.2 793/570.2 = 1.39 410/570.2 = 0.72 BAG1 76 42 56.5 76/56.5 = 1.35 42/56.5 = 0.74 MOV10 521 1196 883.7 521/883.7 = 0.590 1196/883.7 = 1.35 … … … … … … （3）获取每个样本中Fold Change的中位数，我们就得到了非DE基因代表的Fold Change，该基因就是我们选择的该样本的内参基因，它的Fold Change就是该样本的标准化因子 123normalization_factor_sampleA &lt;- median(c(1.28, 1.3, 1.39, 1.35, 0.59))normalization_factor_sampleB &lt;- median(c(0.78, 0.77, 0.72, 0.74, 1.35)) TMM (Trimmed Mean of M value, edgeR)该方法的思想与DESeq2的Median of Ratio相同，假设前提都是：大多数基因的表达是不存在差异的 它与DESeq2的不同之处在于对内参的选择上： DESeq2选择一个内参基因，它的Ratio/Fold-Change就是标准化因子 edgeR选择一组内参基因集合，它们对标准化因子的计算均有贡献：加权平均 （1）移除所有未表达基因 （2）从众多样本中找出一个数据趋势较为平均的样本作为参考样本 对所有样本求总Read数； 各样本除以各自的总Read数，得到修正Read数； 求出各自样本修正Read数的Q3值（第3个四分位数）； 所有的Q3值求平均，与平均Q3相差最小的样本即是参考样本。 （3）找出每个样本中的代表基因集，参考这些代表基因集的fold change，计算出该样本的标准化因子 寻找样本的代表基因集：依据基因的偏倚程度和Reads数大小选出——偏倚程度小、reads数居中的基因 衡量偏倚度的量：LFC (log fold change) LFC过大或过小都表示具有偏倚性，LFC越大表示reads数在samplei中越高，即偏向samplei；LFC越小表示reads数在ref中越高，即偏向ref 衡量reads数的量：read的几何平均数 (read geometric mean, RGM) RGM越大表示基因reads越多，RGM越小表示基因reads越少 结合偏倚度、read数画出散点图： 偏倚度小、表达量居中的那些基因落在图中的红线附近 由参考代表基因集计算样本的标准化因子： 对这些代表基因集计算加权平均数： 该加权平均数就能代表该样本的标准化因子，只是经过了log变换，所以需要恢复为它的正值： Scaling-Factor = 2 weight-average 为什么说FPKM和RPKM都错了？FPKM和RPKM分别是什么FPKM和RPKM分别是什么 RPKM是Reads Per Kilobase per Million的缩写，它的计算方程非常简单： FPKM是Fregments Per Kilobase per Million的缩写，它的计算与RPKM极为类似，如下： 与RPKM唯一的区别为：F是fragments，R是reads，如果是PE(Pair-end)测序，每个fragments会有两个reads，FPKM只计算两个reads能比对到同一个转录本的fragments数量，而RPKM计算的是可以比对到转录本的reads数量而不管PE的两个reads是否能比对到同一个转录本上。如果是SE(single-end)测序，那么FPKM和RPKM计算的结果将是一致的。 这两个量的计算方式的目的是为了解决计算RNA-seq转录本丰度时的两个bias： 相同表达丰度的转录本，往往会由于其基因长度上的差异，导致测序获得的Read（Fregment）数不同。总的来说，越长的转录本，测得的Read（Fregment）数越多； 由测序文库的不同大小而引来的差异。即同一个转录本，其测序深度越深，通过测序获得的Read（Fregment）数就越多。 什么样才算好的统计量首先，到底什么是RNA转录本的表达丰度这个问题 对于样本X，其有一个基因g被转录了mRNA_g次，同时样本X中所有基因的转录总次数假定是mRNA_total, 那么正确描述基因g转录丰度的值应该是： 则一个样本中基因表达丰度的均值为 而 所以 这个期望值竟然和测序状态无关！仅仅由样本中基因的总数所决定的 也就是说，对于同一个物种，不管它的样本是哪种组织（正常的或病变的），也不管有多少个不同的样本，只要它们都拥有相同数量的基因，那么它们的r_mean都将是一致的 由于上面的结果是在理论情况下推导出来的，实际上我们无法直接计算这个r，那么我们可以尝试通过其他方法来近似估计r，只要这些近似统计量可以隐式地包含这一恒等关系即可 FPKM和RPKM犯的错实际数据来证明 假定有两个来自同一个个体不同组织的样本X和Y，这个个体只有5个基因，分别为A、B、C、D和E它们的长度分别如下： r_mean值是: 如果FPKM或RPKM是一个合适的统计量的话，那么，样本X和Y的平均FPKM（或RPKM）应该相等。 以下这个表格列出的分别是样本X和Y在这5个基因分别比对上的fregment数和各自总的fregment数量： 可以算出：样本X在这5个基因上的FPKM均值FPKM_mean = 5,680;样本Y在这5个基因上的FPKM均值FPKM_mean = 161,840 很明显，它们根本不同，而且差距相当大 究竟为什么会有如此之大的差异？ 可以从其公式上找到答案 首先，我们可以把FPKM的计算式拆分成如下两部分。 第一部分的统计量是对一个基因转录本数量的一个等价描述（虽然严格来讲也没那么等价）： 第二部分的统计量是测序获得的总有效Fregment数量的百万分之一： 这么一拆，就可以看出这个公式的问题了：逻辑上根本说不通嘛！ 尤其是第二部分（N/10^6），本来式子的第一部分是为了描述一个基因的转录本数量，那么正常来讲，第二部分就应该是样本的转录本总数量（或至少是其总数量的等价描述）才能形成合理的比例关系，而且可以看出来FPKM/RPMK是有此意的，这本来就是这个统计量的目的。 但是N/10^6并不能描述样本的转录本总数量。N/10^6的大小其实是由RNA-seq测序深度所决定的，并且是一个和总转录本数量无直接线性关系的统计量——N与总转录本数量之间的关系还受转录本的长度分布所决定，而这个分布往往在不同样本中是有差异的！ TPM是一个合适的选择这个统计量在2012年所发表的一篇讨论RPKM的文章（RPKM measure is inconsistent among samples. Wagner GP, Kin K, Lynch VJ. Theory Biosci. 2012.）中就被提出来了，称之为TPM —— Transcripts Per Million，它的计算是： 简单计算之后我们就可以发现TPM的均值是一个独立于样本之外的恒定值，它等于： 这个值刚好是r_mean的一百万倍，满足等价描述的关系。 统计学原理转录组数据统计推断的难题在RNA-seq中进行两组间的差异分析是最正常不过的了。 我们在其它实验中同样会遇到类似的分析，通常，我们可以用方差分析判定两组“分布”数据间是否存在显著差异。原理是：当组间方差大于组内方差（误差效应），并且统计学显著时，则认为组间处理是可以引起差异的。 有伙伴肯定要问，转录组数据到底有什么了不起的？它们为什么不能用我们熟悉的算法简单地进行计算？ 其实统计学家也很无奈啊，看看我们转录组实验得到的这些数据吧：我们的实验只进行少得可怜的生物学重复（n&lt;10），而且，任何基因的表达量都不能是负数，这些数据并不符合正态分布，用于表征表达量的counts是非连续的（芯片信号是连续的），RNA-seq数据的离散通常是高度扭曲的，方差往往会大于均值……，就这些奇怪的特征，使得准确估计方差并没有想象的那么容易。 我们面临两个核心问题： 基因表达数据适合用什么统计学分布进行差异显著性检验？ 如何利用少量生物学重复数据估算基因表达的标准差？ 泊松分布 or 负二项分布？从统计学的角度出发，进行差异分析肯定会需要假设检验，通常对于分布已知的数据，运用参数检验结果的假阳性率会更低。转录组数据中，raw count值符合什么样的分布呢？ count值本质是reads的数目，是一个非零整数，而且是离散的，其分布肯定也是离散型分布。对于转录组数据，学术界常用的分布包括泊松分布 (poisson)和负二项分布 (negative binomial)两种。 为什么泊松分布不行？首先有必要简单地介绍一下泊松分布 泊松分布适合于描述单位时间（或空间）内随机事件发生的次数（事件发生的次数只能是离散的整数）。如某一服务设施在一定时间内到达的人数，电话交换机接到呼叫的次数，汽车站台的候客人数，机器出现的故障数，自然灾害发生的次数，一块产品上的缺陷数，显微镜下单位分区内的细菌分布数等等。 泊松分布大概长这样： λ是波松分布所依赖的唯一参数。 λ值愈小分布愈偏倚， 随着λ的增大 ， 分布趋于对称。 当λ=20时分布接近于正态分布；当λ=50时， 可以认为波松分布呈正态分布。 在数据分析的早期，确实有学者采用泊松分布进行差异分析，但是发展到现在，几乎全部都是基于负二项分布了，究竟是什么因素导致了这种现象呢？为了解释这个问题，我们必须提到一个概念 overdispersion。 dispersion指的是离散程度，研究一个数据分布的离散程度，我们常用方差这个指标。对于泊松分布而言，其均值和方差是相等的，但是我们的数据确不符合这样的规律。通过计算所有基因的均值和方差，可以绘制如下的图片： 横坐标为基因在所有样本中的均值，纵坐标为基因在所有样本中的方差，直线的斜率为1，代表泊松分布的均值和方差的分布。可以看到，真实数据的分布是偏离了泊松分布的，方差明显比均值要大。 如果假定总体分布为泊松分布， 根据我们的定量数据是无法估计出一个合理的参数，能够符合上图中所示分布的，这样的现象就称之为overdispersion。 由于真实数据与泊松分布之间的overdispersion，选择泊松分布分布作为总体的分布是不合理。 以上只证明了泊松分布是个不太恰当的分布估计，那怎么证明负二项分布就是合适的分布估计呢？ 为什么负二项分布行？主要是从均值与方差之间的关系去证明 同样的，也先简单介绍一下负二项分布： 二项分布描述的是n重伯努利实验，在n重贝努利试验中，事件A恰好发生x(0≤x≤n)次的概率为： 它的概率分布图如下： 负二项分布描述的也是伯努利实验，不过它的目标事件变成了：对于Bernoulli过程，我们设定，当某个结果出现固定次数的时候，整个过程的数量，比如我们生产某个零件，假设每个零件的合格与否都是相互独立的，且分布相同，那么当我们生产出了五个不合格零件时，一共生产了多少合格的零件，这个数量就是一个负二项分布，公式如下： 该公式描述的是，在合格率为p的一堆产品中，进行连续有放回的抽样，当抽到r个次品时，停止抽样，此时抽到的正品正好为k个的概率 它的概率分布如下： 负二项分布的均值和方差分别为： 将p用μ表示，得到： 将上一步推出的p和1-p带入到方差的表达式中，得到： 记1/r=α，则 从上面的式子可以看出，均值是方差的二次函数，方差随着均值的增加而进行二次函数形式的递增，正好符合上文 2.2.1. 为什么泊松分布不行？部分均值与方差分布图的情况 其中α和r被称为dispersion parameter 负二项分布与泊松分布的关系，可以用α或r推出： 当 r -∞ 时，α -0，此时 σ2&lt;/sup= μ，为泊松分布； 当 r -&gt; 0 时，α -&gt; ∞，此时overdispersion 方差估计在生物学重复很少时，我们是很难准确计算每个基因表达的标准差的（相当于这个数据集的离散程度）。我们很可能会低估数据的离散程度。 被逼无奈的科学家提出了一个假设：表达丰度相似的基因，在总体上标准差应该也是相似的。我们把不同生物学重复中表达丰度相同的基因的总标准差取个平均值，低于这个值的都用这个值，高于这个值的就用算出来的值。 参考资料： (1) 【生信菜鸟团】quantile normalization到底对数据做了什么？ (2) Introduction to DGE (3) 生信菜鸟团：StatQuest生物统计学专题 - library normalization进阶之edgeR的标准化方法 (4) 【简书】为什么说FPKM和RPKM都错了？ (5) 【生信修炼手册】负二项分布在差异分析中的应用 (6) 【 生信百科】转录组差异表达筛选的真相 (7) 【生信媛】RNA-seq分析中的dispersion，你知道吗？ (8) H. J. Pimentel, et al. Differential analysis of RNA-Seq incorporatingquantification uncertainty. bioRxiv, 2016]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>Bioinformatics</tag>
        <tag>RNAseq</tag>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入了解snp-calling流程]]></title>
    <url>%2F2019%2F02%2F08%2Fcall-snp%2F</url>
    <content type="text"><![CDATA[GATK4流程 准备配套数据要明确你的参考基因组版本了！！！ b36/b37/hg18/hg19/hg38，记住b37和hg19并不是完全一样的，有些微区别哦！！！ 1、下载hg19 这个下载地址非常多，常用的就是NCBI，ensembl和UCSC了，但是这里推荐用这个脚本下载（下载源为UCSC）： 1234567891011121314# 一个个地下载hg19的染色体for i in $(seq 1 22) X Y M;do echo $i;wget http://hgdownload.cse.ucsc.edu/goldenPath/hg19/chromosomes/chr$&#123;i&#125;.fa.gz;donegunzip *.gz# 用cat按照染色体的顺序拼接起来，因为GATK后面的一些步骤对染色体顺序要求非常变态，如果下载整个hg19，很难保证染色体顺序是1-22，X,Y,Mfor i in $(seq 1 22) X Y M;do cat chr$&#123;i&#125;.fa &gt;&gt; hg19.fasta;donerm -fr chr*.fasta BWA: Map to Reference 建立参考序列索引 1$ bwa index -a bwtsw ref.fa 参数-a用于指定建立索引的算法： bwtsw 适用于&gt;10M is 适用于参考序列&lt;2G (默认-a is) 可以不指定-a参数，bwa index会根据基因组大小来自动选择合适的索引方法 序列比对 12$ bwa mem ref.fa sample_1.fq sample_2.fq -R &apos;@RG\tID:sample\tLB:sample\tSM:sample\tPL:ILLUMINA&apos; \ 2&gt;sample_map.log | samtools sort -@ 20 -O bam -o sample.sorted.bam 1&gt;sample_sort.log 2&gt;&amp;1 -R 选项为必须选项，用于定义头文件中的SAM/BAM文件中的read group和sample信息 1234567&gt; The file must have a proper bam header with read groups. Each read group must contain the platform (PL) and sample (SM) tags. &gt; For the platform value, we currently support 454, LS454, Illumina, Solid, ABI_Solid, and CG (all case-insensitive)&gt; &gt; The GATK requires several read group fields to be present in input files and will fail with errors if this requirement is not satisfied&gt; &gt; ID：输入reads集的ID号； LB： reads集的文库名； SM：样本名称； PL：测序平台&gt; 查看SAM/BAM文件中的read group和sample信息： 1234567$ samtools view -H /path/to/my.bam | grep &apos;^@RG&apos;@RG ID:0 PL:solid PU:Solid0044_20080829_1_Pilot1_Ceph_12414_B_lib_1_2Kb_MP_Pilot1_Ceph_12414_B_lib_1_2Kb_MP LB:Lib1 PI:2750 DT:2008-08-28T20:00:00-0400 SM:NA12414 CN:bcm@RG ID:1 PL:solid PU:0083_BCM_20080719_1_Pilot1_Ceph_12414_B_lib_1_2Kb_MP_Pilot1_Ceph_12414_B_lib_1_2Kb_MP LB:Lib1 PI:2750 DT:2008-07-18T20:00:00-0400 SM:NA12414 CN:bcm@RG ID:2 PL:LS454 PU:R_2008_10_02_06_06_12_FLX01080312_retry LB:HL#01_NA11881 PI:0 SM:NA11881 CN:454MSC@RG ID:3 PL:LS454 PU:R_2008_10_02_06_07_08_rig19_retry LB:HL#01_NA11881 PI:0 SM:NA11881 CN:454MSC@RG ID:4 PL:LS454 PU:R_2008_10_02_17_50_32_FLX03080339_retry LB:HL#01_NA11881 PI:0 SM:NA11881 CN:454MSC... read group信息不仅会加在头信息部分，也会在比对结果的每条记录里添加一个 RG:Z:* 标签 123456$ samtools view /path/to/my.bam | grep &apos;^@RG&apos;EAS139_44:2:61:681:18781 35 1 1 0 51M = 9 59 TAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAA B&lt;&gt;;==?=?&lt;==?=?=&gt;&gt;?&gt;&gt;&lt;=&lt;?=?8&lt;=?&gt;?&lt;:=?&gt;?&lt;==?=&gt;:;&lt;?:= RG:Z:4 MF:i:18 Aq:i:0 NM:i:0 UQ:i:0 H0:i:85 H1:i:31EAS139_44:7:84:1300:7601 35 1 1 0 51M = 12 62 TAACCCTAAGCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAA G&lt;&gt;;==?=?&amp;=&gt;?=?&lt;==?&gt;?&lt;&gt;&gt;?=?&lt;==?&gt;?&lt;==?&gt;?1==@&gt;?;&lt;=&gt;&lt;; RG:Z:3 MF:i:18 Aq:i:0 NM:i:1 UQ:i:5 H0:i:0 H1:i:85EAS139_44:8:59:118:13881 35 1 1 0 51M = 2 52 TAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAA @&lt;&gt;;&lt;=?=?==&gt;?&gt;?&lt;==?=&gt;&lt;=&gt;?-?;=&gt;?:&gt;&lt;==?7?;&lt;&gt;?5?&lt;&lt;=&gt;:; RG:Z:1 MF:i:18 Aq:i:0 NM:i:0 UQ:i:0 H0:i:85 H1:i:31EAS139_46:3:75:1326:2391 35 1 1 0 51M = 12 62 TAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAA @&lt;&gt;==&gt;?&gt;@???B&gt;A&gt;?&gt;A?A&gt;??A?@&gt;?@A?@;??A&gt;@7&gt;?&gt;&gt;@:&gt;=@;@ RG:Z:0 MF:i:18 Aq:i:0 NM:i:0 UQ:i:0 H0:i:85 H1:i:31... 若原始SAM/BAM文件没有read group和sample信息，可以通过AddOrReplaceReadGroups添加这部分信息 12345678$ java -jar picard.jar AddOrReplaceReadGroups \ I=input.bam \ O=output.bam \ RGID=4 \ RGLB=lib1 \ RGPL=illumina \ RGPU=unit1 \ RGSM=20 前期处理在进行本部分的操作之前先要做好以下两部的准备工作 1、创建GATK索引。用Samtools为参考序列创建一个索引，这是为方便GATK能够快速地获取fasta上的任何序列做准备 1$ samtools faidx database/chr17.fa # 该命令会在chr17.fa所在目录下创建一个chr17.fai索引文件 2、生成.dict文件 1$ gatk CreateSequenceDictionary -R database/chr17.fa -O database/chr17.dict GATK4的调用语法： 1gatk [--java-options &quot;-Xmx4G&quot;] ToolName [GATK args] 去除PCR重复duplicates的产生原因 PCR duplicates（PCR重复） PCR扩增时，同一个DNA片段会产生多个相同的拷贝，第4步测序的时候，这些来源于同！一！个！拷贝的DNA片段会结合到Fellowcell的不同位置上，生成完全相同的测序cluster，然后被测序出来，这些相同的序列就是duplicate Cluster duplicates 生成测序cluster的时候，某一个cluster中的DNA序列可能搭到旁边的另一个cluster的生成位点上，又再重新长成一个相同的cluster，这也是序列duplicate的另一个来源，这个现象在Illumina HiSeq4000之后的Flowcell中会有这类Cluster duplicates Optical duplicates（光学重复） 某些cluster在测序的时候，捕获的荧光亮点由于光波的衍射，导致形状出现重影（如同近视散光一样），导致它可能会被当成两个荧光点来处理。这也会被读出为两条完全相同的reads Sister duplicates 它是文库分子的两条互补链同时都与Flowcell上的引物结合分别形成了各自的cluster被测序，最后产生的这对reads是完全反向互补的。比对到参考基因组时，也分别在正负链的相同位置上，在有些分析中也会被认为是一种duplicates。 用泊松分布解释 NGS 测序数据的 duplication 问题我曾经有这样的疑惑，为什么文库构建过程中的 PCR 将每个文库分子都扩增了上千倍，以 PCR 10个循环为例 210= 1024 ，但是实际测序数据中 duplication 率并不高（低于20%） 有一篇文章从统计概率的角度详细探讨了一下 duplication 率的影响因素 PCR 的过程中不同长度的文库分子被扩增的效率不同（GC 太高或 AT 含量太高都会影响扩增效率），PCR 更倾向于扩增短片段的文库分子，这里先不考虑文库片段扩增效率的差异，把问题简化一下，假设所有文库分子扩增效率都相同。PCR duplicate 的主要来源是同一个文库分子的不同拷贝都在 flowcell 上生成了可以被测序的 cluster ，导致同一个分子的序列被测序仪读取多次。那么为何在每个分子都有上千个拷贝的情况下，实际却很少出现同一分子的多个拷贝被测序的情况呢？主要原因就是文库中 unique 分子的数量比被 flowcell 上引物捕获的分子数量多很多，直白点说就是 flowcell 上用于捕获文库分子的引物数量太少了，两者不在同一个数量级，导致很少出现同一个文库分子的多个拷贝被 flowcell 上引物捕获生成 cluster。 假设文库中所有分子与引物的结合都是随机的，简化一下就相当于，一个箱子中有 n 种颜色的球（文库中的 n 种 unique 分子），每种颜色有 1000 个（PCR 扩增的，随 cycle 数变化），从这个箱子中随机拿出来 k 个球（最终测序得到 k 条 reads），其中出现相同颜色的球就是 duplicate，那么 duplication 率就可以根据有多少种颜色的球被取出 0,1,2,3…… 次的概率计算，可以近似用泊松分布模型来描述。 以人全基因组重测序 30X 为例，PE150 需要约 3x108条 reads ，文库中 unique 分子数其实可以通过上机文库的浓度和体积（外加 PCR 循环数）计算出来，这里用近似值 3.5x1010 个 unique 分子。每个 unique 分子期望被测序的次数是 3x108/3.5x1010 = 0.0085 ，每个 unique 分子被测 0,1,2,3… 次的概率如下图： 12345678&gt; x &lt;- seq(0,10,1)&gt; xnames &lt;- as.character(x)&gt; xlab &lt;- &quot;一个文库分子的所有拷贝被测序的次数&quot;&gt; ylab &lt;- &quot;概率&quot;&gt; barplot(dpois(x,lambda = 0.0085),+ names.arg = xnames,+ xlab = xlab,+ ylab = ylab) 由于 unique 分子数量太多，被测 0 次的概率远高于 1 和 2 次，我们去除 0 次的看一下： unique 分子被测序 1 次的概率远大于 2次及以上，即便一个 unique 分子被测序 2 次，我们去除 duplicate 时候还会保留其中一条 reads。 如果降低文库中 unique 分子数量到 4.5x109个，PCR 循环数增加以便浓度达到跟上面模拟的情况相同，测序 reads 数还是 3x108 条，每个 unique 分子预期被测序的次数是 3x108/4.5x109 = 0.067 。 unique 分子数量减少，被测序 2次的概率增大，duplication 率显然也会增高。 到这里已经可以很明白的看出 duplication 率主要与文库中 unique 分子数量有关，所以建库过程中最大化 unique 分子数是降低 duplication 率的关键。文库中 unique 分子数越多，说明建库起始量越高，需要 PCR 的循环数越少，而文库中 unique 分子数越少，说明建库起始量越低，需要 PCR 的循环数越多，因此提高建库起始量是关键。 PCR bias的影响 DNA在打断的那一步会发生一些损失， 主要表现是会引发一些碱基发生颠换变换（嘌呤-变嘧啶或者嘧啶变嘌呤） ， 带来假的变异。 PCR过程会扩大这个信号， 导致最后的检测结果中混入了假的结果； PCR反应过程中也会带来新的碱基错误。 发生在前几轮的PCR扩增发生的错误会在后续的PCR过程中扩大， 同样带来假的变异； 对于真实的变异， PCR反应可能会对包含某一个碱基的DNA模版扩增更加剧烈（这个现象称为PCR Bias） 。 因此， 如果反应体系是对含有reference allele的模板扩增偏向强烈， 那么变异碱基的信息会变小， 从而会导致假阴。 探究samtools和picard去除read duplicates的方法1、samtools samtools 去除 duplicates 使用 rmdup 1$ samtools rmdup [-sS] &lt;input.srt.bam&gt; &lt;out.bam&gt; 只需要开始-s的标签， 就可以对单端测序进行去除PCR重复。其实对单端测序去除PCR重复很简单的，因为比对flag情况只有0,4,16，只需要它们比对到染色体的起始终止坐标一致即可，flag很容易一致。 Remove potential PCR duplicates: if multiple read pairs have identical external coordinates, only retain the pair with highest mapping quality. I &lt;img src=./picture/GATK4-pipeline-remove-duplicates-samtools-principle-1.png width=900/&gt; 但是对于双端测序就有点复杂了~ In the paired-end mode, this command ONLY works with FR orientation and requires ISIZE is correctly set. It does not work for unpaired reads (e.g. two ends mapped to different chromosomes or orphan reads). &lt;img src=./picture/GATK4-pipeline-remove-duplicates-samtools-principle-2.png width=900/&gt; 很明显可以看出，去除PCR重复不仅仅需要它们比对到染色体的起始终止坐标一致，尤其是flag，在双端测序里面一大堆的flag情况，所以我们的94741坐标的5个reads，一个都没有去除！ 这样的话，双端测序数据，用samtools rmdup效果就很差，所以很多人建议用picard工具的MarkDuplicates 功能 2、picard picard对于单端或者双端测序数据并没有区分参数，可以用同一个命令！ 对应单端测序，picard的处理结果与samtools rmdup没有差别，不过这个java软件的缺点就是奇慢无比 操作：排序及标记重复 1、排序（SortSam） 对sam文件进行排序并生成bam文件，将sam文件中同一染色体对应的条目按照坐标顺序从小到大进行排序 GATK4的排序功能是通过picard SortSam工具实现的。虽然samtools sort工具也可以实现该功能，但是在GATK流程中还是推荐用picard实现，因为SortSam会在输出文件的头信息部分添加一个SO标签用于说明文件已经被成功排序，且这个标签是必须的，GATK需要检查这个标签以保证后续分析可以正常进行 https://software.broadinstitute.org/gatk/documentation/tooldocs/current/picard_sam_SortSam.php 1234567# 使用GATK命令$ gatk SortSam -I mapping/T.chr17.sam -O preprocess/T.chr17.sort.bam -R database/chr17.fa -SO coordinate --CREATE_INDEX# 使用picard命令$ java -jar picard.jar SortSam \ I=input.bam \ O=sorted.bam \ SORT_ORDER=coordinate 如何检查是否成功排序？ 1234567891011121314151617181920212223242526272829$ samtools view -H /path/to/my.bam@HD VN:1.0 GO:none SO:coordinate@SQ SN:1 LN:247249719@SQ SN:2 LN:242951149@SQ SN:3 LN:199501827@SQ SN:4 LN:191273063@SQ SN:5 LN:180857866@SQ SN:6 LN:170899992@SQ SN:7 LN:158821424@SQ SN:8 LN:146274826@SQ SN:9 LN:140273252@SQ SN:10 LN:135374737@SQ SN:11 LN:134452384@SQ SN:12 LN:132349534@SQ SN:13 LN:114142980@SQ SN:14 LN:106368585@SQ SN:15 LN:100338915@SQ SN:16 LN:88827254@SQ SN:17 LN:78774742@SQ SN:18 LN:76117153@SQ SN:19 LN:63811651@SQ SN:20 LN:62435964@SQ SN:21 LN:46944323@SQ SN:22 LN:49691432@SQ SN:X LN:154913754@SQ SN:Y LN:57772954@SQ SN:MT LN:16571@SQ SN:NT_113887 LN:3994... 若随后的比对记录中的contig那一列的顺序与头文件的顺序一致，且在头信息中包含SO:coordinate这个标签，则说明，该文件是排序过的 2、标记重复（Markduplicates） 标记文库中的重复 https://software.broadinstitute.org/gatk/documentation/tooldocs/current/picard_sam_markduplicates_MarkDuplicates.php 1gatk MarkDuplicates -I preprocess/T.chr17.sort.bam -O preprocess/T.chr17.markdup.bam -M preprocess/T.chr17.metrics --CREATE_INDEX 质量值校正检测碱基质量分数中的系统错误，需要用到 GATK4 中的 BaseRecalibrator 工具 碱基质量分数重校准（Base quality score recalibration，BQSR)，就是利用机器学习的方式调整原始碱基的质量分数。它分为两个步骤: 利用已有的snp数据库，建立相关性模型，产生重校准表( recalibration table) 根据这个模型对原始碱基进行调整，只会调整非已知SNP区域。 注：如果不是人类基因组，并且也缺少相应的已知SNP数据库，可以通过严格SNP筛选过程（例如结合GATK和samtools）建立一个snp数据库。 注意：Base Recalibration是以read group为单位进行的，因此当一个BAM文件中包含了多个read group时，BaseRecalibrator会对每个read group分别建立校正模型 1、建立较正模型 质量值校正，这一步需要用到variants的known-sites，所以需要先准备好已知的snp，indel的VCF文件： 123456789# 下载known-site的VCF文件，到Ensembl上下载$ wget -c -P Ref/mouse/mm10/vcf ftp://ftp.ensembl.org/pub/release-93/variation/vcf/mus_musculus/mus_musculus.vcf.gz &gt;download.log &amp;$ cd Ref/mouse/mm10/vcf &amp;&amp; gunzip mus_musculus.vcf.gz &amp;&amp; mv mus_musculus.vcf dbsnp_150.mm10.vcf# 建好vcf文件的索引，需要用到GATK工具集中的IndexFeatureFile，该命令会在指定的vcf文件的相同路径下生成一个以&quot;.idx&quot;为后缀的文件$ gatk IndexFeatureFile -F dbsnp_150.mm10.vcf# 建立较正模型$ gatk BaseRecalibrator -R Ref/mouse/mm10/bwa/mm10.fa -I PharmacogenomicsDB/mouse/SAM/ERR118300.enriched.markdup.bam -O \PharmacogenomicsDB/mouse/SAM/ERR118300.recal.table --known-sites Ref/mouse/mm10/vcf/dbsnp_150.mm10.vcf 2、质量值校准 123# 质量校正$ gatk ApplyBQSR -R Ref/mouse/mm10/bwa/mm10.fa -I PharmacogenomicsDB/mouse/SAM/ERR118300.enriched.markdup.bam -bqsr \PharmacogenomicsDB/mouse/SAM/ERR118300.recal.table -O PharmacogenomicsDB/mouse/SAM/ERR118300.recal.bam SNP、 INDEL位点识别SNP calling 策略的选择当你有多个samples，然后你call snp时候，你是应该将所有sample分开call完之后，再merge在一起。还是直接将所有samples，同时用作input然后call snp呢？ 这里需要知道有哪些snp calling的策略： single sample calling：每一个sample的bam file都进行单独的snp calling，然后每个sample单独snp calling结果再合成一个总的snp calling的结果。 batch calling： 一定数目群集的bamfiles 一起calling snps，然后再merge在一起 joint calling： 所有samples的BAM files一起call 出一个包含所有samples 变异信息的output 一般来说，如果条件允许（computational power等），使用joint calling ，即将所有samples同时call是比较优的选择 原因： 1、对于低频率的变异具有更高更好的检测sensitivity 在joint calling中，由于所有samples中所有的位点都是同时call的，换句话说就是，所有位点的信息都是share的，因此可以如果某些samples中个别位点是低频率的，但是可能在其他samples中，该位点的频率比较高，因此可以准确的对低频位点有更加好的calling 效果。 如左图，在1到n的samples中，碱基G只出现在其中两个samples中。如果我们将这些samples单独的call snps，这个低频率G的位点将会被忽略。但是joint calling 却可以允许将这些碱基G出现的频率进行累加，将该低频率的突变也call出来。 在右图中，上面的sample是一个跟ref一样具有纯合位点，下面的sample在一些位点中有部分数据缺少的情况，例如rs429358.如果将这两个samples分开call snp，然后merge一起，这样会错误地将这两个samples，在这个位点上看作具有类似的变异频率，但是其实由于下面的samples在改位点的区域存在数据缺少，只能看作non-informative。 2、更好的过滤掉假阳性的变异callling的结果 现在使用过滤变异的方法例如VQSR等利用的统计模型，都基于一个比较大的samples size。joint calling 这种方法可以提供足够的数据，确保过滤这一步是统一应用于所有samples的。 Germline SNPs + Indels将一个或多个个体放在一起call snp，得到一个 joint callset，该snp calling的策略称为joint calling 第一步，单独为每个样本生成后续分析所需的中间文件——gVCF文件。这一步中包含了对原始fastq数据的质控、比对、排序、标记重复序列、BQSR和HaplotypeCaller gVCF等过程。这些过程全部都适合在单样本维度下独立完成。值得注意的是，与单样本模式不同，该模式中每个样本的gVCF应该成为这类流程的标配，在后续的步骤中我们可以通过gVCF很方便地完成群体的Joint Calling； 第二步，依据第一步完成的gVCF对这个群体进行Joint Calling，从而得到这个群体的变异结果和每个人准确的基因型（Genotype），最后使用VQSR完成变异的质控。 SNP、 INDEL位点识别1、单独为每个样本生成后续分析所需的中间文件——gVCF文件 12$ gatk HaplotypeCaller -R Ref/chr17.fa -I sam/T.chr17.recal.bam -ERC GVCF --dbsnp ../Ref/VCF/dbsnp_138.hg19.vcf \ -O calling/T.chr17.raw.snps.indels.vcf -L chr17:7400000-7800000 HaplotypeCaller可以同时call snp 和indel，通过局部 de-novo 组装而非基于mapping的结果。一旦程序在某个区域发现了变异信号，它会忽略已有的mapping信息，在该区域执行reads重组装 如何对指定的区域call snp？ 使用 -L 参数指定识别突变位点的区域 提供字符串指定单个interval，如-L chr17:7400000-7800000 只识别17号染色体7400000-7800000 区域的突变位点。或者使用 -XL 参数排除某个区域 提供一个保存interval lists的文件。GATK支持多种格式的interval lists文件 Picard-style .interval_list 由 SAM-like header 和 intervals 信息组成，坐标系统为 1-based 12345678910&gt; @HD VN:1.0 SO:coordinate&gt; @SQ SN:1 LN:249250621 AS:GRCh37 UR:http://www.broadinstitute.org/ftp/pub/seq/references/Homo_sapiens_assembly19.fasta M5:1b22b98cdeb4a9304cb5d48026a85128 SP:Homo Sapiens&gt; @SQ SN:2 LN:243199373 AS:GRCh37 UR:http://www.broadinstitute.org/ftp/pub/seq/references/Homo_sapiens_assembly19.fasta M5:a0d9851da00400dec1098a9255ac712e SP:Homo Sapiens&gt; 1 30366 30503 + target_1&gt; 1 69089 70010 + target_2&gt; 1 367657 368599 + target_3&gt; 1 621094 622036 + target_4&gt; 1 861320 861395 + target_5&gt; 1 865533 865718 + target_6&gt; - GATK-style `.list` or `.intervals` 这种格式很简单，intervals需要写成这种格式：`&lt;chr&gt;:&lt;start&gt;-&lt;stop&gt;`，坐标系统为 1-based - BED files `.bed` BED3格式：`&lt;chr&gt; &lt;start&gt; &lt;stop&gt;`，坐标系统为 0-based，GATK只接受 1-based 坐标系统，因此GATK会根据文件后缀 `.bed` 识别bed文件格式，然后会将 0-based 转换为 1-based **注意：** &gt; - intervals 必须按照 reference 的坐标进行排序 &gt; - 可以提供多个intervals集合，文件格式不必一致：`-L intervals1.interval_list -L intervals2.list -L intervals3.bed ...` 2、合并多个GVCF文件得到GenomicsDB，为joint genotyping做准备。若是单样本跳过该步骤 123456$ gatk --java-options &quot;-Xmx4g -Xms4g&quot; GenomicsDBImport \ -V data/gvcfs/mother.g.vcf.gz \ -V data/gvcfs/father.g.vcf.gz \ -V data/gvcfs/son.g.vcf.gz \ --genomicsdb-workspace-path my_database \ -L 20 也可以使用CombineGVCFs实现该功能。CombineGVCFs继承自GATK4之前的版本，但是它的运行速度不如GenomicsDB 12345$ gatk CombineGVCFs \ -R reference.fasta \ --variant sample1.g.vcf.gz \ --variant sample2.g.vcf.gz \ -O cohort.g.vcf.gz 3、联合call snp，得到 joint-called SNP&amp;indel 1234567891011# 单样本$ gatk --java-options &quot;-Xmx4g&quot; GenotypeGVCFs \ -R Homo_sapiens_assembly38.fasta \ -V input.g.vcf.gz \ -O output.vcf.gz# 多样本$ gatk --java-options &quot;-Xmx4g&quot; GenotypeGVCFs \ -R Homo_sapiens_assembly38.fasta \ -V gendb://my_database \ -O output.vcf.gz SNP、 INDEL位点过滤 方法一：通过质量校正来过滤（Filter Variants by Variant (Quality Score) Recalibration） 1、用机器学习的方法基于已知的变异位点对caller给出的原始 variant quality score 进行校正 (VQSR)，包含两步：（1）构建校正模型；（2）应用模型进行质量值校正 由于评价SNP和Indel质量高低的标准不同，因此需要分SNP和Indel两种不同的模式，分别进行校正 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# SNP mode## 构建校正模型$ gatk VariantRecalibrator \ -R Homo_sapiens_assembly38.fasta \ -V input.vcf.gz \ --resource hapmap,known=false,training=true,truth=true,prior=15.0:hapmap_3.3.hg38.sites.vcf.gz \ --resource omni,known=false,training=true,truth=false,prior=12.0:1000G_omni2.5.hg38.sites.vcf.gz \ --resource 1000G,known=false,training=true,truth=false,prior=10.0:1000G_phase1.snps.high_confidence.hg38.vcf.gz \ --resource dbsnp,known=true,training=false,truth=false,prior=2.0:Homo_sapiens_assembly38.dbsnp138.vcf.gz \ -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR \ -mode SNP \ -O output.snps.recal \ --tranches-file output.snps.tranches \ --rscript-file output.snps.plots.R## 应用模型进行质量值校正$ gatk ApplyVQSR \ -R Homo_sapiens_assembly38.fasta \ -V input.vcf.gz \ -O output.snps.VQSR.vcf.gz \ --truth-sensitivity-filter-level 99.0 \ --tranches-file output.snps.tranches \ --recal-file output.snps.recal \ -mode SNP# Indel mode## 上一步SNP mode产生的输出作为这一步的输入## 构建校正模型$ gatk VariantRecalibrator \ -R Homo_sapiens_assembly38.fasta \ -V input.snps.VQSR.vcf.gz \ --resource hapmap,known=false,training=true,truth=true,prior=15.0:hapmap_3.3.hg38.sites.vcf.gz \ --resource omni,known=false,training=true,truth=false,prior=12.0:1000G_omni2.5.hg38.sites.vcf.gz \ --resource 1000G,known=false,training=true,truth=false,prior=10.0:1000G_phase1.snps.high_confidence.hg38.vcf.gz \ --resource dbsnp,known=true,training=false,truth=false,prior=2.0:Homo_sapiens_assembly38.dbsnp138.vcf.gz \ -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR \ -mode INDEL \ -O output.indels.recal \ --tranches-file output.indels.tranches \ --rscript-file output.indels.plots.R## 应用模型进行质量值校正$ gatk ApplyVQSR \ -R Homo_sapiens_assembly38.fasta \ -V input.snps.VQSR.vcf.gz \ -O output.snps.indels.VQSR.vcf.gz \ --truth-sensitivity-filter-level 99.0 \ --tranches-file output.indels.tranches \ --recal-file output.indels.recal \ -mode INDEL 执行变异校正需要满足两个条件： 大量高质量的已知variants作为训练集，而这对于许多的物种是不满足的 数据集不能太小，因为它需要足够的数据集来识别 good vs. bad variants 因此对于只有一两个样本、靶向测序、RNA-seq、非模式动物的数据，都不推荐进行变异质量值校正。若以上两个条件都不满足，则需要采取直接过滤 (hard-filtering) 的策略 方法二：直接过滤 (hard-filtering) Steps Extract the SNPs from the call set Apply the filter to the SNP call set Extract the Indels from the call set Apply the filter to the Indel call set Combine SNP and indel call set Get passed call set 提取SNP位点 12$ gatk SelectVariants -R Ref/chr17.fa -V calling/T.chr17.raw.snps.indels.genotype.vcf \--select-type-to-include SNP -O filter/T.chr17.raw.snps.genotype.vcf 参数说明： 1234--select-type-to-include,-select-type:Type Select only a certain type of variants from the input file This argument may be specified 0 or more times. Default value: null. Possible values: &#123;NO_VARIATION, SNP, MNP, INDEL, SYMBOLIC, MIXED&#125; - **提取INDEL位点** 12$ gatk SelectVariants -R Ref/chr17.fa -V calling/T.chr17.raw.snps.indels.genotype.vcf \--select-type-to-include INDEL -O filter/T.chr17.raw.indels.genotype.vcf - **SNP位点过滤** 12$ gatk VariantFiltration -R Ref/chr17.fa -V filter/T.chr17.raw.snps.genotype.vcf --filter-expression &quot;QD &lt; 2.0 || FS &gt; 60.0 || MQ &lt; 40.0 || SOR &gt; 3.0 || MQRankSum &lt; -12.5 || \ReadPosRankSum &lt; -8.0&quot; --filter-name &quot;SNP_FILTER&quot; -O filter/T.chr17.filter.snps.genotype.vcf 参数说明： 123456--filter-expression,-filter:String One or more expression used with INFO fields to filter This argument may be specified 0 or more times. Default value: null.--filter-name:String Names to use for the list of filters This argument may be specified 0 or more times. Default value: null. - **INDEL位点过滤** 12$ gatk VariantFiltration -R Ref/chr17.fa -V filter/T.chr17.raw.indels.genotype.vcf --filter-expression &quot;QD &lt; 2.0 || FS &gt; 200.0 || SOR &gt; 10.0 || MQRankSum &lt; -12.5 || ReadPosRankSum &lt; \-20.0&quot; --filter-name &quot;INDEL_FILTER&quot; -O filter/T.chr17.filter.indels.genotype.vcf - **合并过滤后SNP、 INDEL文件** 12$ gatk MergeVcfs -I filter/T.chr17.filter.snps.genotype.vcf -I \filter/T.chr17.filter.indels.genotype.vcf -O filter/T.chr17.filter.snps.indels.genotype.vcf - **提取PASS突变位点** 12$ gatk SelectVariants -R Ref/chr17.fa -V filter/T.chr17.filter.snps.indels.genotype.vcf -OT.chr17.pass.snps.indels.genotype.vcf -select &quot;vc.isNotFiltered()&quot; 参数说明： 123--selectExpressions,-select:String One or more criteria to use when selecting the data This argument may be specified 0 or more times. Default value: null. Hard-filter阈值探究GATK4官网给出的推荐阈值： For SNPs: 1234567&gt; QD &lt; 2.0&gt; MQ &lt; 40.0&gt; FS &gt; 60.0&gt; SOR &gt; 3.0&gt; MQRankSum &lt; -12.5&gt; ReadPosRankSum &lt; -8.0&gt; For indels: 123456&gt; QD &lt; 2.0&gt; ReadPosRankSum &lt; -20.0&gt; InbreedingCoeff &lt; -0.8&gt; FS &gt; 200.0&gt; SOR &gt; 10.0&gt; 点此 查看GATK4原始网页 该阈值选择来自于GATK4官网的推荐，阈值依据于比较真 vs. 假 snp的特征值（annotation values）统计分布 One of the most helpful ways to approach hard-filtering is to visualize the distribution of annotation values for a truth set called using a particular pipeline. These distributions are sharped by both the pipeline methodology and the underlying physical properties of the sequence data; so for a given pairing of data generation technology + analysis pipeline, you can derive filtering thresholds based on what the distributions look like for the truth set 评估数据来源：1000Genomes 中的 whole genome trio 获得真/假变异的方法： 将whole genome trio用HaplotypeCaller的GVCF mode得到各样本单独的GVCF文件，然后再用GenotypeGVCFs进行joint-calling得到未过滤的VCF文件，最后用VQSR（基于统计机器学习方法）得到过滤的变异集合 将PASS的变异认为是真的，将被过滤的变异当作是假的 比较真变异与假变异的特征值（annotation values）的分布，进行评估的特征值有：QD, FS, SOR, MQ, MQRankSum 和 ReadPosRankSum QualByDepth (QD) 变异质量值（variant confidence (from the QUAL field)）除以该位点覆盖的非同源的depth（与参考基因组不同的），即对质量值按照depth进行了标准化 大多数被过滤掉的变异的落在低QD区域 推荐：QD≥2 FisherStrand (FS) 一个衡量strand bias的量 1Phred-scaled probability that there is strand bias at the site 当某一个位点的strand bias的程度很小或没有，那么 FS 值会接近于0 大多数过滤掉的变异的落在FS大于55的区域 推荐：FS≤60 StrandOddsRatio (SOR) 另一种评估strand bias的量 Notice most of the variants that have an SOR value greater than 3 fail the VQSR filter 推荐：SOR≥3 RMSMappingQuality (MQ) 覆盖改位点的reads的mapping quality的均方根 原图 局部放大 推荐：MQ≥40 MappingQualityRankSumTest (MQRankSum) 推荐：MQRankSum≥-2.5 SAMtools-BCFtools流程关于SNP的过滤SNP过滤的意义： 第一，过滤到一些低质量的SNP可以防止calling一些假阳性的SNP，这些假阳性的SNP会很大程度影响到后续的一系列的分析，例如GWAS等的分析，最后影响相关生物学问题的解答； 第二，如果你有很多的个体，往往你的call完SNP后，VCF文件的大小的会比较大，如果不经过过滤，对下游的群体结构分析，PCA等相关的计算，都会对计算机产生巨大的负担（运行速度还有需要的内存等） 使用vcftools进行SNP过滤VCFtools能干什么 过滤特定的变异 比较文件 变异注释 文件格式转换 校验和合并文件 对变异位点集合进行坐标运算 参考资料： (1) 生信菜鸟团：GATK使用注意事项 (2) 小天师兄《全外显子组测序分析》 (3) RNA-Seq是否可以替代WES完成外显子的变异检测?二代测序的四种Read重复是如何产生的? (4) 生信菜鸟团：仔细探究samtools的rmdup是如何行使去除PCR重复reads功能的 (5) 生信技能树论坛：GATK之BaseRecalibrator (6) GATK Forum: Collected FAQs about input files for sequence read data (BAM/CRAM) (7) GATK Forum: What input files does the GATK accept / require? (8) GATK Forum: Collected FAQs about interval lists (9) GATK Forum：GATK Best Practices (10) 公众号碱基矿工：GATK4全基因组数据分析最佳实践 ，我以这篇文章为标志，终结当前WGS系列数据分析的流程主体问题 | 完全代码 (11) 生信菜鸟团：生信笔记：call snp是应该一起call还是分开call？ (12) GATK Forum：Hard-filtering germline short variants (13) 生信杂谈：用泊松分布解释 NGS 测序数据的 duplication 问题 (14) 【简书】关于SNP的过滤（1）：如何使用vcftools进行SNP过滤 (15) VCFtools官网]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>Bioinformatics</tag>
        <tag>WGS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Andrew NG Coursera 课程编程作业]]></title>
    <url>%2F2019%2F02%2F07%2FAndrew-Ng-Coursera-Homework%2F</url>
    <content type="text"><![CDATA[使用课程推荐的Octave进行编程实现，可以将Octave理解为开源版本的MATLAB Ex1: Linear Regression 读入数据 123456data = load(&apos;ex1data1.txt&apos;); % 导入的数据文件为用逗号隔开的两列，第一列为x，第二列为yX = data(:, 1);y = data(:, 2);% 可以尝试绘图% figure;plot(x,y);m = length(y); 数据分布图如下： 梯度下降前的数据预处理与设置 123456X = [ones(m,1),data(:,1)]; % 添加x0列，都设为1theta = zeros(2,1); % 初始化θ值% 梯度下降的一些设置信息iterations = 1500; % 迭代次数alpha = 0.01; % 学习率α 计算损失函数 线性回归的损失函数为： 1234567% 定义一个函数computeCost来计算损失函数function J = computeCost(X, y, theta) m = length(y); predictions = X*theta; % 计算预测值hθ sqerrors = (predictions - y).^2; % 计算平方误，矩阵的点乘运算得到的是一个向量 J = 1/(2*m)* sum(sqerrors);end 执行梯度下降 12345678910111213% 定义一个函数gradientDescent来执行梯度下降，为了后面观察梯度下降过程中损失函数的变化，记录每一步迭代后的损失函数值function [theta, J_history] = gradientDescent(X, y, theta, alpha, iterations) m = length(y); J_history = zeros(num_iters, 1); % 以迭代次数为唯一迭代终止条件 for iter = 1:num_iters % 计算梯度 predictions = X*theta; updates = X&apos;*(predictions - y); theta = theta - alpha*(1/m)*updates; J_history(iter) = computeCost(X, y, theta); endend 绘制拟合直线 123hold on; % 保留之前的绘图窗口，在这个绘图窗口继续画出拟合直线plot(X(:,2), X*theta, &apos;-&apos;);legend(&apos;Training data&apos;, &apos;Linear regression&apos;); Ex2: Logistic回归目标： 构建一个logistics回归模型，依据学生两次考试的成绩来预测一个学生能否被大学录取 通过的输入数据文件为ex2data1.txt： 123456789101112# 为用逗号隔开的3列，分别为：exam1Score,exam2Score,lable34.62365962451697,78.0246928153624,030.28671076822607,43.89499752400101,035.84740876993872,72.90219802708364,060.18259938620976,86.30855209546826,179.0327360507101,75.3443764369103,145.08327747668339,56.3163717815305,061.10666453684766,96.51142588489624,175.02474556738889,46.55401354116538,176.09878670226257,87.42056971926803,1... 读入数据，并画出数据分布散点图 123456789101112131415161718data = load(&apos;ex2data1.txt&apos;);x = data(:,:2);y = data(:,3);% 画图%% 区分出两类样本pos = find(y==1);neg = find(y==0);figure;%% 画出pos类样本plot(x(pos,1), x(pos,2),&apos;k+&apos;,&apos;MarkerSize&apos;, 3);hold on;%% 画出neg类样本plot(x(neg,1), x(neg,2),&apos;ko&apos;,&apos;MarkerSize&apos;, 3);xlab(&apos;Exam 1 score&apos;);ylab(&apos;Exam 2 score&apos;);legend(&apos;Admitted&apos;, &apos;Not admitted&apos;);hold off; 要得到的目标函数为 sigmoid 函数： 计算Cost和gradient 12345678[m,n] = size(x);X = [ones(m),data(:,:2)];initialTheta = zeros(1,n+1);% 计算初始的Cost和gradient[initalCost,initialGradient] = costFunction(X,y,initialTheta);% 打印出初始时的Cost和gradientfprintf(&apos;Cost at initial theta(zeros): %f\n&quot;,initialCost);fprintf(&apos;Gradient at initial theta(zeros): %f\n&quot;,initialGradient); 在计算初始的Cost (initalCost) 和 gradient (initialGradient) 时，调用了自定义的函数costFunction 损失函数为： 梯度的计算公式为： 下面给出costFunction函数的定义： 1234567function [jVal,gradient] = costFunction(x,y,theta) predict = sigmoid(x*theta); leftCost = -y&apos;*log(predict); rightCost = -(1-y)&apos;*log(1-predict); jVal = (1/m)*(leftCost+rightCost); gradient = (1/m)*((predict-y)&apos;*x);end 在上面的costFunction函数中又调用了sigmoid函数，定义为： 123function g = sigmoid(x) g = 1./(1+exp(-x));end 优化目标函数 使其目标函数的Cost最小化，即 可以像练习一那样使用传统的梯度下降方法进行参数的优化，但Octave内部自带了fminunc函数，可以用于非约束优化问题（unconstrained optimization problem）的求解 fminunc函数的用法为： 1234567%% 设置fminunc函数的内部选项options = optmset(&apos;GradObj&apos;, &apos;on&apos;, &apos;MaxIter&apos;, 400);% &apos;GradObj&apos;, &apos;on&apos; 设置梯度目标参数为打开状态，即需要给这个算法提供一个梯度% &apos;MaxIter&apos;, 400 设置最大迭代次数%% 使用fminunc函数执行非约束优化[optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options); 一般情况下costFunction函数和它的函数名一样，只计算Cost值，不过由于这里要用到fminunc这个非约束优化函数，该函数需要提供Cost和gradient，所以在前面costFunction函数时，增加了一个计算梯度值的功能 画出决策分界面 (Decision Boundary) 先像前面的第一步那样，画出原始数据分布散点图 1234567891011% 画图%% 区分出两类样本pos = find(y==1);neg = find(y==0);figure;%% 画出pos类样本plot(x(pos,1), x(pos,2),&apos;k+&apos;,&apos;MarkerSize&apos;, 3);%% 画出neg类样本plot(x(neg,1), x(neg,2),&apos;ko&apos;,&apos;MarkerSize&apos;, 3);xlab(&apos;Exam 1 score&apos;);ylab(&apos;Exam 2 score&apos;); 然后在散点图的基础上，把分界线画出来 分界线满足： 得到x1和x2直接的关系为： 1234567hold on;% 只需要选择两个点即可将直线画出plot_x = [min(X(:,2))-2, max(X(:,2))+2];plot_y = (-1./theta(3)).*(theta(2).*plot_x + theta(1));plot(plot_x, plot_y)legend(&apos;Admitted&apos;, &apos;Not admitted&apos;, &apos;Decision Boundary&apos;)hold off;]]></content>
      <categories>
        <category>Machine-Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SAM/BAM相关的进阶知识]]></title>
    <url>%2F2019%2F02%2F07%2FAdvanced-knowledge-of-SAM%2F</url>
    <content type="text"><![CDATA[samtools和picard的排序问题samtools和picard都有对SAM/BAM文件进行排序的功能，一般都是基于坐标排序（还提供了-n选项来设定用reads名进行排序），先是对chromosome/contig进行排序，再在chromosome/contig内部基于start site从小到大排序，对start site排序很好理解，可是对chromosome/contig排序的时候是基于什么标准呢？ 基于你提供的ref.fa文件中的chromosome/contig的顺序。当你使用比对工具将fastq文件中的reads比对上参考基因组后会生成SAM文件，SAM文件包含头信息，其中有以@SQ开头的头信息记录，reference中有多少条chromosome/contig就会有多少条这样的记录，而且它们的顺序与ref.fa是一致的。 SAM/BAM文件的头信息： 12345678910111213141516171819202122232425&gt; @HD VN:1.3 SO:coordinate&gt; @SQ SN:chr1 LN:195471971&gt; @SQ SN:chr2 LN:182113224&gt; @SQ SN:chr3 LN:160039680&gt; @SQ SN:chr4 LN:156508116&gt; @SQ SN:chr5 LN:151834684&gt; @SQ SN:chr6 LN:149736546&gt; @SQ SN:chr7 LN:145441459&gt; @SQ SN:chr8 LN:129401213&gt; @SQ SN:chr9 LN:124595110&gt; @SQ SN:chr10 LN:130694993&gt; @SQ SN:chr11 LN:122082543&gt; @SQ SN:chr12 LN:120129022&gt; @SQ SN:chr13 LN:120421639&gt; @SQ SN:chr14 LN:124902244&gt; @SQ SN:chr15 LN:104043685&gt; @SQ SN:chr16 LN:98207768&gt; @SQ SN:chr17 LN:94987271&gt; @SQ SN:chr18 LN:90702639&gt; @SQ SN:chr19 LN:61431566&gt; @SQ SN:chrX LN:171031299&gt; @SQ SN:chrY LN:91744698&gt; @SQ SN:chrM LN:16299&gt; @RG ID:ERR144849 LB:ERR144849 SM:A_J PL:ILLUMINA&gt; ref.fa中chromosome/contig的排列顺序： 1234567891011121314151617181920212223&gt; &gt;chr1&gt; &gt;chr2&gt; &gt;chr3&gt; &gt;chr4&gt; &gt;chr5&gt; &gt;chr6&gt; &gt;chr7&gt; &gt;chr8&gt; &gt;chr9&gt; &gt;chr10&gt; &gt;chr11&gt; &gt;chr12&gt; &gt;chr13&gt; &gt;chr14&gt; &gt;chr15&gt; &gt;chr16&gt; &gt;chr17&gt; &gt;chr18&gt; &gt;chr19&gt; &gt;chrX&gt; &gt;chrY&gt; &gt;chrM&gt; 它们的顺序一致 当使用samtools或picard对SAM/BAM文件进行排序时，这些工具就会读取头信息，按照头信息指定的顺序来排chromosome/contig。所以进行排序时需要提供包含头信息的SAM/BAM文件。 那么普通情况下我们的chromosome/contig排序情况是什么样的? 一般情况下我们获取参考基因组序列文件的来源有三个： NCBI ENSEMBEL UCSC Genome Browser 这里以UCSC FTP下载源为例： 这是一个压缩文件，使用tar zxvf chromFa.tar.gz解压后，会得到多个fasta文件，每条chromosome/contig一个fasta文件：chr1.fa, chr2.fa … 之后我们会将它们用cat *.fa &gt;ref.fa合并成一个包含多条chromosome/contig的物种参考基因组序列文件 用grep &quot;&gt;&quot; ref.fa可以查看合并后发ref.fa文件中染色体的排列顺序为： 1234567891011121314151617181920&gt; &gt;chr10&gt; &gt;chr11&gt; &gt;chr12&gt; &gt;chr13&gt; &gt;chr14&gt; &gt;chr15&gt; &gt;chr16&gt; &gt;chr17&gt; &gt;chr18&gt; &gt;chr19&gt; &gt;chr1&gt; &gt;chr1_GL456210_random&gt; &gt;chr1_GL456211_random&gt; &gt;chr1_GL456212_random&gt; &gt;chr1_GL456213_random&gt; &gt;chr1_GL456221_random&gt; &gt;chr2&gt; &gt;chr3&gt; &gt;chr4&gt; 这和我们平时想象的染色体的排列顺序是不是有一些出入？难道不应该是从chr1开始到chr22，最后是chrX和chrY这样的顺序吗？ 想象归想象，实际上它是按照字符顺序进行的，chr11就应该排在chr2前面 一般情况下在进行SAM文件的排序时，染色体的排序到底是按照哪种规则进行排序的，不是一个很重要的问题，也不会对后续的分析产生影响，但是在执行GATK流程时，GATK对染色体的排序是有要求的，必须按照从chr1开始到chr22，最后是chrX和chrY这样的顺序，否则会报错 面对这样变态的要求，我们怎么解决？ 在构造ref.fa文件时，让它按照从chr1开始到chr22，最后是chrX和chrY这样的顺序进行组织就可以了： 123for i in $(seq 1 22) X Y M;do cat chr$&#123;i&#125;.fa &gt;&gt; hg19.fasta;done SAM文件中FLAG值的理解FLAG列在SAM文件的第二列，这是一个很重要的列，包含了很多mapping过程中的有用信息，但很多初学者在学习SAM文件格式的介绍时，遇到FLAG列的说明，常常会一头雾水 what?还二进制，这也太反人类的设计了吧！ 不过如果你站在开发者的角度去思考这个问题，就会豁然开朗 在mapping过程中，我们想记录一条read的mapping的信息包括： 这条read是read1 (forward-read) 还是read2 (reverse-read)？ 这条read比对上了吗？与它对应的另一头read比对上了吗？ … 这些信息总结起来总共包括以下12项： 序号 简写 说明 1 PAIRED paired-end (or multiple-segment) sequencing technology 2 PROPER_PAIR each segment properly aligned according to the aligner 3 UNMAP segment unmapped 4 MUNMAP next segment in the template unmapped 5 REVERSE SEQ is reverse complemented 6 MREVERSE SEQ of the next segment in the template is reversed 7 READ1 the first segment in the template 8 READ2 the last segment in the template 9 SECONDARY secondary alignment 10 QCFAIL not passing quality controls 11 DUP PCR or optical duplicate 12 SUPPLEMENTARY supplementary alignment 而每一项又只有两种情况，是或否，那么我可以用一个12位的二进制数来记录所有的信息，每一位表示某一项的情况，这就是原始FLAG信息的由来，但是二进制数适合给计算机看，不适合人看，需要转换成对应的十进制数，也就有了我们在SAM文件中看到的FLAG值 但是FLAG值所包含信息的解读还是要转换为12位的二进制数 SAM文件中那些未比对的readsSAM格式文件的第3和第7列，可以用来判断某条reads是否比对成功到了基因组的染色体，左右两条reads是否比对到同一条染色体 有两个方法可以提取未比对成功的测序数据： SAM文件的第3列是*的(如果是PE数据，需要考虑第3,7列) 12&gt; $ samtools view sample.bam | perl -alne &apos;&#123;print if $F[2] eq &quot;*&quot; or $F[6] eq &quot;*&quot; &#125;&apos; sample.unmapped.sam&gt; 或者SAM文件的flag标签包含0x4的 123&gt; # 小写的f是提取，大写的F是过滤&gt; $ samtools view -f4 sample.bam sample.unmapped.sam&gt; 虽然上面两个方法得到的结果是一模一样的，但是这个perl脚本运行速度远远比不上上面的samtools自带的参数 对于PE数据，在未比对成功的测序数据可以分成3类： 仅reads1没有比对成功 该提取条件包括： 该read是read1，对应于二进制FLAG的第7位，该位取1，其十进制值为64； 该read未成功比对到参考基因组，对应于二进制FLAG的第3位，该位取1，其十进制值为4； 另一配对read成功比对到参考基因组，对应于二进制FLAG的第4位，该位取0，其十进制值为8； 1234&gt; # 对于取1的位点采用提取的策略，用-f参数，值设为64+4=68&gt; # 对于取0的位点采取过滤的策略，用-F参数，值设为8&gt; $ samtools view -u -f 68 -F 8 alignments.bam &gt;read1_unmap.bam&gt; 仅reads2没有比对成功 该提取条件包括： 该read是read2，对应于二进制FLAG的第8位，该位取1，其十进制值为128； 该read未成功比对到参考基因组，对应于二进制FLAG的第3位，该位取1，其十进制值为4； 另一配对read成功比对到参考基因组，对应于二进制FLAG的第4位，该位取0，其十进制值为8； 1234&gt; # 对于取1的位点采用提取的策略，用-f参数，值设为128+4=132&gt; # 对于取0的位点采取过滤的策略，用-F参数，值设为8&gt; $ samtools view -u -f 132 -F 8 alignments.bam &gt;read2_unmap.bam&gt; 两端reads都没有比对成功 该提取条件包括： 该read未成功比对到参考基因组，对应于二进制FLAG的第3位，该位取1，其十进制值为4； 另一配对read未成功比对到参考基因组，对应于二进制FLAG的第4位，该位取1，其十进制值为8； 123&gt; # 对于取1的位点采用提取的策略，用-f参数，值设为4+8=12&gt; $ samtools view -u -f 12 alignments.bam &gt;pairs_unmap.bam&gt; 看完这一部分，是不是有一个感觉：FLAG玩得溜，SAM文件可以处理得出神入化 为什么一条read会有多条比对记录？首先，思考一个问题：对于PE数据，一条测序片段（fragment）有read1和read2两条测序片段，它们俩的名字相同，那么对于这一条测序片段，对它进行mapping之后得到的SAM文件中会出现几条记录呢？ 先声明以下只对BWA比对得到的SAM文件进行讨论，对其他比对工具输出的SAM文件可能不适用 首先，基于经验积累告诉我，它会得到有且只有两条记录，原因在于，BWA在对每条read执行比对时只会给出一个hit，若这条read是multiple mapping的情况，它会从中选择MAPQ值最高的那个hit作为输出，若存在多个hit的MAPQ值相等且最高，那么BWA会从中随机选择一个作为输出 对于我的这个假设可以用以下的方法进行验证： 123456789101112131415161718# 将SAM文件的第一列提出来，排序去重，同时统计每个QNAME出现的次数$ samtools view alignment.bam | cut -f1 | sort | uniq -c &gt;record.count得到的统计结果如下： 2 ERR144849.1 2 ERR144849.10 2 ERR144849.100 2 ERR144849.1000 2 ERR144849.10000 2 ERR144849.100000 2 ERR144849.1000000 2 ERR144849.10000000 2 ERR144849.10000001 2 ERR144849.10000002 2 ERR144849.10000003 2 ERR144849.10000004 2 ERR144849.10000005 ... 上面的测试结果与我们的假设吻合 但是在一次处理三代测序数据（三代测序数据是Single-End）中发现了不同： 12 在输出中出现了一些不太和谐的结果：有极少部分的QNAME对应2条以上的记录，这意味着存在一条read会有多条比对记录的情况，why？ 对这个与预期不完全相符的结果，尝试去寻找里面的原因，其间进行了各种各样的推理、假设、验证，最终在李恒的github中找到了答案 2. Why does a read appear multiple times in the output SAM? BWA-SW and BWA-MEM perform local alignments. If there is a translocation, a gene fusion or a long deletion, a read bridging the break point may have two hits, occupying two lines in the SAM output. With the default setting of BWA-MEM, one and only one line is primary and is soft clipped; other lines are tagged with 0x800 SAM flag (supplementary alignment) and are hard clipped. 参考资料： (1) 【简书】从零开始完整学习全基因组测序数据分析：第5节 理解并操作BAM文件 (2) 【生信技能树】【直播】我的基因组（十五）:提取未比对的测序数据]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>SAM</tag>
        <tag>FLAG</tag>
        <tag>Bioinformatics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo + GitHub Homepage 搭建个人博客]]></title>
    <url>%2F2019%2F01%2F30%2FSetup-private-BlogSite%2F</url>
    <content type="text"><![CDATA[Do not just seek happiness for yourself. Seek happiness for all. Through kindness. Through mercy. David LevithanWide Awake 实现原理看GitHub某一个页面的URL： 1https://github.com/Ming-Lian/NGS-analysis/blob/master/Stat-on-RNAseq.md 从它的URL里就可以看出服务器端的文件夹组织形式，https://github.com对应于其web服务器的家目录，以~表示： 12345678910111213~|---- User1 # 用户名 |---- repo1 # 仓库名 |---- blob # 固定文件夹，意义不明 |---- branch1 # 仓库下的分支名，默认分支为master |---- branch2 ... |---- repo2 |---- repo3 ...|---- User2|---- User3... 一般情况下，GitHub对每个仓库地下的文件是有一些限制的，对于纯文本形式的脚本文件可以以纯文本形式进行查看，若提交的是以*.md形式命名的文本，会进行Markdown语法解析，若是图片、pdf这样的文件也能进行查看，但是如果提交的是类似于可执行脚本或二进制程序，那么它是禁止它们执行的 但是若你建立的仓库是作为GitHub Homepage，GitHub服务器会给你破个例，这个仓库对应的文件夹下的文件，若是php、js这样的脚本文件，允许你执行（当然还是会有一些限制的）——这就好办了，那我就可以像在本地搭建网站那样，在GitHub服务器上搭建一个属于自己的网页，只需要在我的GitHub账号下创建一个GitHub Homepage类型的仓库，然后将我的这个网站需要的文件统统上传到这个仓库下就可以了 所以，如果想要利用GitHub Homepage搭建属于自己的博客，问题就变成了： 如何在本地把我的博客网站搭好？ 如何把我在本地搭好的网站提交到GitHub上？ 搭建本地博客网站搭建Hexo博客网站大家常用与博客网站搭建的开源框架就是WordPress了，但是WordPress比较大，这里我选择了用Hexo框架，Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 安装Hexo Hexo 基于 Node.js，因此需要先安装 Node.js 12$ https://raw.github.com/creationix/nvm/v0.33.11/install.sh$ sh install.sh 安装完成后，重启终端并执行下列命令即可安装 Node.js 1$ nvm install stable 所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo 1$ npm install -g hexo-cli Hexo使用 首先初始化博客 1$ hexo init myBlog 会在工作目录下生成myBlog文件夹，该文件夹下的内容如下图所示： 接下来，进入文件夹 myBlog，输入 1$ hexo s # hexo server 的简写形式，两种写法都可以 这样你已经搭好了本地的博客网站，且启动了本地预览服务，在浏览器地址栏中输入`localhost:4000`即可查看 ![](/images/Setup-private-BlogSite-2.png) 到这里，基于Hexo的博客网站就搭好了，剩下的就是如何编辑博客，以及如何修改Hexo主题 编辑博客博客文章保存在myBlog/source/_post文件夹下，注意文章类型得是md格式 可以执行下列命令来创建一篇新文章 12345$ hexo new [layout] &lt;title&gt;例如：$ hexo new testBlogINFO Created: ~/myBlog/source/_posts/testBlog.md 生成的md文件会自动生成以下的头信息： 12345---title: testBlogdate: 2019-01-30 20:31:50tags:--- 默认文章的标题与文件名一致，若想改动，比如在标题里加上一些汉字，则可以在头信息的title栏进行修改 编辑文章就可以直接使用vim编辑器对这个md文件进行修改 修改Hexo主部署本地网站至GitHub首先，你得有一个GitHub账号，登录后，新建一个命名为&lt;username&gt;.github.io的仓库 接着是要在本地连接GitHub远程仓库 由于你的本地Git仓库和GitHub仓库之间的传输是通过SSH加密的，所以我们需要配置验证信息，即提供本地仓库与GitHub仓库之间能够相互识别校验的SSH key 先检查一下本地是否已经存在SSH key。检查方法为在~/.ssh文件夹下是否存在id_dsa.pub文件 若没有SSH key，需要新建一个： 1$ ssh-keygen -t rsa -b 4096 -C &quot;your_email@example.com&quot; 新建的SSH key需要用户提供邮箱来进行标记（是否需要与GitHub中提供的邮箱一致，目前还不清楚，所以谨慎起见，还是用GitHub中提供的邮箱） 执行该命令后，会询问你SSH key信息文件的保存位置，直接回车会保存在默认位置~/.ssh。然后会询问你passphrase 这样就在~/.ssh文件夹下生成了以下两个文件： id_rsa id_rsa.pub 打开并复制id_rsa.pub中的信息至剪切板中，然后用电脑浏览器进入GitHub网站，进入Setting菜单，左边选择 SSH and GPG keys，然后点击 New SSH key 按钮,title 设置标题，可以随便填，粘贴在你电脑上生成的 key。 为了验证是否成功，输入以下命令： 1$ ssh -T git@github.com 随后你会看到以下的警告信息 123The authenticity of host &apos;github.com (IP ADDRESS)&apos; can&apos;t be established.RSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48.Are you sure you want to continue connecting (yes/no)? 输入yes然后回车，若输出以下信息则说明连接成功 1Hi username! You&apos;ve successfully authenticated, but GitHub does not provide shell access. 从GitHub的 Account =&gt;Settings =&gt;SSH and GPG keys 也可以看到，原先灰色的钥匙图标被点亮激活了，变成了绿色的 最后就是GitHub服务器端的部署了 Hexo 提供了快速方便的一键部署功能，只需一条命令就能将网站部署到服务器上 1$ hexo deploy # 也可以简写成 &quot;hexo d&quot; 在开始之前，必须先在 myBlog/_config.yml 中修改参数 这一步的目的是将 Hexo 与 GitHub 进行关联 在执行hexo d进行远程部署时，可能出现以下形式的报错： 12&gt; ERROR Deployer not found: git&gt; 报错信息说明未安装针对git的远程部署工具，那就安装吧 12&gt; $ npm install hexo-deployer-git --save&gt; 在执行git远程部署工具hexo-deployer-git的安装时，可能又会遇到下面的报错： 12&gt; npm ERR! Error: UNABLE_TO_VERIFY_LEAF_SIGNATURE&gt; 这个报错信息的分析与解决请看 node.js请求HTTPS报错 番外本地图片加载虽然在官方语法中，Markdown插入图片的格式是这样的： 1![Alt text](/path/to/img.jpg) 然而，如果你用了Hexo框架，那你得小心了。 首先，你不可能在网页里用绝对路径，这样怎么部署到服务器上呢？所以必定是相对路径。Markdown本来的语法中，只要img和md文件的相对路径是对的就行，然而Hexo不知道对路径做了什么处理，你需要在source文件夹中新建一个images文件夹，然后把图片放在images文件夹里。路径也必须是这样的： 1![Alt text](../../images/img.jpg) node.js请求HTTPS报错在用Nodejs发送https请求时候，出现Error: UNABLE_TO_VERIFY_LEAF_SIGNATURE的错误 错误的原因是：对方数字证书设置不正确 解决方案，设置不进行证书的验证 1$ npm config set strict-ssl false 这个设置只是为了临时解决这个证书不正确而我们又不得不用的网站的问题，为了安全起见，执行完上面的操作之后，最好再开启证书验证 1$ npm config set strict-ssl true 修改默认的style.css当我搭好Hexo框架，把一篇md笔记上传到对应目录中，启动本地Hexo预览服务，最好兴冲冲地在浏览器中打开https://localhost:4000预览效果时，发现了一点异常： 我的md文件并没有渲染出我预期的效果，特别是区块注释部分 (blockquote) 预期效果是这样的 实际效果是这样的 差别体现在： 区块注释左侧标志性的竖线没有了； 左侧竖线的实现需要的css语句为： 12345blockquote &#123; border-left:.5em solid #eee; padding: 0 0 0 2em; margin-left:0;&#125; 主要是要有border-left:.5em solid #eee;语句 区块中的文本莫名其妙地字体放大且居中了； 这个的原因是因为style.css文件中有这样一段 123456.article-entry blockquote &#123; font-family: Georgia, &quot;Times New Roman&quot;, serif; font-size: 1.4em; margin: 1.6em 20px; text-align: center;&#125; 主要是要将text-align: center;改为text-align: left; 将font-size: 1.4em;这句话删除，就能关闭字体放大 参考资料： (1) 【五分钟学算法】【新手向】从零开始搭建一个酷炫免费的个人博客 (2) Hexo中如何用Markdown插入本地图片 (3) Error: UNABLE_TO_VERIFY_LEAF_SIGNATURE Phonegap Installation (4) node.js请求HTTPS报错：UNABLE_TO_VERIFY_LEAF_SIGNATURE\的解决方法]]></content>
      <categories>
        <category>前后端技术</category>
      </categories>
  </entry>
</search>
