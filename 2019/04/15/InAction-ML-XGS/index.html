<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">


























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.0.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/Favico.png?v=7.0.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/Favico.png?v=7.0.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/Favico.png?v=7.0.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.0.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.0.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="1. 线性回归1.1. 目标函数与损失函数$$\hat y = \theta_0+\theta_1x_1+…+\theta_nx_n$$ 写成向量化的形式为： $$h_{\theta}=\theta^TX$$ 优化目标为最小化均方差 (MSE)： $$\min_{\theta} MSE(\theta)=\frac 12 \sum_{i=1}^m (\theta^TX^{(i)}-y^{(i)})^">
<meta name="keywords" content="Machine-Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="西瓜书带学训练营·实战任务">
<meta property="og:url" content="http://yoursite.com/2019/04/15/InAction-ML-XGS/index.html">
<meta property="og:site_name" content="Lianm&#39;s Blog">
<meta property="og:description" content="1. 线性回归1.1. 目标函数与损失函数$$\hat y = \theta_0+\theta_1x_1+…+\theta_nx_n$$ 写成向量化的形式为： $$h_{\theta}=\theta^TX$$ 优化目标为最小化均方差 (MSE)： $$\min_{\theta} MSE(\theta)=\frac 12 \sum_{i=1}^m (\theta^TX^{(i)}-y^{(i)})^">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-lr-1.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-lr-2.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-lr-3.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-lr-4.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-lr-5.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-lr-6.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-lr-7.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-decision-tree-1.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-decision-tree-2.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-1.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-2.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-3.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-4.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-5.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-6.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-7.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-8.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-9.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-10.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-11.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-12.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-13.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-14.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-naive-bayes-1.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-naive-bayes-2.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-naive-bayes-3.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-naive-bayes-4.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-naive-bayes-5.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-naive-bayes-6.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-1.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-2.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-3.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-4.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-5.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-6.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-7.jpg">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-8.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-9.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-10.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-11.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-12.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-13.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-14.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-15.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-16.png">
<meta property="og:updated_time" content="2019-04-15T09:10:39.380Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="西瓜书带学训练营·实战任务">
<meta name="twitter:description" content="1. 线性回归1.1. 目标函数与损失函数$$\hat y = \theta_0+\theta_1x_1+…+\theta_nx_n$$ 写成向量化的形式为： $$h_{\theta}=\theta^TX$$ 优化目标为最小化均方差 (MSE)： $$\min_{\theta} MSE(\theta)=\frac 12 \sum_{i=1}^m (\theta^TX^{(i)}-y^{(i)})^">
<meta name="twitter:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-lr-1.png">






  <link rel="canonical" href="http://yoursite.com/2019/04/15/InAction-ML-XGS/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>西瓜书带学训练营·实战任务 | Lianm's Blog</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Lianm's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">Focus on Bioinformatics and Machine-Learning</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    
  
  
  
  

  

  <a href="https://github.com/Ming-Lian" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" style="fill: #222; color: #fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a>



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/15/InAction-ML-XGS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lianm">
      <meta itemprop="description" content="中国科学院北京基因组研究所研究生">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lianm's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">西瓜书带学训练营·实战任务

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-15 16:57:28 / 修改时间：17:10:39" itemprop="dateCreated datePublished" datetime="2019-04-15T16:57:28+08:00">2019-04-15</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine-Learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1. 线性回归"></a>1. 线性回归</h2><h3 id="1-1-目标函数与损失函数"><a href="#1-1-目标函数与损失函数" class="headerlink" title="1.1. 目标函数与损失函数"></a>1.1. 目标函数与损失函数</h3><p>$$\hat y = \theta_0+\theta_1x_1+…+\theta_nx_n$$</p>
<p>写成向量化的形式为：</p>
<p>$$h_{\theta}=\theta^TX$$</p>
<p>优化目标为最小化均方差 (MSE)：</p>
<p>$$\min_{\theta} MSE(\theta)=\frac 12 \sum_{i=1}^m (\theta^TX^{(i)}-y^{(i)})^2$$</p>
<h3 id="1-2-标准方程求解"><a href="#1-2-标准方程求解" class="headerlink" title="1.2. 标准方程求解"></a>1.2. 标准方程求解</h3><p>用标准方程法得到线性回归问题的闭式解，表达式为</p>
<p>$$\theta^*=(X^TX)^{-1}X^Ty$$</p>
<p>可以使用Numpy中的线性代数模块 <code>np.linalg</code> 进行求解</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"># 构成原始表达式为 y=4+3x 直线，加上高斯噪声</span><br><span class="line">X = 2*np.random.rand(100,1)</span><br><span class="line">y = 4 + 3*X + np.random.rand(100,1)</span><br><span class="line"></span><br><span class="line"># 用标准方程法进行求解</span><br><span class="line">X_b = np.c_[np.ones(100,1),X]</span><br><span class="line">theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; theta_best</span><br><span class="line">array([4.2151],</span><br><span class="line">	[2.7701])</span><br><span class="line"></span><br><span class="line"># 画出拟合直线</span><br><span class="line">X_new = np.array([[0], [2]])</span><br><span class="line">X_new_b = np.c_[np.ones((2,1)),X_new]</span><br><span class="line">y_predict = X_new_b.T.dot(theta_best)</span><br><span class="line">plt.plot(X, y, &apos;b.&apos;)</span><br><span class="line">plt.plot(X_new, y_predic, &apos;r-&apos;)</span><br><span class="line">plt.axis([0, 2, 0, 15])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>也可以用Scikit-Learn实现</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line">lin_reg = LinearRegression()</span><br><span class="line">lin_reg.fit(X,y)</span><br><span class="line">lin_reg.predict(X_new)</span><br></pre></td></tr></table></figure>
<h3 id="1-3-梯度下降求解"><a href="#1-3-梯度下降求解" class="headerlink" title="1.3. 梯度下降求解"></a>1.3. 梯度下降求解</h3><p>应用梯度下降时，需要保证所有特征值的大小比例差不多（比如使用Scikit-Learn的StandardScaler类），否则收敛时间会长很多</p>
<ul>
<li><p><strong>批量梯度下降</strong> (Batch Gradient Descent, BGD)</p>
<p>  成本函数的偏导数</p>
<p>  $$\frac{\partial}{\partial \theta_j}  MSE(\theta)=\frac{2}{m}\sum_{i=1}^m (\theta^TX^{(i)}-y^{(i)})X_j^{(i)}$$</p>
<p>  向量化表示为</p>
<p>  $$\nabla_{\theta}MSE(\theta)=\frac 2m X^T(X\theta-y)$$</p>
<p>  则每一步迭代后得到新的$\theta$为</p>
<p>  $$\theta = \theta - \eta \nabla_{\theta}MSE(\theta)$$</p>
<p>  我们来看看这个算法的快速实现：</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">eta = 0.1 # 学习率</span><br><span class="line">n_iterations = 1000</span><br><span class="line">m = 100</span><br><span class="line"></span><br><span class="line"># 随机初始化</span><br><span class="line">theta = np.random.rand(2,1)</span><br><span class="line"></span><br><span class="line">for iteration in range(n_iterations):</span><br><span class="line">	gradients = 2/m * X_b.T.dot(X_b.dot(theta)-y)</span><br><span class="line">	theta = theta - eta * gradients</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>随机梯度下降</strong> (Stochastic Gradient Descent, SGD)</p>
<p>  批量梯度下降的主要问题是每一次迭代要用到整个训练集，所以如果训练集很大的话，算法会很慢</p>
<p>  随机梯度下降是另外一个极端：每一步迭代只从训练集中随机选择一个样本，并且仅依据这一个样本来计算梯度，由于它的随机性，使得它与梯度下降相比要不规则得多，存在比较明显的震荡，但从整体上来看，还是在慢慢下降的</p>
<p>  当成本函数不规则时，随机梯度下降其实可以帮助算法跳出局部最优，所以相比梯度下降，它对找到全局最优更有优势</p>
<p>  随机性的好处在于可以逃离局部最优，但缺点是永远定位不出最优解，如果要解决这个问题，有一个办法是逐步降低学习率：开始的步长比较大（这有助于快速进展和逃离局部最优），然后越来越小，让算法尽量靠近全局最小值——这个过程叫作<strong>模拟退火</strong></p>
<p>  确定每个迭代学习率的函数叫作<strong>学习计划</strong> (learning schedule)</p>
<p>  用自定义的脚本来执行SGD：</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">n_epochs = 50</span><br><span class="line">t0, t1 = 5, 50 # 学习计划的超参数</span><br><span class="line"></span><br><span class="line">def learning_schedule(t):</span><br><span class="line">	return t0 / (t + t1)</span><br><span class="line"></span><br><span class="line"># 随机初始化</span><br><span class="line">theta = np.random.rand(2,1)</span><br><span class="line"></span><br><span class="line">for epoch in range(n_epochs):</span><br><span class="line">	for i in range(m):</span><br><span class="line">		random_index = np.random.randint(m) # 随机选择一个样本</span><br><span class="line">		xi = X_b[random_index:random_index+1]</span><br><span class="line">		yi = y[random_index:random_index+1]</span><br><span class="line">		gradients = 2/m * xi.T.dot(xi.dot(theta) - yi)</span><br><span class="line">		eta = learning_schedule(epoch * m + i)</span><br><span class="line">		theta = theta - eta * gradients</span><br></pre></td></tr></table></figure>
<p>  在Scikit-Learn里，用SGD执行线性回归可以使用<code>SGDRegression</code>类，其默认优化的成本函数为平方误差</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import SGDRegression</span><br><span class="line"></span><br><span class="line">sgd_reg = SGDRegression(n_iter=50, penalty=None, eta0=o.1)</span><br><span class="line">sgd_reg.fit(X, y.ravel())</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>小批量梯度下降</strong> （Mini-Batch Gradient Descent)</p>
<p>  优点：</p>
<ul>
<li>可以从矩阵运算的硬件优化中获得显著的性能提升；</li>
<li><p>在参数空间的搜索进程不像SGD那样不稳定，所以它会比SGD跟接近最小值；</p>
<p>缺点：</p>
</li>
<li><p>更难从局部最小值中逃脱</p>
</li>
</ul>
</li>
</ul>
<h2 id="2-logistic回归：“达观杯”文本智能处理挑战赛"><a href="#2-logistic回归：“达观杯”文本智能处理挑战赛" class="headerlink" title="2. logistic回归：“达观杯”文本智能处理挑战赛"></a>2. logistic回归：“达观杯”文本智能处理挑战赛</h2><h3 id="2-1-任务描述"><a href="#2-1-任务描述" class="headerlink" title="2.1. 任务描述"></a>2.1. 任务描述</h3><p>具体任务可以到 <a href="http://www.dcjingsai.com/common/cmpt/%E2%80%9C%E8%BE%BE%E8%A7%82%E6%9D%AF%E2%80%9D%E6%96%87%E6%9C%AC%E6%99%BA%E8%83%BD%E5%A4%84%E7%90%86%E6%8C%91%E6%88%98%E8%B5%9B_%E8%B5%9B%E4%BD%93%E4%B8%8E%E6%95%B0%E6%8D%AE.html" target="_blank" rel="noopener">‘达观杯’文本智能处理挑战赛官网</a> 查看</p>
<p><strong>任务</strong>：建立模型通过长文本数据正文(article)，预测文本对应的类别(class)</p>
<p>数据：</p>
<blockquote>
<p>数据包含2个csv文件：</p>
<ul>
<li><p><strong>train_set.csv</strong></p>
<p>  此数据集用于训练模型，每一行对应一篇文章。文章分别在“字”和“词”的级别上做了脱敏处理。共有四列：</p>
<p>  第一列是文章的索引(id)，第二列是文章正文在“字”级别上的表示，即字符相隔正文(article)；第三列是在“词”级别上的表示，即词语相隔正文(word_seg)；第四列是这篇文章的标注(class)。</p>
<p>  注：每一个数字对应一个“字”，或“词”，或“标点符号”。“字”的编号与“词”的编号是独立的！</p>
</li>
<li><p><strong>test_set.csv</strong> </p>
<p>  此数据用于测试。数据格式同train_set.csv，但不包含class</p>
<p>  注：test_set与train_test中文章id的编号是独立的。</p>
</li>
</ul>
</blockquote>
<h3 id="2-2-编程实现"><a href="#2-2-编程实现" class="headerlink" title="2.2. 编程实现"></a>2.2. 编程实现</h3><p>使用<strong>logistic回归</strong>来实现这个多元分类任务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">from sklearn.feature_extract.txt import TfidfVectorizer</span><br><span class="line">from sklearn.decomposition import TruncatedSVD</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.model_select import train_test_split</span><br><span class="line">from sklearn.externals import joblib</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">t_start = time.time()</span><br><span class="line"></span><br><span class="line"># ===========================================================</span><br><span class="line"># 1. 载人数据与数据预处理</span><br><span class="line">df_train = pd.read_csv(&apos;data/train_set.csv&apos;)</span><br><span class="line">df_train = df_train.drop(columns=&apos;article&apos;,inplace=True)</span><br><span class="line">df_test = pd.read_csv(&apos;data/test_set.csv&apos;)</span><br><span class="line">df_test = df_test.drop(columns=&apos;article&apos;,inplace=True)</span><br><span class="line">y_train = (df_train[&apos;class&apos;] - 1).values</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ===========================================================</span><br><span class="line"># 2. 特征工程，这里先使用经典的文本特征提取方法TFIDF，提取的TFIDF特征</span><br><span class="line">vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.9, sublinear_tf=True)</span><br><span class="line">vectorizer.fit(df_train[&apos;word_seg&apos;])</span><br><span class="line">X_train = vectorizer.transform(df_train[&apos;word_seg&apos;])</span><br><span class="line">X_test = vectorizer.transform(df_test[&apos;word_seg&apos;])</span><br><span class="line"></span><br><span class="line"># 可以将得到的TFIDF特征保存至本地</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">data = (X_train,y_train,X_test)</span><br><span class="line">f = open(&apos;data/data_tfidf.pkl&apos;,&apos;wb&apos;)</span><br><span class="line">pickle.dump(data,f)</span><br><span class="line">f.close()</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ===========================================================</span><br><span class="line"># 3. 特征降维，将上一步提取的TFIDF特征使用lsa方法进行特征降维</span><br><span class="line">lsa = TruncatedSVD(n_components=200)</span><br><span class="line">X_train = lsa.transform(X_train)</span><br><span class="line">X_test = lsa.transform(X_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ===========================================================</span><br><span class="line"># 4. 训练分类器</span><br><span class="line"></span><br><span class="line"># 划分训练集和验证集</span><br><span class="line">X_train,X_vali,y_train,y_vali = train_test_split(X_train,y_train,test_size=0.1,random_state=0)</span><br><span class="line"></span><br><span class="line">##  multi_class:分类方式选择参数，有&quot;ovr(默认)&quot;和&quot;multinomial&quot;两个值可选择，在二元逻辑回归中无区别</span><br><span class="line">##  solver:优化算法选择参数，当penalty为&quot;l1&quot;时，参数只能是&quot;liblinear(坐标轴下降法)&quot;；&quot;lbfgs&quot;和&quot;cg&quot;都是关于目标函数的二阶泰勒展开</span><br><span class="line">##  当penalty为&quot;l2&quot;时，参数可以是&quot;lbfgs(拟牛顿法)&quot;,&quot;newton_cg(牛顿法变种)&quot;,&quot;seg(minibactch随机平均梯度下降)&quot;</span><br><span class="line">lr = LogisticRegression(multi_class=&quot;ovr&quot;,penalty=&quot;l2&quot;,solver=&quot;lbfgs&quot;)</span><br><span class="line">lr.fit(X_train,y_train)</span><br><span class="line"></span><br><span class="line"># 模型的保存与持久化</span><br><span class="line">joblib.dump(lr,&quot;logistic_lr.model&quot;)</span><br><span class="line">joblib.load(&quot;logistic_lr.model&quot;) #加载模型,会保存该model文件</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ===========================================================</span><br><span class="line"># 5. 在验证集上评估模型</span><br><span class="line">pre_vali = lr.predict(X_vali)</span><br><span class="line">pre_score = f1_score(y_true=y_vali,y_pred=pre_vali,average=&apos;macro&apos;)</span><br><span class="line">print(&quot;验证集分数：&#123;&#125;&quot;.format(score_vali))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ===========================================================</span><br><span class="line"># 6. 对测试集进行预测</span><br><span class="line">y_test = lr.predict(X_test) + 1</span><br></pre></td></tr></table></figure>
<h3 id="2-3-sklearn包中logistic算法的使用"><a href="#2-3-sklearn包中logistic算法的使用" class="headerlink" title="2.3. sklearn包中logistic算法的使用"></a>2.3. sklearn包中logistic算法的使用</h3><p><img src="/images/InAction-MachineLearning-XGS-sklearn-lr-1.png" alt></p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-lr-2.png" alt></p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-lr-3.png" alt></p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-lr-4.png" alt></p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-lr-5.png" alt></p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-lr-6.png" alt></p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-lr-7.png" alt></p>
<h2 id="3-决策树：鸢尾花数据分类"><a href="#3-决策树：鸢尾花数据分类" class="headerlink" title="3. 决策树：鸢尾花数据分类"></a>3. 决策树：鸢尾花数据分类</h2><h3 id="3-1-DecisionTreeClassifier实例"><a href="#3-1-DecisionTreeClassifier实例" class="headerlink" title="3.1. DecisionTreeClassifier实例"></a>3.1. DecisionTreeClassifier实例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">from itertools import product</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">from sklearn import datasets</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.preprocessing import MinMaxScaler</span><br><span class="line">from sklearn.feature_selection import SelectKBest</span><br><span class="line">from sklearn.feature_selection import chi2</span><br><span class="line">from sklearn.decomposition import PCA</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 使用自带的iris数据</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data[:, np.arange(0,4)]</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"># 划分训练集与测试集</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=14)</span><br><span class="line">print(&quot;训练数据集样本总数:%d;测试数据集样本总数:%d&quot; %(x_train.shape[0],x_test.shape[0]))</span><br><span class="line"></span><br><span class="line"># 对数据集进行标准化</span><br><span class="line">ss = MinMaxScaler()</span><br><span class="line">x_train = ss.fit_transform(x_train,y_train)</span><br><span class="line">x_test = ss.transform(x_test)</span><br><span class="line"></span><br><span class="line"># 特征选择：从已有的特征属性中选择出影响目标最大的特征属性</span><br><span class="line"># 常用方法：</span><br><span class="line">#	离散属性：F统计量、卡方系数、互信息mutual_info_classif</span><br><span class="line">#	连续属性：皮尔逊相关系数、F统计量、互信息mutual_info_classif&#125;</span><br><span class="line"># 这里使用离散属性的卡方系数，实现函数为SelectKBest，用SelectKBest方法从四个原始特征属性中选择出最能影响目标的3个特征属性</span><br><span class="line">ch2 = SelectKBest(chi2,k=3) # k默认为10，指定后会返回想要的特征个数</span><br><span class="line">ch2.fit(x_train,y_train)</span><br><span class="line">x_train = ch2.transform(x_train)</span><br><span class="line">x_test = ch2.transform(x_test)</span><br><span class="line"></span><br><span class="line"># 特征降维，这里使用PCA方法</span><br><span class="line">pca = PCA(n_components=2)   # 构建一个PCA对象，设置最终维度为2维。这里为了后边画图方便，将数据维度设置为 2，一般用默认不设置就可以</span><br><span class="line">x_train = pca.fit_transform(x_train) # 训练与转换，也可以拆分成两步</span><br><span class="line">x_test = pca.transform(x_test)</span><br><span class="line"></span><br><span class="line"># 训练模型</span><br><span class="line">#	criterion：指定特征选择标准，可以使用&quot;gini&quot;或者&quot;entropy&quot;，前者代表基尼系数，后者代表信息增益</span><br><span class="line">#	max_depth：限制树的最大深度4</span><br><span class="line">clf = DecisionTreeClassifier(criterion=&quot;entropy&quot;,max_depth=4)</span><br><span class="line">clf.fit(X, y) # 拟合模型</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 画图</span><br><span class="line">x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line"># 生成网格采样点</span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),</span><br><span class="line">                     np.arange(y_min, y_max, 0.1))</span><br><span class="line"></span><br><span class="line">Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">Z = Z.reshape(xx.shape)</span><br><span class="line"></span><br><span class="line">plt.contourf(xx, yy, Z, alpha=0.4)</span><br><span class="line">plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-decision-tree-1.png" alt></p>
<h3 id="3-2-可视化决策树"><a href="#3-2-可视化决策树" class="headerlink" title="3.2. 可视化决策树"></a>3.2. 可视化决策树</h3><p>scikit-learn中决策树的可视化一般需要安装graphviz，主要包括graphviz的安装和python的graphviz插件的安装</p>
<blockquote>
<ul>
<li><p>1) 安装graphviz。下载地址在：<code>http://www.graphviz.org</code>/。如果你是linux，可以用apt-get或者yum的方法安装。如果是windows，就在官网下载msi文件安装。无论是linux还是windows，装完后都要设置环境变量，将graphviz的bin目录加到PATH，比如我是windows，将<code>C:/Program Files (x86)/Graphviz2.38/bin/</code>加入了<code>PATH</code></p>
</li>
<li><p>2) 安装python插件graphviz： <code>pip install graphviz</code></p>
</li>
</ul>
<ul>
<li>3) 安装python插件pydotplus: <code>pip install pydotplus</code></li>
</ul>
</blockquote>
<p>这样环境就搭好了，有时候python会很笨，仍然找不到graphviz，这时，可以在代码里面加入这一行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.environ[&quot;PATH&quot;] += os.pathsep + &apos;C:/Program Files (x86)/Graphviz2.38/bin/&apos;</span><br></pre></td></tr></table></figure>
<p>可视化决策树的代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from IPython.display import Image  </span><br><span class="line">from sklearn import tree</span><br><span class="line">import pydotplus </span><br><span class="line">dot_data = tree.export_graphviz(clf, out_file=None, </span><br><span class="line">                         feature_names=iris.feature_names,  </span><br><span class="line">                         class_names=iris.target_names,  </span><br><span class="line">                         filled=True, rounded=True,  </span><br><span class="line">                         special_characters=True)  </span><br><span class="line">graph = pydotplus.graph_from_dot_data(dot_data)  </span><br><span class="line">Image(graph.create_png())</span><br></pre></td></tr></table></figure>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-decision-tree-2.png" alt></p>
<h2 id="4-SVM：sklearn中测试数据"><a href="#4-SVM：sklearn中测试数据" class="headerlink" title="4. SVM：sklearn中测试数据"></a>4. SVM：sklearn中测试数据</h2><h3 id="4-1-SVM相关知识点回顾"><a href="#4-1-SVM相关知识点回顾" class="headerlink" title="4.1. SVM相关知识点回顾"></a>4.1. SVM相关知识点回顾</h3><h4 id="4-1-1-SVM与SVR"><a href="#4-1-1-SVM与SVR" class="headerlink" title="4.1.1. SVM与SVR"></a>4.1.1. SVM与SVR</h4><ul>
<li><p><strong>SVM分类算法</strong></p>
<p>  其原始形式是：</p>
<p>  <img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-1.png" alt></p>
<p>  其中m为样本个数，我们的样本为(x<sub>1</sub>,y<sub>1</sub>),(x<sub>2</sub>,y<sub>2</sub>),…,(x<sub>m</sub>,y<sub>m</sub>)。w,b是我们的分离超平面的w∙ϕ(x<sub>i</sub>)+b=0系数, ξ<sub>i</sub>为第i个样本的松弛系数， C为惩罚系数。ϕ(x<sub>i</sub>)为低维到高维的映射函数</p>
<p>  通过拉格朗日函数以及对偶化后的形式为：</p>
<p>  <img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-2.png" alt></p>
</li>
<li><p><strong>SVR回归算法</strong></p>
<p>  <img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-3.png" alt></p>
<p>  其中m为样本个数，我们的样本为(x<sub>1</sub>,y<sub>1</sub>),(x<sub>2</sub>,y<sub>2</sub>),…,(x<sub>m</sub>,y<sub>m</sub>)。w,b是我们的回归超平面的w∙x<sub>i</sub>+b=0系数, ξ<sup>∨</sup><sub>i</sub>，ξ<sup>∧</sup><sub>i</sub>为第i个样本的松弛系数， C为惩罚系数，ϵ为损失边界，到超平面距离小于ϵ的训练集的点没有损失。ϕ(x<sub>i</sub>)为低维到高维的映射函数。</p>
<p>  <img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-4.png" alt></p>
</li>
</ul>
<h4 id="4-1-2-核函数"><a href="#4-1-2-核函数" class="headerlink" title="4.1.2. 核函数"></a>4.1.2. 核函数</h4><p>在scikit-learn中，内置的核函数一共有4种：</p>
<ul>
<li><p><strong>线性核函数</strong>（Linear Kernel）表达式为：K(x,z)=x∙z，就是普通的内积</p>
</li>
<li><p><strong>多项式核函数</strong>（Polynomial Kernel）是线性不可分SVM常用的核函数之一，表达式为：K(x,z)=（γx∙z+r)<sup>d</sup> ，其中，γ,r,d都需要自己调参定义</p>
</li>
<li><p><strong>高斯核函数</strong>（Gaussian Kernel），在SVM中也称为径向基核函数（Radial Basis Function,RBF），它是 libsvm 默认的核函数，当然也是 scikit-learn 默认的核函数。表达式为：K(x,z)=exp(−γ||x−z||<sup>2</sup>)， 其中，γ大于0，需要自己调参定义</p>
</li>
<li><p><strong>Sigmoid核函数</strong>（Sigmoid Kernel）也是线性不可分SVM常用的核函数之一，表达式为：K(x,z)=tanh（γx∙z+r)， 其中，γ，r都需要自己调参定义</p>
</li>
</ul>
<p>一般情况下，对非线性数据使用默认的高斯核函数会有比较好的效果，如果你不是SVM调参高手的话，建议使用高斯核来做数据分析。　</p>
<h3 id="4-2-sklearn中SVM相关库的简介"><a href="#4-2-sklearn中SVM相关库的简介" class="headerlink" title="4.2. sklearn中SVM相关库的简介"></a>4.2. sklearn中SVM相关库的简介</h3><p>scikit-learn SVM算法库封装了libsvm 和 liblinear 的实现，仅仅重写了算法了接口部分</p>
<h4 id="4-2-1-分类库与回归库"><a href="#4-2-1-分类库与回归库" class="headerlink" title="4.2.1. 分类库与回归库"></a>4.2.1. 分类库与回归库</h4><ul>
<li><p><strong>分类算法库</strong></p>
<p>  包括SVC， NuSVC，和LinearSVC 3个类</p>
<p>  对于SVC， NuSVC，和LinearSVC 3个分类的类，SVC和 NuSVC差不多，区别仅仅在于对损失的度量方式不同，而LinearSVC从名字就可以看出，他是线性分类，也就是不支持各种低维到高维的核函数，仅仅支持线性核函数，对线性不可分的数据不能使用</p>
</li>
<li><p><strong>回归算法库</strong></p>
<p>  包括SVR， NuSVR，和LinearSVR 3个类</p>
<p>  同样的，对于SVR， NuSVR，和LinearSVR 3个回归的类， SVR和NuSVR差不多，区别也仅仅在于对损失的度量方式不同。LinearSVR是线性回归，只能使用线性核函数</p>
</li>
</ul>
<h4 id="4-2-2-高斯核调参"><a href="#4-2-2-高斯核调参" class="headerlink" title="4.2.2. 高斯核调参"></a>4.2.2. 高斯核调参</h4><h5 id="4-2-2-1-需要调节的参数"><a href="#4-2-2-1-需要调节的参数" class="headerlink" title="4.2.2.1. 需要调节的参数"></a>4.2.2.1. 需要调节的参数</h5><ul>
<li><p><strong>SVM分类模型</strong></p>
<p>  如果是SVM分类模型，这两个超参数分别是<strong>惩罚系数C</strong>和<strong>RBF核函数的系数γ</strong></p>
<p>  <strong>惩罚系数C</strong></p>
<blockquote>
<p>它在优化函数里主要是平衡支持向量的复杂度和误分类率这两者之间的关系，可以理解为正则化系数</p>
<ul>
<li><p>当C比较大时，我们的损失函数也会越大，这意味着我们不愿意放弃比较远的离群点。这样我们会有更加多的支持向量，也就是说支持向量和超平面的模型也会变得越复杂，也容易过拟合</p>
</li>
<li><p>当C比较小时，意味我们不想理那些离群点，会选择较少的样本来做支持向量，最终的支持向量和超平面的模型也会简单</p>
</li>
</ul>
<p>scikit-learn中默认值是1</p>
</blockquote>
<p>  C越大，泛化能力越差，易出现过拟合现象；C越小，泛化能力越好，易出现过欠拟合现象</p>
<p>  <strong>BF核函数的参数γ</strong></p>
<blockquote>
<p>RBF 核函数K(x,z)=exp(−γ||x−z||<sup>2</sup>) γ&gt;0</p>
<p>γ主要定义了单个样本对整个分类超平面的影响</p>
<ul>
<li><p>当γ比较小时，单个样本对整个分类超平面的影响比较小，不容易被选择为支持向量</p>
</li>
<li><p>当γ比较大时，单个样本对整个分类超平面的影响比较大，更容易被选择为支持向量，或者说整个模型的支持向量也会多</p>
</li>
</ul>
<p>scikit-learn中默认值是 <code>1/样本特征数</code></p>
</blockquote>
<p>  γ越大，训练集拟合越好，泛化能力越差，易出现过拟合现象</p>
<p>  如果把惩罚系数C和RBF核函数的系数γ一起看，当C比较大， γ比较大时，我们会有更多的支持向量，我们的模型会比较复杂，容易过拟合一些。如果C比较小 ， γ比较小时，模型会变得简单，支持向量的个数会少</p>
</li>
<li><p><strong>SVM回归模型</strong></p>
<p>  SVM回归模型的RBF核比分类模型要复杂一点，因为此时我们除了惩罚系数C和RBF核函数的系数γ之外，还多了一个<strong>损失距离度量ϵ</strong></p>
<blockquote>
<p>对于损失距离度量ϵ，它决定了样本点到超平面的距离损失</p>
<ul>
<li><p>当 ϵ 比较大时，损失较小，更多的点在损失距离范围之内，而没有损失,模型较简单</p>
</li>
<li><p>当 ϵ 比较小时，损失函数会较大，模型也会变得复杂</p>
</li>
</ul>
<p>scikit-learn中默认值是0.1</p>
</blockquote>
<p>  如果把惩罚系数C，RBF核函数的系数γ和损失距离度量ϵ一起看，当C比较大， γ比较大，ϵ比较小时，我们会有更多的支持向量，我们的模型会比较复杂，容易过拟合一些。如果C比较小 ， γ比较小，ϵ比较大时，模型会变得简单，支持向量的个数会少</p>
</li>
</ul>
<h5 id="4-2-2-2-调参方法：网格搜索"><a href="#4-2-2-2-调参方法：网格搜索" class="headerlink" title="4.2.2.2. 调参方法：网格搜索"></a>4.2.2.2. 调参方法：网格搜索</h5><p>对于SVM的RBF核，我们主要的调参方法都是交叉验证。具体在scikit-learn中，主要是使用网格搜索，即GridSearchCV类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import GridSearchCV</span><br><span class="line">grid = GridSearchCV(SVC(), param_grid=&#123;&quot;C&quot;:[0.1, 1, 10], &quot;gamma&quot;: [1, 0.1, 0.01]&#125;, cv=4)</span><br><span class="line">grid.fit(X, y)</span><br></pre></td></tr></table></figure>
<p>将GridSearchCV类用于SVM RBF调参时要注意的参数有：</p>
<blockquote>
<p>1) <strong>estimator</strong>：即我们的模型，此处我们就是带高斯核的SVC或者SVR</p>
<p>2) <strong>param_grid</strong>：即我们要调参的参数列表。 比如我们用SVC分类模型的话，那么param_grid可以定义为{“C”:[0.1, 1, 10], “gamma”: [0.1, 0.2, 0.3]}，这样我们就会有9种超参数的组合来进行网格搜索，选择一个拟合分数最好的超平面系数</p>
<p>3) <strong>cv</strong>：S折交叉验证的折数，即将训练集分成多少份来进行交叉验证。默认是3。如果样本较多的话，可以适度增大cv的值</p>
</blockquote>
<h3 id="4-3-编程实现"><a href="#4-3-编程实现" class="headerlink" title="4.3. 编程实现"></a>4.3. 编程实现</h3><ol>
<li><p>生成测试数据</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import make_circles</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line"></span><br><span class="line"># 生成一些随机数据用于后续分类</span><br><span class="line">X, y = make_circles(noise=0.2, factor=0.5, random_state=1) # 生成时加入了一些噪声</span><br><span class="line">X = StandardScaler().fit_transform(X) # 把数据归一化</span><br></pre></td></tr></table></figure>
<p> 生成的随机数据可视化结果如下：</p>
<p> <img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-5.png" alt></p>
</li>
<li><p>调参</p>
<p> 接着采用网格搜索的策略进行RBF核函数参数搜索</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import GridSearchCV</span><br><span class="line">grid = GridSearchCV(SVC(), param_grid=&#123;&quot;C&quot;:[0.1, 1, 10], &quot;gamma&quot;: [1, 0.1, 0.01]&#125;, cv=4) # 总共有9种参数组合的搜索空间</span><br><span class="line">grid.fit(X, y)</span><br><span class="line">print(&quot;The best parameters are %s with a score of %0.2f&quot;</span><br><span class="line">      % (grid.best_params_, grid.best_score_))</span><br><span class="line"></span><br><span class="line">输出为：</span><br><span class="line">The best parameters are &#123;&apos;C&apos;: 10, &apos;gamma&apos;: 0.1&#125; with a score of 0.91</span><br></pre></td></tr></table></figure>
<p> 可以对9种参数组合训练的结果进行可视化，观察分类的效果：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max,0.02),</span><br><span class="line">                     np.arange(y_min, y_max, 0.02))</span><br><span class="line"></span><br><span class="line">for i, C in enumerate((0.1, 1, 10)):</span><br><span class="line">    for j, gamma in enumerate((1, 0.1, 0.01)):</span><br><span class="line">        plt.subplot()       </span><br><span class="line">        clf = SVC(C=C, gamma=gamma)</span><br><span class="line">        clf.fit(X,y)</span><br><span class="line">        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line"></span><br><span class="line">        # Put the result into a color plot</span><br><span class="line">        Z = Z.reshape(xx.shape)</span><br><span class="line">        plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)</span><br><span class="line"></span><br><span class="line">        # Plot also the training points</span><br><span class="line">        plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)</span><br><span class="line"></span><br><span class="line">        plt.xlim(xx.min(), xx.max())</span><br><span class="line">        plt.ylim(yy.min(), yy.max())</span><br><span class="line">        plt.xticks(())</span><br><span class="line">        plt.yticks(())</span><br><span class="line">        plt.xlabel(&quot; gamma=&quot; + str(gamma) + &quot; C=&quot; + str(C))</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure>
</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align:center"><code>C \ gamma</code></th>
<th style="text-align:center">1</th>
<th style="text-align:center">0.1</th>
<th style="text-align:center">0.001</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0.1</td>
<td style="text-align:center"><img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-6.png" alt></td>
<td style="text-align:center"><img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-7.png" alt></td>
<td style="text-align:center"><img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-8.png" alt></td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center"><img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-9.png" alt></td>
<td style="text-align:center"><img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-10.png" alt></td>
<td style="text-align:center"><img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-11.png" alt></td>
</tr>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center"><img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-12.png" alt></td>
<td style="text-align:center"><img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-13.png" alt></td>
<td style="text-align:center"><img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-14.png" alt></td>
</tr>
</tbody>
</table>
<h2 id="5-朴素贝叶斯"><a href="#5-朴素贝叶斯" class="headerlink" title="5.朴素贝叶斯"></a>5.朴素贝叶斯</h2><h3 id="5-1-朴素贝叶斯相关知识点回顾"><a href="#5-1-朴素贝叶斯相关知识点回顾" class="headerlink" title="5.1. 朴素贝叶斯相关知识点回顾"></a>5.1. 朴素贝叶斯相关知识点回顾</h3><h4 id="5-1-1-什么是朴素贝叶斯分类器"><a href="#5-1-1-什么是朴素贝叶斯分类器" class="headerlink" title="5.1.1. 什么是朴素贝叶斯分类器"></a>5.1.1. 什么是朴素贝叶斯分类器</h4><p>判别式模型（discriminative models)：像决策树、BP神经网络、支持向量机等，都可以归入判别式模型，它们都是直接学习出输出Y与特征X的关系，如：</p>
<ul>
<li>决策函数 Y=f(X)</li>
<li>条件概率 P(Y|X)</li>
</ul>
<p>生成式模型 (gernerative models)：先对联合概率分布 P(X,Y) 进行建模，然后再由此获得P(Y|X) = P(X,Y)/P(X)</p>
<p>贝叶斯学派的思想：</p>
<blockquote>
<p>贝叶斯学派的思想可以概括为先验概率+数据=后验概率。也就是说我们在实际问题中需要得到的后验概率，可以通过先验概率和数据一起综合得到。数据大家好理解，被频率学派攻击的是先验概率，一般来说先验概率就是我们对于数据所在领域的历史经验，但是这个经验常常难以量化或者模型化，于是贝叶斯学派大胆的假设先验分布的模型，比如正态分布，beta分布等。这个假设一般没有特定的依据，因此一直被频率学派认为很荒谬。虽然难以从严密的数学逻辑里推出贝叶斯学派的逻辑，但是在很多实际应用中，贝叶斯理论很好用，比如垃圾邮件分类，文本分类</p>
</blockquote>
<p>如何对P(X,Y)进行建模？</p>
<blockquote>
<p>假如我们的分类模型样本是：</p>
<p>(x<sup>(1)</sup><sub>1</sub>,x<sup>(1)</sup><sub>2</sub>,…x<sup>(1)</sup><sub>n</sub>,y<sub>1</sub>), (x<sup>(2)</sup><sub>1</sub>,x<sup>(2)</sup><sub>2</sub>,…x<sup>(2)</sup><sub>n</sub>,y<sub>2</sub>),…(x<sup>(m)</sup><sub>1</sub>,x<sup>(m)</sup><sub>2</sub>, …, x<sup>(m)</sup><sub>n</sub>,y<sub>m</sub>)</p>
<p>则</p>
<p>P(X, Y) = P(Y) * P( X = (x<sub>1</sub>, x<sub>2</sub>, …, x<sub>n</sub>) | Y ) </p>
<p>其中</p>
<p>P( X = (x<sub>1</sub>, x<sub>2</sub>, …, x<sub>n</sub>) | Y )  =  P( x<sub>1</sub> | Y) * P( x<sub>2</sub> | Y, x<sub>1</sub>) * … P( x<sub>n</sub> | Y, x<sub>1</sub>, … , x<sub>n-1</sub>)</p>
</blockquote>
<p>这是一个超级复杂的有n个维度的条件分布，很难求出</p>
<p>朴素贝叶斯模型在这里做了一个大胆的假设，即X的n个维度之间相互独立，这样就可以得出：</p>
<p>P( X = (x<sub>1</sub>, x<sub>2</sub>, …, x<sub>n</sub>) | Y ) = P( x<sub>1</sub> | Y) * P( x<sub>2</sub> | Y) * … * P( x<sub>n</sub> | Y) </p>
<h4 id="5-1-2-朴素贝叶斯推断"><a href="#5-1-2-朴素贝叶斯推断" class="headerlink" title="5.1.2. 朴素贝叶斯推断"></a>5.1.2. 朴素贝叶斯推断</h4><p><img src="/images/InAction-MachineLearning-XGS-sklearn-naive-bayes-1.png" alt></p>
<h4 id="5-1-3-朴素贝叶斯学习"><a href="#5-1-3-朴素贝叶斯学习" class="headerlink" title="5.1.3. 朴素贝叶斯学习"></a>5.1.3. 朴素贝叶斯学习</h4><p>需要从训练样本中学习到以下两个参数：</p>
<ul>
<li><p><strong>先验概率</strong> P(c)</p>
<p>  P(c)表示了样本空间中各类样本所占的比例</p>
<p>  根据大数定律，当训练集中包含充足的独立同分布样本时，P(c)可根据各类样本出现的频率来估计</p>
<p>  <img src="/images/InAction-MachineLearning-XGS-sklearn-naive-bayes-2.png" alt></p>
</li>
<li><p><strong>类条件概率</strong>（又称为似然） P(x<sub>i</sub> | c)</p>
<p>  （1）如果 x<sub>i</sub> 是离散的，可以假设 x<sub>i</sub>符合多项式分布，这样得到 P(x<sub>i</sub> | c) 是在样本类别 c 中，特征 x<sub>i</sub> 出现的频率</p>
<p>  <img src="/images/InAction-MachineLearning-XGS-sklearn-naive-bayes-3.png" alt></p>
<p>  （2）如果 x<sub>i</sub> 是连续属性，可以假设 P(x<sub>i</sub> | c) ~ N( μ<sub>c,i</sub> , σ<sup>2</sup><sub>c,i</sub> )</p>
<p>  <img src="/images/InAction-MachineLearning-XGS-sklearn-naive-bayes-4.png" alt></p>
</li>
</ul>
<h3 id="5-2-sklearn中朴素贝叶斯类库的简介"><a href="#5-2-sklearn中朴素贝叶斯类库的简介" class="headerlink" title="5.2. sklearn中朴素贝叶斯类库的简介"></a>5.2. sklearn中朴素贝叶斯类库的简介</h3><p>在scikit-learn中，一共有3个朴素贝叶斯的分类算法类</p>
<blockquote>
<ul>
<li><p>GaussianNB：先验为高斯分布的朴素贝叶斯</p>
</li>
<li><p>MultinomialNB：先验为多项式分布的朴素贝叶斯</p>
</li>
<li><p>BernoulliNB：先验为伯努利分布的朴素贝叶斯</p>
</li>
</ul>
</blockquote>
<p>这三个类适用的分类场景各不相同</p>
<blockquote>
<p>一般来说，如果样本特征的分布大部分是<strong>连续值</strong>，使用GaussianNB会比较好</p>
<p>如果如果样本特征的分大部分是<strong>多元离散值</strong>，使用MultinomialNB比较合适</p>
<p>如果样本特征是<strong>二元离散值</strong>或者<strong>很稀疏的多元离散值</strong>，应该使用BernoulliNB。</p>
</blockquote>
<h4 id="5-2-1-GaussianNB类"><a href="#5-2-1-GaussianNB类" class="headerlink" title="5.2.1. GaussianNB类"></a>5.2.1. GaussianNB类</h4><p>GaussianNB类的主要参数仅有一个，即先验概率priors</p>
<p>这个值默认不给出，如果不给出此时P(Y=c)= m<sub>c</sub> / m，如果给出的话就以priors 为准</p>
<p>在使用GaussianNB的fit方法拟合数据后，我们可以进行预测。此时预测有三种方法</p>
<blockquote>
<ul>
<li><p>predict方法：就是我们最常用的预测方法，直接给出测试集的预测类别输出；</p>
</li>
<li><p>predict_proba方法：给出测试集样本在各个类别上预测的概率；</p>
</li>
<li><p>predict_log_proba方法：和predict_proba类似，它会给出测试集样本在各个类别上预测的概率的一个对数转化；</p>
</li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.naive_bayes import GaussianNB</span><br><span class="line"></span><br><span class="line">clf = GaussianNB()</span><br><span class="line">#拟合数据</span><br><span class="line">clf.fit(X, Y)</span><br><span class="line"></span><br><span class="line">#进行预测</span><br><span class="line">clf.predict([[-0.8, -1]])</span><br></pre></td></tr></table></figure>
<h4 id="5-2-2-MultinomialNB类"><a href="#5-2-2-MultinomialNB类" class="headerlink" title="5.2.2. MultinomialNB类"></a>5.2.2. MultinomialNB类</h4><p>MultinomialNB假设特征的先验概率为多项式分布，即如下式：</p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-naive-bayes-5.png" alt></p>
<p>MultinomialNB参数比GaussianNB多，但是一共也只有仅仅3个</p>
<ul>
<li><p><strong>参数alpha</strong>：为上面的常数λ。如果你没有特别的需要，用默认的1即可。如果发现拟合的不好，需要调优时，可以选择稍大于1或者稍小于1的数</p>
</li>
<li><p><strong>参数fit_prior</strong>：是否要考虑先验概率，如果是false，则所有的样本类别输出都有相同的类别先验概率；否则可以自己用第三个参数class_prior输入先验概率，或者不输入第三个参数class_prior让MultinomialNB自己从训练集样本来计算先验概率</p>
</li>
<li><p><strong>参数class_prior</strong>：输入先验概率，若不输入第三个参数class_prior让MultinomialNB自己从训练集样本来计算先验概率</p>
</li>
</ul>
<h4 id="5-2-3-BernoulliNB类"><a href="#5-2-3-BernoulliNB类" class="headerlink" title="5.2.3. BernoulliNB类"></a>5.2.3. BernoulliNB类</h4><p><img src="/images/InAction-MachineLearning-XGS-sklearn-naive-bayes-6.png" alt></p>
<p>其中，x <sub>i</sub> 只能取0或1</p>
<p>BernoulliNB一共有4个参数，其中3个参数的名字和意义和MultinomialNB完全相同</p>
<p>唯一增加的一个参数是binarize，这个参数主要是用来帮BernoulliNB处理二项分布的。如果不输入，则BernoulliNB认为每个数据特征都已经是二元的。否则的话，小于binarize的会归为一类，大于binarize的会归为另外一类</p>
<h2 id="6-神经网络（sklearn）：手写数字识别"><a href="#6-神经网络（sklearn）：手写数字识别" class="headerlink" title="6. 神经网络（sklearn）：手写数字识别"></a>6. 神经网络（sklearn）：手写数字识别</h2><h3 id="5-1-神经网络相关知识点回顾"><a href="#5-1-神经网络相关知识点回顾" class="headerlink" title="5.1. 神经网络相关知识点回顾"></a>5.1. 神经网络相关知识点回顾</h3><h4 id="6-1-1-感知机：神经网络的最小单元"><a href="#6-1-1-感知机：神经网络的最小单元" class="headerlink" title="6.1.1. 感知机：神经网络的最小单元"></a>6.1.1. 感知机：神经网络的最小单元</h4><p>神经网络中最基本的成分是神经元（neuron）模型，最常见的模型是M-P神经元模型，也可以称为感知机（Perceptron）</p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-1.png" alt></p>
<p>若对生物学有一点基础的了解，就可以知道上面构造的简化的M-P神经元模型几乎完全吻合生物学上的定义：</p>
<blockquote>
<p>当前神经元通过与上游神经元之间的突触结构，接受来自上游的神经元1到n传递过来的信号x<sub>1</sub>,x<sub>2</sub>…x<sub>n</sub>，由于不同的上游信号传递过来它们的影响程度是不一样的，因此对它们进行加权求和，即∑w<sub>i</sub>x<sub>i</sub>，则得到总的上游信号</p>
<p>另外当前神经元还有一个激活阈值θ，若上游信号的汇总信号强度大于θ，即则∑w<sub>i</sub>x<sub>i</sub>-θ &gt; 0，则当前神经元被激活，处于兴奋状态；若若上游信号的汇总信号强度小于θ，即则∑w<sub>i</sub>x<sub>i</sub>-θ &lt; 0，则当前神经元未被激活，处于抑制状态</p>
</blockquote>
<p>对于生物学上的神经元来说，它只有两个状态：</p>
<ul>
<li>“1”：对应神经元兴奋；</li>
<li>“0”：对应神经元抑制；</li>
</ul>
<p>则它接受的信号为上游神经元的输出为 x<sub>i</sub>∈{0,1}，它的输出信号为 y∈{0,1}</p>
<p>则它的理想激活函数是阶跃函数，即<code>f(x)=sgn(x)</code></p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-2.png" alt></p>
<p>然而，阶跃函数具有不连续、不光滑等不太好的性质，因此实际上常用sigmoid函数作为激活函数</p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-3.png" alt></p>
<p>则此时激活函数为f(x)=sigmoid(∑w<sub>i</sub>x<sub>i</sub>-θ），很明显若选择sigmod函数作为感知机的激活函数，则此时感知机等价于一个logistic回归分类器</p>
<p>类比logistic回归分类器</p>
<blockquote>
<ul>
<li><p>对于线性可分的分类任务，感知机完全等价于logistic回归分类器</p>
</li>
<li><p>对于线性不可分问题，logistic回归可以构造额外的高阶多项式：</p>
<p>  <img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-4.png" alt></p>
<p>  <img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-5.png" alt></p>
<p>  本质上是将原始特征空间中线性不可分的样本向高维特征空间进行映射，使得它们在新的高维特征空间中线性可分</p>
<p>  而感知器无法进行高维空间的映射，只能基于原始特征空间寻找线性判别边界，因此感知机在面对简单的非线性可分问题是，往往无法得到理想的判别面</p>
</li>
</ul>
</blockquote>
<p>感知机的学习规则：</p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-6.png" alt></p>
<h4 id="6-1-2-神经网络的学习：BackPropagation算法"><a href="#6-1-2-神经网络的学习：BackPropagation算法" class="headerlink" title="6.1.2. 神经网络的学习：BackPropagation算法"></a>6.1.2. 神经网络的学习：BackPropagation算法</h4><p>神经网络的学习算法本质上是数学上常用的梯度下降（Gradient Descend）算法</p>
<p>依据在一轮神经网络的训练过程中所用到的训练样本的数量的不同，可分为以下两类：</p>
<blockquote>
<ul>
<li><p>标准BP算法：每次网络训练只针对单个训练样本，一次训练就输入一个训练样本，更新一次网络参数；</p>
</li>
<li><p>累积BP算法：每次网络训练只针对所有训练样本，一次将所有训练样本输入，更新一次网络参数；</p>
</li>
</ul>
</blockquote>
<h4 id="6-1-2-1-标准BP算法"><a href="#6-1-2-1-标准BP算法" class="headerlink" title="6.1.2.1. 标准BP算法"></a>6.1.2.1. 标准BP算法</h4><p><img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-7.jpg" alt></p>
<p>以w<sub>h,j</sub>为例进行推导</p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-8.png" alt></p>
<p>在标准BP算法中，由于只对输入的一个训练样本进行网络参数的训练，因此它的目标函数为当前样本k的均方误差E<sub>k</sub></p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-9.png" alt></p>
<p>此时关键在于怎么推出△w<sub>hj</sub>的表达式？</p>
<p>根据链式求导法则，W<sub>hj</sub>的影响链条为：</p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-10.png" alt></p>
<p>因此△w<sub>hj</sub>可以写成：</p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-11.png" alt></p>
<p>中间推导过程省略，最后得到的△w<sub>hj</sub>的表达式为：</p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-12.png" alt></p>
<h4 id="6-1-2-2-累积BP算法"><a href="#6-1-2-2-累积BP算法" class="headerlink" title="6.1.2.2. 累积BP算法"></a>6.1.2.2. 累积BP算法</h4><p>累积BP算法的目标是最小化训练集的累积误差：</p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-13.png" alt></p>
<p>这是累积BP算法与标准BP算法的最大的也是最本质的差别</p>
<p>标准BP算法存在的问题：</p>
<blockquote>
<p>每次更新只针对单个训练样本，参数更新得非常频繁，而且不同样本训练的效果可能出现相互“抵消”的现象</p>
<p>因此为了达到与累积误差相同的极小点，标准BP算法往往需要进行更多次的迭代</p>
</blockquote>
<p>累积BP算法的优缺点：</p>
<blockquote>
<ul>
<li><p>优点：直接对累积误差最小化，它在读取整个训练样本集D后才对参数进行一次更新，其参数更新的频率低得多</p>
</li>
<li><p>缺点：累积误差下降到一点程度之后，进一步下降会非常缓慢</p>
<p>  此时标准BP算法往往会更快得到较好的解，因为其解的震荡性给它带来了一个好处——容易跳出局部最优</p>
</li>
</ul>
</blockquote>
<h3 id="6-2-sklearn中神经网络库的简介"><a href="#6-2-sklearn中神经网络库的简介" class="headerlink" title="6.2. sklearn中神经网络库的简介"></a>6.2. sklearn中神经网络库的简介</h3><p>sklearn中神经网络的实现不适用于大规模数据应用。特别是 scikit-learn 不支持 GPU</p>
<h4 id="6-2-1-MLPClassifier"><a href="#6-2-1-MLPClassifier" class="headerlink" title="6.2.1. MLPClassifier"></a>6.2.1. MLPClassifier</h4><p>神经网络又称为多层感知机（Multi-layer Perceptron，MLP）</p>
<p>sklearn中用MLP进行分类的类为MLPClassifier</p>
<p>主要参数说明：</p>
<table>
<thead>
<tr>
<th style="text-align:left">参数</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">hidden_​​layer_sizes</td>
<td style="text-align:left">tuple，length = n_layers - 2，默认值（100，）第i个元素表示第i个隐藏层中的神经元数量。</td>
</tr>
<tr>
<td style="text-align:left">activation</td>
<td style="text-align:left">{‘identity’，‘logistic’，‘tanh’，‘relu’}，默认’relu’ 隐藏层的激活函数：‘identity’，无操作激活，对实现线性瓶颈很有用，返回f（x）= x；‘logistic’，logistic sigmoid函数，返回f（x）= 1 /（1 + exp（-x））；‘tanh’，双曲tan函数，返回f（x）= tanh（x）；‘relu’，整流后的线性单位函数，返回f（x）= max（0，x）</td>
</tr>
<tr>
<td style="text-align:left">slover</td>
<td style="text-align:left">{‘lbfgs’，‘sgd’，‘adam’}，默认’adam’。权重优化的求解器：’lbfgs’是准牛顿方法族的优化器；’sgd’指的是随机梯度下降。’adam’是指由Kingma，Diederik和Jimmy Ba提出的基于随机梯度的优化器。注意：默认解算器“adam”在相对较大的数据集（包含数千个训练样本或更多）方面在训练时间和验证分数方面都能很好地工作。但是，对于小型数据集，“lbfgs”可以更快地收敛并且表现更好。</td>
</tr>
<tr>
<td style="text-align:left">alpha</td>
<td style="text-align:left">float，可选，默认为0.0001。L2惩罚（正则化项）参数。</td>
</tr>
<tr>
<td style="text-align:left">batch_size</td>
<td style="text-align:left">int，optional，默认’auto’。用于随机优化器的minibatch的大小。如果slover是’lbfgs’，则分类器将不使用minibatch。设置为“auto”时，batch_size = min（200，n_samples）</td>
</tr>
<tr>
<td style="text-align:left">learning_rate</td>
<td style="text-align:left">{‘常数’，‘invscaling’，‘自适应’}，默认’常数”。 用于权重更新。仅在solver =’sgd’时使用。’constant’是’learning_rate_init’给出的恒定学习率；’invscaling’使用’power_t’的逆缩放指数在每个时间步’t’逐渐降低学习速率learning_rate_， effective_learning_rate = learning_rate_init / pow（t，power_t）；只要训练损失不断减少，“adaptive”将学习速率保持为“learning_rate_init”。每当两个连续的时期未能将训练损失减少至少tol，或者如果’early_stopping’开启则未能将验证分数增加至少tol，则将当前学习速率除以5。</td>
</tr>
</tbody>
</table>
<p>MLPClassifier的训练使用BP算法，其使用<strong>交叉熵损失函数</strong>（Cross-Entropy loss function）</p>
<p>MLP的训练需要准备</p>
<blockquote>
<ul>
<li><p>X<sub>m*n</sub>：m个训练样本的n维特征向量构成的特征矩阵</p>
</li>
<li><p>Y<sub>m</sub>：m训练样本的目标值组成的向量</p>
</li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.neural_network import MLPClassifier</span><br><span class="line"></span><br><span class="line">X = [[0., 0.], [1., 1.]]</span><br><span class="line">y = [0, 1]</span><br><span class="line"></span><br><span class="line"># 模型训练</span><br><span class="line">clf = MLPClassifier(solver=&apos;lbfgs&apos;, alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line"></span><br><span class="line"># 模型预测</span><br><span class="line">clf.predict([[2., 2.], [-1., -2.]])</span><br></pre></td></tr></table></figure>
<p>以上训练好的MLP模型保存在clf对象中，该对象有以下两个子变量：</p>
<blockquote>
<ul>
<li><p><code>clf.coefs_</code>：模型的一系列权重矩阵</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; 	# 用以下命令查看每一层的权重矩阵的维度</span><br><span class="line">&gt; 	&gt;&gt;[coef.shape for coef in clf.coefs_]</span><br><span class="line">&gt; 	[(2, 5), (5, 2), (2, 1)]</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</blockquote>
<blockquote>
<pre><code>其中下标为 i 的权重矩阵表示第 i 层和第 i+1 层之间的权重
</code></pre><ul>
<li><p><code>intercepts_</code>：一系列偏置向量</p>
<p>  其中的下标为 i 的向量表示添加到第 i+1 层的偏置值</p>
</li>
</ul>
</blockquote>
<ul>
<li><p><strong>多分类任务</strong></p>
<p>  MLPClassifier 通过应用 Softmax 作为输出函数来支持多分类</p>
<p>  softmax回归本质上是将原先常见的二元分类任务神经网络的输出层采用的sigmoid激活函数换成softmax回归</p>
<p>  例如要利用神经网络进行k类的分类，则神经网络结构如下：</p>
<p>  <img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-14.png" alt></p>
<p>  那么最后一层即网络的输出层所采用的激活函数——softmax回归到底长什么样呢？</p>
<p>  <img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-15.png" alt></p>
<p>  因此该网络的输出层又被称为softmax layer</p>
<p>  因此在提供训练集时，若总共有k的类别，当某一个训练样本的类别为第i类时，它的目标值应该为<code>[0, 0, ..., 1, ..., 0]</code>，只在向量的第i个位置标为1，其他位置都为0</p>
</li>
<li><p><strong>多标签分类任务</strong></p>
<p>  一个样本可能属于多个类别。 对于每个类，原始输出经过 logistic 函数变换后，大于或等于 0.5 的值将进为 1，否则为 0。 对于样本的预测输出，值为 1 的索引位置表示该样本的分类类别</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; X = [[0., 0.], [1., 1.]]</span><br><span class="line">&gt;&gt;&gt; y = [[0, 1], [1, 1]]</span><br><span class="line">&gt;&gt;&gt; clf = MLPClassifier(solver=&apos;lbfgs&apos;, alpha=1e-5,</span><br><span class="line">...                     hidden_layer_sizes=(15,), random_state=1)</span><br><span class="line">...</span><br><span class="line">&gt;&gt;&gt; clf.fit(X, y) </span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; clf.predict([[1., 2.]])</span><br><span class="line">array([[1, 1]])</span><br><span class="line">&gt;&gt;&gt; clf.predict([[0., 0.]])</span><br><span class="line">array([[0, 1]])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="6-3-编程实现"><a href="#6-3-编程实现" class="headerlink" title="6.3. 编程实现"></a>6.3. 编程实现</h3><p>对于神经网络，要说它的Hello world，莫过于识别手写数字了</p>
<p>首先要获取实验数据，下载地址：<a href="http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz" target="_blank" rel="noopener">http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz</a></p>
<p>数组中的每一行都表示一个灰度图像。每个元素都表示像素所对应的灰度值。</p>
<p>数据集中将数据分成3类：训练集，验证集，测试集。一般情况下会将训练数据分为训练集和测试集</p>
<p>为什么要将训练数据分为训练集和测试集？</p>
<blockquote>
<p>将原始数据分开，保证用于测试的数据是训练模型时从未遇到的数据可以使测试更客观。否则就像学习教课书的知识，又只考教课书的知识，就算不理解记下了就能得高分但遇到新问题就傻眼了。</p>
<p>好一点的做法就是用训练集当课本给他上课，先找出把课本知识掌握好的人，再参加由新题组成的月考即测试集，若是还是得分高，那就是真懂不是死记硬背了。</p>
<p>但这样选出来的模型实际是还是用训练集和测试集共同得到的，再进一步，用训练集和验证集反复训练和检测，得到最好的模型，再用测试集来一局定输赢即期末考试，这样选出来的就更好了。</p>
</blockquote>
<p>本实验中为了方便将训练集与验证集合并</p>
<ol>
<li><p>先载入数据，并简单查看一下数据</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.neural_network import MLPClassifier</span><br><span class="line">import numpy as np</span><br><span class="line">import pickle</span><br><span class="line">import gzip</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"># 加载数据</span><br><span class="line">with gzip.open(r&quot;mnist.pkl.gz&quot;) as fp:</span><br><span class="line">    training_data, valid_data, test_data = pickle.load(fp)</span><br><span class="line"></span><br><span class="line">X_training_data, y_training_data = training_data</span><br><span class="line">X_valid_data, y_valid_data = valid_data</span><br><span class="line">X_test_data, y_test_data = test_data</span><br><span class="line"></span><br><span class="line"># 合并训练集,验证集</span><br><span class="line">X_training = np.vstack((X_training_data, X_valid_data))</span><br><span class="line">   y_training = np.append(y_training_data, y_valid_data)</span><br><span class="line"></span><br><span class="line">def show_data_struct():</span><br><span class="line">    print X_training_data.shape, y_training_data.shape</span><br><span class="line">    print X_valid_data.shape, y_valid_data.shape</span><br><span class="line">    print X_test_data.shape, y_test_data.shape</span><br><span class="line">    print X_training_data[0]</span><br><span class="line">    print y_training_data[0]</span><br><span class="line"></span><br><span class="line">show_data_struct()</span><br></pre></td></tr></table></figure>
</li>
<li><p>随便看几张训练图片</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def show_image():</span><br><span class="line">    plt.figure(1)</span><br><span class="line">    for i in range(10):</span><br><span class="line">        image = X_training[i]	# 得到包含第i张图的像素向量，为1*768</span><br><span class="line">        pixels = image.reshape((28, 28)) # 将原始像素向量转换为28*28的像素矩阵</span><br><span class="line">        plt.subplot(5,2,i+1)</span><br><span class="line">        plt.imshow(pixels, cmap=&apos;gray&apos;)</span><br><span class="line">        plt.title(y_training[i])</span><br><span class="line">        plt.axis(&apos;off&apos;)</span><br><span class="line">    plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.45,</span><br><span class="line">                        wspace=0.85)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">show_image()</span><br></pre></td></tr></table></figure>
<p> <img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-16.png" alt></p>
</li>
<li><p>训练模型</p>
<p> 为了获得比默认参数更佳的模型，我们采用网格搜索法搜索更优的训练超参数，使用gridSearchCV实现，上文<a href="#4-2-2-2-调参方法：网格搜索">4.2.2.2. 调参方法：网格搜索</a>中有较为详细的说明</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import GridSearchCV</span><br><span class="line"></span><br><span class="line">mlp = MLPClassifier()</span><br><span class="line">mlp_clf__tuned_parameters = &#123;&quot;hidden_layer_sizes&quot;: [(100,), (100, 30)],</span><br><span class="line">                                 &quot;solver&quot;: [&apos;adam&apos;, &apos;sgd&apos;, &apos;lbfgs&apos;],</span><br><span class="line">                                 &quot;max_iter&quot;: [20],</span><br><span class="line">                                 &quot;verbose&quot;: [True]</span><br><span class="line">                                 &#125;</span><br><span class="line">estimator = GridSearchCV(mlp, mlp_clf__tuned_parameters, n_jobs=6)</span><br><span class="line">estimator.fit(X_training, y_training)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="7-特征选择-特征降维：sklearn包中feature-selection模块的使用"><a href="#7-特征选择-特征降维：sklearn包中feature-selection模块的使用" class="headerlink" title="7. 特征选择/特征降维：sklearn包中feature_selection模块的使用"></a>7. 特征选择/特征降维：sklearn包中feature_selection模块的使用</h2><h3 id="6-1-相关知识回顾"><a href="#6-1-相关知识回顾" class="headerlink" title="6.1. 相关知识回顾"></a>6.1. 相关知识回顾</h3><h4 id="7-1-1-特征降维"><a href="#7-1-1-特征降维" class="headerlink" title="7.1.1. 特征降维"></a>7.1.1. 特征降维</h4><h4 id="7-1-2-特征选择"><a href="#7-1-2-特征选择" class="headerlink" title="7.1.2. 特征选择"></a>7.1.2. 特征选择</h4><h5 id="7-1-2-1-特征选择的一般解决思路"><a href="#7-1-2-1-特征选择的一般解决思路" class="headerlink" title="7.1.2.1. 特征选择的一般解决思路"></a>7.1.2.1. 特征选择的一般解决思路</h5><p>目标：从初始特征集合中选取一个包含所有重要信息的特征子集</p>
<p>可用策略:</p>
<blockquote>
<ul>
<li><p>（1）找到该领域懂业务的专家，让他们给一些建议。比如我们需要解决一个药品疗效的分类问题，那么先找到领域专家，向他们咨询哪些因素（特征）会对该药品的疗效产生影响，较大影响的和较小影响的都要。这些特征就是我们的特征的第一候选集</p>
</li>
<li><p>（2）若没有任何领域知识作为先验假设</p>
<ul>
<li><p>方差筛选：最简单的方法，方差越大的特征，那么我们可以认为它是比较有用的。如果方差较小，比如小于1，那么这个特征可能对我们的算法作用没有那么大。最极端的，如果某个特征方差为0，即所有的样本该特征的取值都是一样的，那么它对我们的模型训练没有任何作用，可以直接舍弃；</p>
</li>
<li><p>遍历所有可能的子集——计算上不可行；</p>
</li>
<li><p>产生一个“候选子集”，评价出它的好坏，基于评价结果产生下一个候选子集，再对其进行评价…这个过程持续下去，直到无法再找到更好的候选子集为止——这就是我们所说的一般的特征选择问题的处理策略；</p>
</li>
</ul>
</li>
</ul>
</blockquote>
<p>那么这就产生了两个问题：</p>
<blockquote>
<ul>
<li><p>如何根据评价结果获取下一个候选特征子集？——子集搜索</p>
</li>
<li><p>如何评价候选特征子集的好坏？——子集评价</p>
</li>
</ul>
</blockquote>
<ul>
<li><p><strong>子集搜索</strong></p>
<p>  （1）给定特征结合${a_1,a_2,…,a_d}$，可将每个特征看作一个候选子集，对这d个候选单特征子集进行评价，假定${a_2}$最优，将${a_2}$作为第一轮的选定子集；</p>
<p>  （2）在上一轮的选定子集中加入一个特征，构成包含两个特征的候选子集</p>
<p>  假定在这d-1个候选两特征子集中${a_2,a_4}$最优，且由于${a_2}$，于是将${a_2,a_4}$作为本轮的选定子集；</p>
<p>  （3）假定在第k+1轮时，最优的候选特征子集不如上一轮的选定集，则停止生产候选子集，并将上一轮的k特征子集作为特征选择结果</p>
<p>  以上逐渐增加相关特征的策略称为 <strong>“前向”搜索</strong> ，与它相对于的还有 <strong>“反向”搜索</strong> 和 <strong>“双向”搜索</strong></p>
<p>  注意：上述策略都是贪心的，因为它们都仅考虑了使本轮选定集最优</p>
</li>
<li><p><strong>子集评价</strong></p>
<p>  给定数据集D，假定D中第i类样本所占的比例为$p_i(i=1,2,…,|y|)$，为了后面方便进行讨论，假定样本属性均为离散型</p>
<p>  对于属性子集A，假定根据其取值将D分成V个子集${D^1,D^2,…,D^V}$，每个子集中的样本在A上取值相同</p>
<p>  属性子集A的信息增益：</p>
<p>  $$Gain(A)=Ent(D)-\sum_{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v)$$</p>
<p>  其中信息熵定义为：</p>
<p>  $$Ent(D)=-\sum_{k=1}^{|y|}p_klog_2p_k$$</p>
<p>  信息熵增益越大，意味着特征子集A包含的有助于分类的信息越多</p>
</li>
</ul>
<h5 id="7-1-2-2-具体的几种特征选择类型"><a href="#7-1-2-2-具体的几种特征选择类型" class="headerlink" title="7.1.2.2. 具体的几种特征选择类型"></a>7.1.2.2. 具体的几种特征选择类型</h5><p>特征选择方法有很多，一般分为三类：</p>
<ul>
<li><p><strong>过滤式选择</strong></p>
<p>  过滤式选择：先对数据集进行特征选择，它按照特征的发散性或者相关性指标对各个特征进行评分，设定评分阈值或者待选择阈值的个数，选择合适特征然后再训练学习器——特征选择过程与后续学习器无关</p>
<p>  相当于：先用特征选择过程对初始特征进行“过滤”，再用过滤后的特征来训练模型</p>
<p>  方差筛选就是过滤法的一种</p>
<p>  一个经典的过滤式特征选择方法为Relief（Relevant Features）方法：</p>
<blockquote>
<blockquote>
<p>该方法设计了一个“相关性统计量”来度量特征的重要性，该统计量是一个向量，其每一个分量对于一个初始特征，而特征子集的重要性则由子集中每个特征所对应的相关统计量之和来决定</p>
</blockquote>
<p>Relief的关键是如何确定相关统计量的：</p>
<blockquote>
<p>给定训练集${(x_1,y_1),(x_2,y_2),…,(x_m,y_m)}$，对每个示例$x_i$，Relief先在$x_i$的同类样本中寻找其最近邻$x_{i,nh}$，称为“猜中近邻”(near-hit)，再从$x_i$的异类样本中寻找其最近邻$x_{i,nm}$，称为“猜错近邻”(near-miss)</p>
<p>相关统计量对应于属性j的分量为：</p>
<p>$$\delta^j=\sum_i[-diff(x_i^j,x_{i,nh}^j)^2+diff(x_i^j,x_{i,nm}^j)^2]$$</p>
<p>其中$diff(x_a^j,x_b^j)$的取值取决于属性j的类型：</p>
<ul>
<li>离散型：$x_a^j=x_b^j$时，$diff(x_a^j,x_b^j)=0$，否则为1；</li>
<li>连续型：$diff(x_a^j,x_b^j)=|x_a^j-x_b^j|$</li>
</ul>
</blockquote>
<p>理解Relief相关统计量的意义：</p>
<blockquote>
<p>若$x_i$与其猜中近邻$x_{i,nh}$在属性j上的距离小于$x_i$与其猜错近邻$x_{i,nm}$的距离，则说明该属性对于区分同类样本与异类样本是有益的，于是增大j的统计量；</p>
<p>反之，说明属性j起负面作用，生与死减小j对于的统计量</p>
</blockquote>
<p>以上说的是Relief对于二分类问题的情况，其扩展变体Relief-F能处理多分类问题，由于篇幅有限，此处略去</p>
</blockquote>
<p>  其实不同过滤式选择方法主要的差别在于“相关统计量”的选择，其他常用的统计量和它们的适用场景：</p>
<ul>
<li><p><strong>相关系数</strong>：主要用于输出连续值的监督学习算法中。分别计算所有训练集中各个特征与输出值之间的相关系数，设定一个阈值，选择相关系数较大的部分特征；</p>
</li>
<li><p><strong>假设检验</strong>：比如卡方检验。卡方检验可以检验某个特征分布和输出值分布之间的相关性，它比粗暴的方差法好用。</p>
<p>  在sklearn中，使用chi2这个类来做卡方检验得到所有特征的卡方值与显著性水平P临界值，我们可以给定卡方值阈值， 选择卡方值较大的部分特征。</p>
<p>  除了卡方检验，我们还可以使用F检验和t检验，它们都是使用假设检验的方法，只是使用的统计分布不是卡方分布，而是F分布和t分布而已。在sklearn中，有F检验的函数<code>f_classif</code>和<code>f_regression</code>，分别在分类和回归特征选择时使用。</p>
</li>
<li><p><strong>互信息</strong>：从信息熵的角度分析各个特征和输出值之间的关系评分</p>
<p>  互信息值越大，说明该特征和输出值之间的相关性越大，越需要保留。在sklearn中，可以使用<code>mutual_info_classif</code>(分类)和<code>mutual_info_regression</code>(回归)来计算各个输入特征和输出值之间的互信息。</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>包裹式选择</strong></p>
<p>  包裹式选择：直接把最终将要使用的学习器的性能作为特征子集的评价准则</p>
<p>  即，为给定的学习器选择最有利于其性能、“量身定做”的特征子集</p>
<p>  其特点：</p>
<blockquote>
<ul>
<li>由于包裹式特征选择方法直接针对给定学习器进行优化，因此从最终性能来看，它比过滤式特征选择要好；</li>
<li>特征选择过程中需多次训练学习器，因此其计算开销通常比过滤式大得多；</li>
</ul>
</blockquote>
<p>  最常用的包装法是<strong>递归消除特征法</strong>(recursive feature elimination,以下简称RFE)。递归消除特征法使用一个机器学习模型来进行多轮训练，每轮训练后，消除若干权值系数的对应的特征，再基于新的特征集进行下一轮训练。在sklearn中，可以使用RFE函数来选择特征。</p>
<p>  以经典的SVM-RFE算法来讨论这个特征选择的思路：</p>
<blockquote>
<p>这个算法以支持向量机来做RFE的机器学习模型选择特征</p>
<p>它在第一轮训练的时候，会选择所有的特征来训练，得到了分类的超平面$wx+b=0$后，如果有n个特征，那么RFE-SVM会选择出$w$中分量的平方值$w^2_i$最小的那个序号i对应的特征，将其排除</p>
<p>在第二轮的时候，特征数就剩下n-1个了，我们继续用这n-1个特征和输出值来训练SVM，同样的，去掉$w^2_i$最小的那个序号i对应的特征。以此类推，直到剩下的特征数满足我们的需求为止。</p>
</blockquote>
</li>
<li><p><strong>嵌入式选择——基于$L_1$正则化</strong></p>
<p>  嵌入式特征选择：将特征选择与学习器训练过程融合为一体，两者在同一个优化过程中完成，即在学习训练过程中自动地进行了特征选择</p>
<p>  对于简单的线性回归问题，它的优化目标一般为最小化平方误差：</p>
<p>  $$\min_\omega \, \sum_{i=1}^m(y_i-\omega^Tx_i)$$</p>
<p>  当样本特征多而样本数又相对较少时，很容易陷入过拟合，为了缓解过拟合，需要对上面的优化目标引入最终项</p>
<p>  若使用L2范数正则化则有</p>
<p>  $$\min_\omega \, \sum_{i=1}^m(y_i-\omega^Tx_i)-\lambda||\omega||_2^2\quad(岭回归）$$</p>
<p>  若使用L1范数则有</p>
<p>  $$\min_\omega \, \sum_{i=1}^m(y_i-\omega^Tx_i)-\lambda||\omega||_1\quad(LASSO）$$</p>
<p>  L1和L2范数都能降低过拟合的风险，而L1范数还会带来一个额外的好处：跟容易获得“稀疏”（sparse）解，即它求得的$\omega$会有更少的非零分量</p>
</li>
</ul>
<h4 id="7-1-3-构造高级特征"><a href="#7-1-3-构造高级特征" class="headerlink" title="7.1.3. 构造高级特征"></a>7.1.3. 构造高级特征</h4><p>在我们拿到已有的特征后，我们还可以根据需要寻找到更多的高级特征</p>
<p>比如有车的路程特征和时间间隔特征，我们就可以得到车的平均速度这个二级特征。根据车的速度特征，我们就可以得到车的加速度这个三级特征，根据车的加速度特征，我们就可以得到车的加加速度这个四级特征……也就是说，高级特征可以一直寻找下去</p>
<p>在Kaggle之类的算法竞赛中，高分团队主要使用的方法除了集成学习算法，剩下的主要就是在高级特征上面做文章。所以寻找高级特征是模型优化的必要步骤之一</p>
<p>寻找高级特征最常用的方法有：</p>
<blockquote>
<ul>
<li><p>若干项特征加和： 我们假设你希望根据每日销售额得到一周销售额的特征。你可以将最近的7天的销售额相加得到</p>
</li>
<li><p>若干项特征之差： 假设你已经拥有每周销售额以及每月销售额两项特征，可以求一周前一月内的销售额</p>
</li>
<li><p>若干项特征乘积： 假设你有商品价格和商品销量的特征，那么就可以得到销售额的特征</p>
</li>
<li><p>若干项特征除商： 假设你有每个用户的销售额和购买的商品件数，那么就是得到该用户平均每件商品的销售额</p>
</li>
</ul>
</blockquote>
<p>寻找高级特征的方法远不止于此，它需要你根据你的业务和模型需要而得，而不是随便的两两组合形成高级特征，这样容易导致特征爆炸，反而没有办法得到较好的模型</p>
<h3 id="7-2-功能实现"><a href="#7-2-功能实现" class="headerlink" title="7.2. 功能实现"></a>7.2. 功能实现</h3><p>在 <code>sklearn.feature_selection</code> 模块中的类可以用来对样本集进行 feature selection（特征选择）和 dimensionality reduction（降维）</p>
<h4 id="7-2-1-特征降维"><a href="#7-2-1-特征降维" class="headerlink" title="7.2.1. 特征降维"></a>7.2.1. 特征降维</h4><h4 id="7-2-2-特征选择"><a href="#7-2-2-特征选择" class="headerlink" title="7.2.2. 特征选择"></a>7.2.2. 特征选择</h4><h5 id="7-2-2-1-移除低方差特征"><a href="#7-2-2-1-移除低方差特征" class="headerlink" title="7.2.2.1. 移除低方差特征"></a>7.2.2.1. 移除低方差特征</h5><p><code>VarianceThreshold</code> 是特征选择的一个简单基本方法，它会移除所有那些方差不满足一些阈值的特征。默认情况下，它将会移除所有的零方差特征，即那些在所有的样本上的取值均不变的特征。</p>
<p>例如，假设我们有一个特征是布尔值的数据集，我们想要移除那些在整个数据集中特征值为0或者为1的比例超过80%的特征。布尔特征是伯努利（ Bernoulli ）随机变量，变量的方差为</p>
<p>$$Var[X] = p(1 - p)$$</p>
<p>因此，我们可以使用阈值 <code>.8 * (1 - .8)</code> 进行选择:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_selection import VarianceThreshold</span><br><span class="line">X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]</span><br><span class="line">sel = VarianceThreshold(threshold=(.8 * (1 - .8)))</span><br><span class="line">sel.fit_transform(X)</span><br><span class="line"></span><br><span class="line">特征选择结果如下：</span><br><span class="line"></span><br><span class="line">array([[0, 1],</span><br><span class="line"> [1, 0],</span><br><span class="line"> [0, 0],</span><br><span class="line"> [1, 1],</span><br><span class="line"> [1, 0],</span><br><span class="line"> [1, 1]])</span><br></pre></td></tr></table></figure>
<h5 id="7-2-2-2-单变量特征选择"><a href="#7-2-2-2-单变量特征选择" class="headerlink" title="7.2.2.2. 单变量特征选择"></a>7.2.2.2. 单变量特征选择</h5><p>单变量的特征选择是通过基于单变量的统计测试来选择最好的特征，因此它实际采用的特征选择策略是<strong>过滤式特征选择方法中的Relief方法</strong></p>
<blockquote>
<ul>
<li><p><code>SelectKBest</code> 移除那些除了评分最高的 K 个特征之外的所有特征</p>
</li>
<li><p><code>SelectPercentile</code> 移除除了用户指定的最高得分百分比之外的所有特征</p>
<p>  对每个特征应用常见的单变量统计测试: 假阳性率（false positive rate） SelectFpr, 伪发现率（false discovery rate） SelectFdr , 或者族系误差（family wise error） SelectFwe 。</p>
</li>
<li><code>GenericUnivariateSelect</code> 允许使用可配置方法来进行单变量特征选择。它允许超参数搜索评估器来选择最好的单变量特征。</li>
</ul>
</blockquote>
<p>例如下面的实例，我们可以使用 x<sup>2</sup> 检验样本集来选择最好的两个特征：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.feature_selection import SelectKBest</span><br><span class="line">from sklearn.feature_selection import chi2</span><br><span class="line">iris = load_iris()</span><br><span class="line">X, y = iris.data, iris.target</span><br><span class="line">X.shape</span><br><span class="line"># 输出为：(150, 4)</span><br><span class="line">X_new = SelectKBest(chi2, k=2).fit_transform(X, y)</span><br><span class="line">X_new.shape</span><br><span class="line"># 输出为：(150, 2)</span><br></pre></td></tr></table></figure>
<p>对于回归问题和分类问题，适用的统计量是不一样的：</p>
<blockquote>
<ul>
<li>对于回归: <code>f_regression</code> , <code>mutual_info_regression</code></li>
<li>对于分类: <code>chi2</code> , <code>f_classif</code> , <code>mutual_info_classif</code></li>
</ul>
</blockquote>
<p>注意：不要使用一个回归评分函数来处理分类问题，你会得到无用的结果</p>
<h5 id="7-2-2-3-递归式特征消除"><a href="#7-2-2-3-递归式特征消除" class="headerlink" title="7.2.2.3. 递归式特征消除"></a>7.2.2.3. 递归式特征消除</h5><p>给定一个外部的估计器，可以对特征赋予一定的权重（比如，线性模型的相关系数），<code>recursive feature elimination ( RFE )</code> 通过考虑越来越小的特征集合来递归的选择特征。</p>
<p>首先，评估器在初始的特征集合上面训练并且每一个特征的重要程度是通过一个 <code>coef_</code> 属性 或者 <code>feature_importances_</code> 属性来获得。 然后，从当前的特征集合中移除最不重要的特征。在特征集合上不断的重复递归这个步骤，直到最终达到所需要的特征数量为止。 RFECV 在一个交叉验证的循环中执行 RFE 来找到最优的特征数量</p>
<hr>
<p>参考资料：</p>
<p>(1) 机械工业出版社《机器学习实战：基于Scikit-Learn和TensorFlow》</p>
<p>(2) <a href="https://github.com/MLjian/TextClassificationImplement" target="_blank" rel="noopener">【GitHub】MLjian/TextClassificationImplement</a></p>
<p>(3) <a href="https://www.cnblogs.com/pinard/p/6035872.html" target="_blank" rel="noopener">刘建平Pinard《scikit-learn 逻辑回归类库使用小结》</a></p>
<p>(4) <a href="https://blog.csdn.net/loveliuzz/article/details/78708359" target="_blank" rel="noopener">loveliuzz《机器学习sklearn19.0——Logistic回归算法》</a></p>
<p>(5) <a href="https://www.cnblogs.com/pinard/p/6056319.html" target="_blank" rel="noopener">刘建平Pinard《scikit-learn决策树算法类库使用小结》</a></p>
<p>(6) <a href="https://blog.csdn.net/loveliuzz/article/details/78739438" target="_blank" rel="noopener">loveliuzz《机器学习sklearn19.0——决策树算法》</a></p>
<p>(7) <a href="https://www.cnblogs.com/pinard/p/6117515.html" target="_blank" rel="noopener">刘建平Pinard《scikit-learn 支持向量机算法库使用小结》</a></p>
<p>(8) <a href="https://www.cnblogs.com/pinard/p/6126077.html" target="_blank" rel="noopener">刘建平Pinard《支持向量机高斯核调参小结》</a></p>
<p>(9) <a href="https://blog.csdn.net/loveliuzz/article/details/78768063" target="_blank" rel="noopener">loveliuzz《机器学习sklearn19.0——SVM算法》</a></p>
<p>(10) 周志华《机器学习：第7章 贝叶斯分类器》</p>
<p>(11) <a href="https://www.cnblogs.com/pinard/p/6069267.html" target="_blank" rel="noopener">刘建平Pinard《朴素贝叶斯算法原理小结》</a></p>
<p>(12) <a href="https://www.cnblogs.com/pinard/p/6074222.html" target="_blank" rel="noopener">刘建平Pinard《scikit-learn 朴素贝叶斯类库使用小结》</a></p>
<p>(13) 周志华《机器学习：第5章 神经网络》</p>
<p>(14) 吴恩达《deeplearning.ai：改善深层神经网络》</p>
<p>(15) <a href="http://sklearn.apachecn.org/#/docs/18" target="_blank" rel="noopener">scikit-learn中文网《神经网络模型（有监督）》</a></p>
<p>(16) <a href="https://blog.csdn.net/weixin_38278334/article/details/83023958" target="_blank" rel="noopener">啊噗不是阿婆主《sklearn 神经网络MLPclassifier参数详解》</a></p>
<p>(17) <a href="https://www.jianshu.com/p/d4fd9c52a915" target="_blank" rel="noopener">多问Why.简书《SkLearn之MLP》</a></p>
<p>(18) 周志华《机器学习：第11章 特征选择与稀疏学习》</p>
<p>(19) <a href="https://www.cnblogs.com/pinard/p/9032759.html" target="_blank" rel="noopener">刘建平Pinard《特征工程之特征选择》</a></p>
<p>(20) <a href="http://sklearn.apachecn.org/#/docs/14" target="_blank" rel="noopener">sklearn中文文档：特征选择</a></p>
<p>(21) <a href="https://scikit-learn.org/stable/modules/feature_selection.html" target="_blank" rel="noopener">sklearn英文文档：1.13. Feature selection</a></p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag"># Machine-Learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/06/Beauty-Mathmatics-Note/" rel="next" title="《数学之美》阅读笔记">
                <i class="fa fa-chevron-left"></i> 《数学之美》阅读笔记
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/22/Biopython-Note/" rel="prev" title="Biopython学习笔记">
                Biopython学习笔记 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Lianm">
            
              <p class="site-author-name" itemprop="name">Lianm</p>
              <p class="site-description motion-element" itemprop="description">中国科学院北京基因组研究所研究生</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">31</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">6</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">31</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/Ming-Lian" title="GitHub &rarr; https://github.com/Ming-Lian" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:lianming17m@big.ac.cn" title="E-Mail &rarr; mailto:lianming17m@big.ac.cn" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://www.jianshu.com/u/42e711970fa8" title="简书 &rarr; https://www.jianshu.com/u/42e711970fa8" rel="noopener" target="_blank"><i class="fa fa-fw fa-jianshu"></i>简书</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://www.zhihu.com/people/understorm" title="知乎 &rarr; https://www.zhihu.com/people/understorm" rel="noopener" target="_blank"><i class="fa fa-fw fa-zhihu"></i>知乎</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-线性回归"><span class="nav-text">1. 线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-目标函数与损失函数"><span class="nav-text">1.1. 目标函数与损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-标准方程求解"><span class="nav-text">1.2. 标准方程求解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-梯度下降求解"><span class="nav-text">1.3. 梯度下降求解</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-logistic回归：“达观杯”文本智能处理挑战赛"><span class="nav-text">2. logistic回归：“达观杯”文本智能处理挑战赛</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-任务描述"><span class="nav-text">2.1. 任务描述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-编程实现"><span class="nav-text">2.2. 编程实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-sklearn包中logistic算法的使用"><span class="nav-text">2.3. sklearn包中logistic算法的使用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-决策树：鸢尾花数据分类"><span class="nav-text">3. 决策树：鸢尾花数据分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-DecisionTreeClassifier实例"><span class="nav-text">3.1. DecisionTreeClassifier实例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-可视化决策树"><span class="nav-text">3.2. 可视化决策树</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-SVM：sklearn中测试数据"><span class="nav-text">4. SVM：sklearn中测试数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-SVM相关知识点回顾"><span class="nav-text">4.1. SVM相关知识点回顾</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-1-SVM与SVR"><span class="nav-text">4.1.1. SVM与SVR</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-2-核函数"><span class="nav-text">4.1.2. 核函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-sklearn中SVM相关库的简介"><span class="nav-text">4.2. sklearn中SVM相关库的简介</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-1-分类库与回归库"><span class="nav-text">4.2.1. 分类库与回归库</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-2-高斯核调参"><span class="nav-text">4.2.2. 高斯核调参</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-2-1-需要调节的参数"><span class="nav-text">4.2.2.1. 需要调节的参数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-2-2-调参方法：网格搜索"><span class="nav-text">4.2.2.2. 调参方法：网格搜索</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-编程实现"><span class="nav-text">4.3. 编程实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-朴素贝叶斯"><span class="nav-text">5.朴素贝叶斯</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-朴素贝叶斯相关知识点回顾"><span class="nav-text">5.1. 朴素贝叶斯相关知识点回顾</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-1-什么是朴素贝叶斯分类器"><span class="nav-text">5.1.1. 什么是朴素贝叶斯分类器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-2-朴素贝叶斯推断"><span class="nav-text">5.1.2. 朴素贝叶斯推断</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-3-朴素贝叶斯学习"><span class="nav-text">5.1.3. 朴素贝叶斯学习</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-sklearn中朴素贝叶斯类库的简介"><span class="nav-text">5.2. sklearn中朴素贝叶斯类库的简介</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-1-GaussianNB类"><span class="nav-text">5.2.1. GaussianNB类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-2-MultinomialNB类"><span class="nav-text">5.2.2. MultinomialNB类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-3-BernoulliNB类"><span class="nav-text">5.2.3. BernoulliNB类</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-神经网络（sklearn）：手写数字识别"><span class="nav-text">6. 神经网络（sklearn）：手写数字识别</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-神经网络相关知识点回顾"><span class="nav-text">5.1. 神经网络相关知识点回顾</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-1-感知机：神经网络的最小单元"><span class="nav-text">6.1.1. 感知机：神经网络的最小单元</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-2-神经网络的学习：BackPropagation算法"><span class="nav-text">6.1.2. 神经网络的学习：BackPropagation算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-2-1-标准BP算法"><span class="nav-text">6.1.2.1. 标准BP算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-2-2-累积BP算法"><span class="nav-text">6.1.2.2. 累积BP算法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-sklearn中神经网络库的简介"><span class="nav-text">6.2. sklearn中神经网络库的简介</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-1-MLPClassifier"><span class="nav-text">6.2.1. MLPClassifier</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-编程实现"><span class="nav-text">6.3. 编程实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-特征选择-特征降维：sklearn包中feature-selection模块的使用"><span class="nav-text">7. 特征选择/特征降维：sklearn包中feature_selection模块的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-相关知识回顾"><span class="nav-text">6.1. 相关知识回顾</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-1-1-特征降维"><span class="nav-text">7.1.1. 特征降维</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-1-2-特征选择"><span class="nav-text">7.1.2. 特征选择</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#7-1-2-1-特征选择的一般解决思路"><span class="nav-text">7.1.2.1. 特征选择的一般解决思路</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#7-1-2-2-具体的几种特征选择类型"><span class="nav-text">7.1.2.2. 具体的几种特征选择类型</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-1-3-构造高级特征"><span class="nav-text">7.1.3. 构造高级特征</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-功能实现"><span class="nav-text">7.2. 功能实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-1-特征降维"><span class="nav-text">7.2.1. 特征降维</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-2-特征选择"><span class="nav-text">7.2.2. 特征选择</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#7-2-2-1-移除低方差特征"><span class="nav-text">7.2.2.1. 移除低方差特征</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#7-2-2-2-单变量特征选择"><span class="nav-text">7.2.2.2. 单变量特征选择</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#7-2-2-3-递归式特征消除"><span class="nav-text">7.2.2.3. 递归式特征消除</span></a></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lianm</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.0.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>














  
    
    
  
  <script color="0,0,255" opacity="0.5" zindex="-1" count="99" src="/lib/canvas-nest/canvas-nest.min.js"></script>



  
  











  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script src="/lib/three/three.min.js"></script>

  
  <script src="/lib/three/three-waves.min.js"></script>


  


  <script src="/js/src/utils.js?v=7.0.0"></script>

  <script src="/js/src/motion.js?v=7.0.0"></script>



  
  


  <script src="/js/src/schemes/muse.js?v=7.0.0"></script>




  
  <script src="/js/src/scrollspy.js?v=7.0.0"></script>
<script src="/js/src/post-details.js?v=7.0.0"></script>



  


  <script src="/js/src/bootstrap.js?v=7.0.0"></script>



  


  


  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: "AMS"
      }
    }
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
      for (i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
  overflow: auto hidden;
}
</style>

    
  


  

  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js'; 
      }
      else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>


  

  

  

  

  

  

  

  

</body>
</html>
