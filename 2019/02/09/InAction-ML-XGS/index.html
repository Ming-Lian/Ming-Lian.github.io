<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">


























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.0.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/Favico.png?v=7.0.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/Favico.png?v=7.0.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/Favico.png?v=7.0.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.0.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.0.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="1. “达观杯”文本智能处理挑战赛：sklearn包中logistic算法的使用1.1. 任务描述具体任务可以到 ‘达观杯’文本智能处理挑战赛官网 查看 任务：建立模型通过长文本数据正文(article)，预测文本对应的类别(class) 数据：  数据包含2个csv文件：  train_set.csv   此数据集用于训练模型，每一行对应一篇文章。文章分别在“字”和“词”的级别上做了脱敏处理。共">
<meta name="keywords" content="sklearn,logistic回归,决策树,朴素贝叶斯方法">
<meta property="og:type" content="article">
<meta property="og:title" content="西瓜书带学训练营·实战任务">
<meta property="og:url" content="http://yoursite.com/2019/02/09/InAction-ML-XGS/index.html">
<meta property="og:site_name" content="Lianm&#39;s Blog">
<meta property="og:description" content="1. “达观杯”文本智能处理挑战赛：sklearn包中logistic算法的使用1.1. 任务描述具体任务可以到 ‘达观杯’文本智能处理挑战赛官网 查看 任务：建立模型通过长文本数据正文(article)，预测文本对应的类别(class) 数据：  数据包含2个csv文件：  train_set.csv   此数据集用于训练模型，每一行对应一篇文章。文章分别在“字”和“词”的级别上做了脱敏处理。共">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-lr-1.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-lr-2.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-lr-3.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-lr-4.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-lr-5.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-lr-6.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-lr-7.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-decision-tree-1.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-decision-tree-2.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-1.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-2.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-3.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-4.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-5.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-6.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-7.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-8.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-9.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-10.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-11.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-12.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-13.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-SVM-14.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-naive-bayes-1.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-naive-bayes-2.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-naive-bayes-3.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-naive-bayes-4.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-naive-bayes-5.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-naive-bayes-6.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-1.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-2.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-3.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-4.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-5.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-6.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-7.jpg">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-8.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-9.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-10.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-11.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-12.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-13.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-14.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-15.png">
<meta property="og:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-neural-network-16.png">
<meta property="og:updated_time" content="2019-03-09T07:49:51.650Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="西瓜书带学训练营·实战任务">
<meta name="twitter:description" content="1. “达观杯”文本智能处理挑战赛：sklearn包中logistic算法的使用1.1. 任务描述具体任务可以到 ‘达观杯’文本智能处理挑战赛官网 查看 任务：建立模型通过长文本数据正文(article)，预测文本对应的类别(class) 数据：  数据包含2个csv文件：  train_set.csv   此数据集用于训练模型，每一行对应一篇文章。文章分别在“字”和“词”的级别上做了脱敏处理。共">
<meta name="twitter:image" content="http://yoursite.com/images/InAction-MachineLearning-XGS-sklearn-lr-1.png">






  <link rel="canonical" href="http://yoursite.com/2019/02/09/InAction-ML-XGS/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>西瓜书带学训练营·实战任务 | Lianm's Blog</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Lianm's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">Focus on Bioinformatics and Machine-Learning</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-search">

    
    
    
      
    

    

    <a href="/search/" rel="section"><i class="menu-item-icon fa fa-fw fa-search"></i> <br>搜索</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    
  
  
  
  

  

  <a href="https://github.com/Ming-Lian" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" style="fill: #222; color: #fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a>



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/02/09/InAction-ML-XGS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lianm">
      <meta itemprop="description" content="中国科学院北京基因组研究所研究生">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lianm's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">西瓜书带学训练营·实战任务

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-02-09 00:17:04" itemprop="dateCreated datePublished" datetime="2019-02-09T00:17:04+00:00">2019-02-09</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-03-09 07:49:51" itemprop="dateModified" datetime="2019-03-09T07:49:51+00:00">2019-03-09</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine-Learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="1-“达观杯”文本智能处理挑战赛：sklearn包中logistic算法的使用"><a href="#1-“达观杯”文本智能处理挑战赛：sklearn包中logistic算法的使用" class="headerlink" title="1. “达观杯”文本智能处理挑战赛：sklearn包中logistic算法的使用"></a>1. “达观杯”文本智能处理挑战赛：sklearn包中logistic算法的使用</h2><h3 id="1-1-任务描述"><a href="#1-1-任务描述" class="headerlink" title="1.1. 任务描述"></a>1.1. 任务描述</h3><p>具体任务可以到 <a href="http://www.dcjingsai.com/common/cmpt/%E2%80%9C%E8%BE%BE%E8%A7%82%E6%9D%AF%E2%80%9D%E6%96%87%E6%9C%AC%E6%99%BA%E8%83%BD%E5%A4%84%E7%90%86%E6%8C%91%E6%88%98%E8%B5%9B_%E8%B5%9B%E4%BD%93%E4%B8%8E%E6%95%B0%E6%8D%AE.html" target="_blank" rel="noopener">‘达观杯’文本智能处理挑战赛官网</a> 查看</p>
<p><strong>任务</strong>：建立模型通过长文本数据正文(article)，预测文本对应的类别(class)</p>
<p>数据：</p>
<blockquote>
<p>数据包含2个csv文件：</p>
<ul>
<li><p><strong>train_set.csv</strong></p>
<p>  此数据集用于训练模型，每一行对应一篇文章。文章分别在“字”和“词”的级别上做了脱敏处理。共有四列：</p>
<p>  第一列是文章的索引(id)，第二列是文章正文在“字”级别上的表示，即字符相隔正文(article)；第三列是在“词”级别上的表示，即词语相隔正文(word_seg)；第四列是这篇文章的标注(class)。</p>
<p>  注：每一个数字对应一个“字”，或“词”，或“标点符号”。“字”的编号与“词”的编号是独立的！</p>
</li>
<li><p><strong>test_set.csv</strong> </p>
<p>  此数据用于测试。数据格式同train_set.csv，但不包含class</p>
<p>  注：test_set与train_test中文章id的编号是独立的。</p>
</li>
</ul>
</blockquote>
<h3 id="1-2-编程实现"><a href="#1-2-编程实现" class="headerlink" title="1.2. 编程实现"></a>1.2. 编程实现</h3><p>使用<strong>logistic回归</strong>来实现这个多元分类任务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">from sklearn.feature_extract.txt import TfidfVectorizer</span><br><span class="line">from sklearn.decomposition import TruncatedSVD</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.model_select import train_test_split</span><br><span class="line">from sklearn.externals import joblib</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">t_start = time.time()</span><br><span class="line"></span><br><span class="line"># ===========================================================</span><br><span class="line"># 1. 载人数据与数据预处理</span><br><span class="line">df_train = pd.read_csv(&apos;data/train_set.csv&apos;)</span><br><span class="line">df_train = df_train.drop(columns=&apos;article&apos;,inplace=True)</span><br><span class="line">df_test = pd.read_csv(&apos;data/test_set.csv&apos;)</span><br><span class="line">df_test = df_test.drop(columns=&apos;article&apos;,inplace=True)</span><br><span class="line">y_train = (df_train[&apos;class&apos;] - 1).values</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ===========================================================</span><br><span class="line"># 2. 特征工程，这里先使用经典的文本特征提取方法TFIDF，提取的TFIDF特征</span><br><span class="line">vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.9, sublinear_tf=True)</span><br><span class="line">vectorizer.fit(df_train[&apos;word_seg&apos;])</span><br><span class="line">X_train = vectorizer.transform(df_train[&apos;word_seg&apos;])</span><br><span class="line">X_test = vectorizer.transform(df_test[&apos;word_seg&apos;])</span><br><span class="line"></span><br><span class="line"># 可以将得到的TFIDF特征保存至本地</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">data = (X_train,y_train,X_test)</span><br><span class="line">f = open(&apos;data/data_tfidf.pkl&apos;,&apos;wb&apos;)</span><br><span class="line">pickle.dump(data,f)</span><br><span class="line">f.close()</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ===========================================================</span><br><span class="line"># 3. 特征降维，将上一步提取的TFIDF特征使用lsa方法进行特征降维</span><br><span class="line">lsa = TruncatedSVD(n_components=200)</span><br><span class="line">X_train = lsa.transform(X_train)</span><br><span class="line">X_test = lsa.transform(X_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ===========================================================</span><br><span class="line"># 4. 训练分类器</span><br><span class="line"></span><br><span class="line"># 划分训练集和验证集</span><br><span class="line">X_train,X_vali,y_train,y_vali = train_test_split(X_train,y_train,test_size=0.1,random_state=0)</span><br><span class="line"></span><br><span class="line">##  multi_class:分类方式选择参数，有&quot;ovr(默认)&quot;和&quot;multinomial&quot;两个值可选择，在二元逻辑回归中无区别</span><br><span class="line">##  solver:优化算法选择参数，当penalty为&quot;l1&quot;时，参数只能是&quot;liblinear(坐标轴下降法)&quot;；&quot;lbfgs&quot;和&quot;cg&quot;都是关于目标函数的二阶泰勒展开</span><br><span class="line">##  当penalty为&quot;l2&quot;时，参数可以是&quot;lbfgs(拟牛顿法)&quot;,&quot;newton_cg(牛顿法变种)&quot;,&quot;seg(minibactch随机平均梯度下降)&quot;</span><br><span class="line">lr = LogisticRegression(multi_class=&quot;ovr&quot;,penalty=&quot;l2&quot;,solver=&quot;lbfgs&quot;)</span><br><span class="line">lr.fit(X_train,y_train)</span><br><span class="line"></span><br><span class="line"># 模型的保存与持久化</span><br><span class="line">joblib.dump(lr,&quot;logistic_lr.model&quot;)</span><br><span class="line">joblib.load(&quot;logistic_lr.model&quot;) #加载模型,会保存该model文件</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ===========================================================</span><br><span class="line"># 5. 在验证集上评估模型</span><br><span class="line">pre_vali = lr.predict(X_vali)</span><br><span class="line">pre_score = f1_score(y_true=y_vali,y_pred=pre_vali,average=&apos;macro&apos;)</span><br><span class="line">print(&quot;验证集分数：&#123;&#125;&quot;.format(score_vali))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ===========================================================</span><br><span class="line"># 6. 对测试集进行预测</span><br><span class="line">y_test = lr.predict(X_test) + 1</span><br></pre></td></tr></table></figure>
<h3 id="1-3-sklearn包中logistic算法的使用"><a href="#1-3-sklearn包中logistic算法的使用" class="headerlink" title="1.3. sklearn包中logistic算法的使用"></a>1.3. sklearn包中logistic算法的使用</h3><p><img src="/images/InAction-MachineLearning-XGS-sklearn-lr-1.png" alt></p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-lr-2.png" alt></p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-lr-3.png" alt></p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-lr-4.png" alt></p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-lr-5.png" alt></p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-lr-6.png" alt></p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-lr-7.png" alt></p>
<h2 id="2-鸢尾花数据分类：sklearn包中决策树算法类库的使用"><a href="#2-鸢尾花数据分类：sklearn包中决策树算法类库的使用" class="headerlink" title="2. 鸢尾花数据分类：sklearn包中决策树算法类库的使用"></a>2. 鸢尾花数据分类：sklearn包中决策树算法类库的使用</h2><h3 id="2-1-DecisionTreeClassifier实例"><a href="#2-1-DecisionTreeClassifier实例" class="headerlink" title="2.1. DecisionTreeClassifier实例"></a>2.1. DecisionTreeClassifier实例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">from itertools import product</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">from sklearn import datasets</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.preprocessing import MinMaxScaler</span><br><span class="line">from sklearn.feature_selection import SelectKBest</span><br><span class="line">from sklearn.feature_selection import chi2</span><br><span class="line">from sklearn.decomposition import PCA</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 使用自带的iris数据</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data[:, np.arange(0,4)]</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"># 划分训练集与测试集</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=14)</span><br><span class="line">print(&quot;训练数据集样本总数:%d;测试数据集样本总数:%d&quot; %(x_train.shape[0],x_test.shape[0]))</span><br><span class="line"></span><br><span class="line"># 对数据集进行标准化</span><br><span class="line">ss = MinMaxScaler()</span><br><span class="line">x_train = ss.fit_transform(x_train,y_train)</span><br><span class="line">x_test = ss.transform(x_test)</span><br><span class="line"></span><br><span class="line"># 特征选择：从已有的特征属性中选择出影响目标最大的特征属性</span><br><span class="line"># 常用方法：</span><br><span class="line">#	离散属性：F统计量、卡方系数、互信息mutual_info_classif</span><br><span class="line">#	连续属性：皮尔逊相关系数、F统计量、互信息mutual_info_classif&#125;</span><br><span class="line"># 这里使用离散属性的卡方系数，实现函数为SelectKBest，用SelectKBest方法从四个原始特征属性中选择出最能影响目标的3个特征属性</span><br><span class="line">ch2 = SelectKBest(chi2,k=3) # k默认为10，指定后会返回想要的特征个数</span><br><span class="line">ch2.fit(x_train,y_train)</span><br><span class="line">x_train = ch2.transform(x_train)</span><br><span class="line">x_test = ch2.transform(x_test)</span><br><span class="line"></span><br><span class="line"># 特征降维，这里使用PCA方法</span><br><span class="line">pca = PCA(n_components=2)   # 构建一个PCA对象，设置最终维度为2维。这里为了后边画图方便，将数据维度设置为 2，一般用默认不设置就可以</span><br><span class="line">x_train = pca.fit_transform(x_train) # 训练与转换，也可以拆分成两步</span><br><span class="line">x_test = pca.transform(x_test)</span><br><span class="line"></span><br><span class="line"># 训练模型</span><br><span class="line">#	criterion：指定特征选择标准，可以使用&quot;gini&quot;或者&quot;entropy&quot;，前者代表基尼系数，后者代表信息增益</span><br><span class="line">#	max_depth：限制树的最大深度4</span><br><span class="line">clf = DecisionTreeClassifier(criterion=&quot;entropy&quot;,max_depth=4)</span><br><span class="line">clf.fit(X, y) # 拟合模型</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 画图</span><br><span class="line">x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line"># 生成网格采样点</span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),</span><br><span class="line">                     np.arange(y_min, y_max, 0.1))</span><br><span class="line"></span><br><span class="line">Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">Z = Z.reshape(xx.shape)</span><br><span class="line"></span><br><span class="line">plt.contourf(xx, yy, Z, alpha=0.4)</span><br><span class="line">plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-decision-tree-1.png" alt></p>
<h3 id="2-2-可视化决策树"><a href="#2-2-可视化决策树" class="headerlink" title="2.2. 可视化决策树"></a>2.2. 可视化决策树</h3><p>scikit-learn中决策树的可视化一般需要安装graphviz，主要包括graphviz的安装和python的graphviz插件的安装</p>
<blockquote>
<ul>
<li><p>1) 安装graphviz。下载地址在：<code>http://www.graphviz.org</code>/。如果你是linux，可以用apt-get或者yum的方法安装。如果是windows，就在官网下载msi文件安装。无论是linux还是windows，装完后都要设置环境变量，将graphviz的bin目录加到PATH，比如我是windows，将<code>C:/Program Files (x86)/Graphviz2.38/bin/</code>加入了<code>PATH</code></p>
</li>
<li><p>2) 安装python插件graphviz： <code>pip install graphviz</code></p>
</li>
</ul>
<ul>
<li>3) 安装python插件pydotplus: <code>pip install pydotplus</code></li>
</ul>
</blockquote>
<p>这样环境就搭好了，有时候python会很笨，仍然找不到graphviz，这时，可以在代码里面加入这一行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.environ[&quot;PATH&quot;] += os.pathsep + &apos;C:/Program Files (x86)/Graphviz2.38/bin/&apos;</span><br></pre></td></tr></table></figure>
<p>可视化决策树的代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from IPython.display import Image  </span><br><span class="line">from sklearn import tree</span><br><span class="line">import pydotplus </span><br><span class="line">dot_data = tree.export_graphviz(clf, out_file=None, </span><br><span class="line">                         feature_names=iris.feature_names,  </span><br><span class="line">                         class_names=iris.target_names,  </span><br><span class="line">                         filled=True, rounded=True,  </span><br><span class="line">                         special_characters=True)  </span><br><span class="line">graph = pydotplus.graph_from_dot_data(dot_data)  </span><br><span class="line">Image(graph.create_png())</span><br></pre></td></tr></table></figure>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-decision-tree-2.png" alt></p>
<h2 id="3-sklearn中测试数据：sklearn包中SVM算法库的使用"><a href="#3-sklearn中测试数据：sklearn包中SVM算法库的使用" class="headerlink" title="3. sklearn中测试数据：sklearn包中SVM算法库的使用"></a>3. sklearn中测试数据：sklearn包中SVM算法库的使用</h2><h3 id="3-1-SVM相关知识点回顾"><a href="#3-1-SVM相关知识点回顾" class="headerlink" title="3.1. SVM相关知识点回顾"></a>3.1. SVM相关知识点回顾</h3><h4 id="3-1-1-SVM与SVR"><a href="#3-1-1-SVM与SVR" class="headerlink" title="3.1.1. SVM与SVR"></a>3.1.1. SVM与SVR</h4><ul>
<li><p><strong>SVM分类算法</strong></p>
<p>  其原始形式是：</p>
<p>  <img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-1.png" alt></p>
<p>  其中m为样本个数，我们的样本为(x<sub>1</sub>,y<sub>1</sub>),(x<sub>2</sub>,y<sub>2</sub>),…,(x<sub>m</sub>,y<sub>m</sub>)。w,b是我们的分离超平面的w∙ϕ(x<sub>i</sub>)+b=0系数, ξ<sub>i</sub>为第i个样本的松弛系数， C为惩罚系数。ϕ(x<sub>i</sub>)为低维到高维的映射函数</p>
<p>  通过拉格朗日函数以及对偶化后的形式为：</p>
<p>  <img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-2.png" alt></p>
</li>
<li><p><strong>SVR回归算法</strong></p>
<p>  <img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-3.png" alt></p>
<p>  其中m为样本个数，我们的样本为(x<sub>1</sub>,y<sub>1</sub>),(x<sub>2</sub>,y<sub>2</sub>),…,(x<sub>m</sub>,y<sub>m</sub>)。w,b是我们的回归超平面的w∙x<sub>i</sub>+b=0系数, ξ<sup>∨</sup><sub>i</sub>，ξ<sup>∧</sup><sub>i</sub>为第i个样本的松弛系数， C为惩罚系数，ϵ为损失边界，到超平面距离小于ϵ的训练集的点没有损失。ϕ(x<sub>i</sub>)为低维到高维的映射函数。</p>
<p>  <img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-4.png" alt></p>
</li>
</ul>
<h4 id="3-1-2-核函数"><a href="#3-1-2-核函数" class="headerlink" title="3.1.2. 核函数"></a>3.1.2. 核函数</h4><p>在scikit-learn中，内置的核函数一共有4种：</p>
<ul>
<li><p><strong>线性核函数</strong>（Linear Kernel）表达式为：K(x,z)=x∙z，就是普通的内积</p>
</li>
<li><p><strong>多项式核函数</strong>（Polynomial Kernel）是线性不可分SVM常用的核函数之一，表达式为：K(x,z)=（γx∙z+r)<sup>d</sup> ，其中，γ,r,d都需要自己调参定义</p>
</li>
<li><p><strong>高斯核函数</strong>（Gaussian Kernel），在SVM中也称为径向基核函数（Radial Basis Function,RBF），它是 libsvm 默认的核函数，当然也是 scikit-learn 默认的核函数。表达式为：K(x,z)=exp(−γ||x−z||<sup>2</sup>)， 其中，γ大于0，需要自己调参定义</p>
</li>
<li><p><strong>Sigmoid核函数</strong>（Sigmoid Kernel）也是线性不可分SVM常用的核函数之一，表达式为：K(x,z)=tanh（γx∙z+r)， 其中，γ，r都需要自己调参定义</p>
</li>
</ul>
<p>一般情况下，对非线性数据使用默认的高斯核函数会有比较好的效果，如果你不是SVM调参高手的话，建议使用高斯核来做数据分析。　</p>
<h3 id="3-2-sklearn中SVM相关库的简介"><a href="#3-2-sklearn中SVM相关库的简介" class="headerlink" title="3.2. sklearn中SVM相关库的简介"></a>3.2. sklearn中SVM相关库的简介</h3><p>scikit-learn SVM算法库封装了libsvm 和 liblinear 的实现，仅仅重写了算法了接口部分</p>
<h4 id="3-2-1-分类库与回归库"><a href="#3-2-1-分类库与回归库" class="headerlink" title="3.2.1. 分类库与回归库"></a>3.2.1. 分类库与回归库</h4><ul>
<li><p><strong>分类算法库</strong></p>
<p>  包括SVC， NuSVC，和LinearSVC 3个类</p>
<p>  对于SVC， NuSVC，和LinearSVC 3个分类的类，SVC和 NuSVC差不多，区别仅仅在于对损失的度量方式不同，而LinearSVC从名字就可以看出，他是线性分类，也就是不支持各种低维到高维的核函数，仅仅支持线性核函数，对线性不可分的数据不能使用</p>
</li>
<li><p><strong>回归算法库</strong></p>
<p>  包括SVR， NuSVR，和LinearSVR 3个类</p>
<p>  同样的，对于SVR， NuSVR，和LinearSVR 3个回归的类， SVR和NuSVR差不多，区别也仅仅在于对损失的度量方式不同。LinearSVR是线性回归，只能使用线性核函数</p>
</li>
</ul>
<h4 id="3-2-2-高斯核调参"><a href="#3-2-2-高斯核调参" class="headerlink" title="3.2.2. 高斯核调参"></a>3.2.2. 高斯核调参</h4><h5 id="3-2-2-1-需要调节的参数"><a href="#3-2-2-1-需要调节的参数" class="headerlink" title="3.2.2.1. 需要调节的参数"></a>3.2.2.1. 需要调节的参数</h5><ul>
<li><p><strong>SVM分类模型</strong></p>
<p>  如果是SVM分类模型，这两个超参数分别是<strong>惩罚系数C</strong>和<strong>RBF核函数的系数γ</strong></p>
<p>  <strong>惩罚系数C</strong></p>
<blockquote>
<p>它在优化函数里主要是平衡支持向量的复杂度和误分类率这两者之间的关系，可以理解为正则化系数</p>
<ul>
<li><p>当C比较大时，我们的损失函数也会越大，这意味着我们不愿意放弃比较远的离群点。这样我们会有更加多的支持向量，也就是说支持向量和超平面的模型也会变得越复杂，也容易过拟合</p>
</li>
<li><p>当C比较小时，意味我们不想理那些离群点，会选择较少的样本来做支持向量，最终的支持向量和超平面的模型也会简单</p>
</li>
</ul>
<p>scikit-learn中默认值是1</p>
</blockquote>
<p>  C越大，泛化能力越差，易出现过拟合现象；C越小，泛化能力越好，易出现过欠拟合现象</p>
<p>  <strong>BF核函数的参数γ</strong></p>
<blockquote>
<p>RBF 核函数K(x,z)=exp(−γ||x−z||<sup>2</sup>) γ&gt;0</p>
<p>γ主要定义了单个样本对整个分类超平面的影响</p>
<ul>
<li><p>当γ比较小时，单个样本对整个分类超平面的影响比较小，不容易被选择为支持向量</p>
</li>
<li><p>当γ比较大时，单个样本对整个分类超平面的影响比较大，更容易被选择为支持向量，或者说整个模型的支持向量也会多</p>
</li>
</ul>
<p>scikit-learn中默认值是 <code>1/样本特征数</code></p>
</blockquote>
<p>  γ越大，训练集拟合越好，泛化能力越差，易出现过拟合现象</p>
<p>  如果把惩罚系数C和RBF核函数的系数γ一起看，当C比较大， γ比较大时，我们会有更多的支持向量，我们的模型会比较复杂，容易过拟合一些。如果C比较小 ， γ比较小时，模型会变得简单，支持向量的个数会少</p>
</li>
<li><p><strong>SVM回归模型</strong></p>
<p>  SVM回归模型的RBF核比分类模型要复杂一点，因为此时我们除了惩罚系数C和RBF核函数的系数γ之外，还多了一个<strong>损失距离度量ϵ</strong></p>
<blockquote>
<p>对于损失距离度量ϵ，它决定了样本点到超平面的距离损失</p>
<ul>
<li><p>当 ϵ 比较大时，损失较小，更多的点在损失距离范围之内，而没有损失,模型较简单</p>
</li>
<li><p>当 ϵ 比较小时，损失函数会较大，模型也会变得复杂</p>
</li>
</ul>
<p>scikit-learn中默认值是0.1</p>
</blockquote>
<p>  如果把惩罚系数C，RBF核函数的系数γ和损失距离度量ϵ一起看，当C比较大， γ比较大，ϵ比较小时，我们会有更多的支持向量，我们的模型会比较复杂，容易过拟合一些。如果C比较小 ， γ比较小，ϵ比较大时，模型会变得简单，支持向量的个数会少</p>
</li>
</ul>
<h5 id="3-2-2-2-调参方法：网格搜索"><a href="#3-2-2-2-调参方法：网格搜索" class="headerlink" title="3.2.2.2. 调参方法：网格搜索"></a>3.2.2.2. 调参方法：网格搜索</h5><p>对于SVM的RBF核，我们主要的调参方法都是交叉验证。具体在scikit-learn中，主要是使用网格搜索，即GridSearchCV类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import GridSearchCV</span><br><span class="line">grid = GridSearchCV(SVC(), param_grid=&#123;&quot;C&quot;:[0.1, 1, 10], &quot;gamma&quot;: [1, 0.1, 0.01]&#125;, cv=4)</span><br><span class="line">grid.fit(X, y)</span><br></pre></td></tr></table></figure>
<p>将GridSearchCV类用于SVM RBF调参时要注意的参数有：</p>
<blockquote>
<p>1) <strong>estimator</strong>：即我们的模型，此处我们就是带高斯核的SVC或者SVR</p>
<p>2) <strong>param_grid</strong>：即我们要调参的参数列表。 比如我们用SVC分类模型的话，那么param_grid可以定义为{“C”:[0.1, 1, 10], “gamma”: [0.1, 0.2, 0.3]}，这样我们就会有9种超参数的组合来进行网格搜索，选择一个拟合分数最好的超平面系数</p>
<p>3) <strong>cv</strong>：S折交叉验证的折数，即将训练集分成多少份来进行交叉验证。默认是3。如果样本较多的话，可以适度增大cv的值</p>
</blockquote>
<h3 id="3-3-编程实现"><a href="#3-3-编程实现" class="headerlink" title="3.3. 编程实现"></a>3.3. 编程实现</h3><ol>
<li><p>生成测试数据</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import make_circles</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line"></span><br><span class="line"># 生成一些随机数据用于后续分类</span><br><span class="line">X, y = make_circles(noise=0.2, factor=0.5, random_state=1) # 生成时加入了一些噪声</span><br><span class="line">X = StandardScaler().fit_transform(X) # 把数据归一化</span><br></pre></td></tr></table></figure>
</li>
</ol>
<pre><code>生成的随机数据可视化结果如下：
</code></pre><p><img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-5.png" alt></p>
<ol start="2">
<li><p>调参</p>
<p> 接着采用网格搜索的策略进行RBF核函数参数搜索</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import GridSearchCV</span><br><span class="line">grid = GridSearchCV(SVC(), param_grid=&#123;&quot;C&quot;:[0.1, 1, 10], &quot;gamma&quot;: [1, 0.1, 0.01]&#125;, cv=4) # 总共有9种参数组合的搜索空间</span><br><span class="line">grid.fit(X, y)</span><br><span class="line">print(&quot;The best parameters are %s with a score of %0.2f&quot;</span><br><span class="line">      % (grid.best_params_, grid.best_score_))</span><br><span class="line"></span><br><span class="line">输出为：</span><br><span class="line">The best parameters are &#123;&apos;C&apos;: 10, &apos;gamma&apos;: 0.1&#125; with a score of 0.91</span><br></pre></td></tr></table></figure>
<p> 可以对9种参数组合训练的结果进行可视化，观察分类的效果：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max,0.02),</span><br><span class="line">                     np.arange(y_min, y_max, 0.02))</span><br><span class="line"></span><br><span class="line">for i, C in enumerate((0.1, 1, 10)):</span><br><span class="line">    for j, gamma in enumerate((1, 0.1, 0.01)):</span><br><span class="line">        plt.subplot()       </span><br><span class="line">        clf = SVC(C=C, gamma=gamma)</span><br><span class="line">        clf.fit(X,y)</span><br><span class="line">        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line"></span><br><span class="line">        # Put the result into a color plot</span><br><span class="line">        Z = Z.reshape(xx.shape)</span><br><span class="line">        plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)</span><br><span class="line"></span><br><span class="line">        # Plot also the training points</span><br><span class="line">        plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)</span><br><span class="line"></span><br><span class="line">        plt.xlim(xx.min(), xx.max())</span><br><span class="line">        plt.ylim(yy.min(), yy.max())</span><br><span class="line">        plt.xticks(())</span><br><span class="line">        plt.yticks(())</span><br><span class="line">        plt.xlabel(&quot; gamma=&quot; + str(gamma) + &quot; C=&quot; + str(C))</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure>
</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align:center"><code>C \ gamma</code></th>
<th style="text-align:center">1</th>
<th style="text-align:center">0.1</th>
<th style="text-align:center">0.001</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0.1</td>
<td style="text-align:center"><img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-6.png" alt></td>
<td style="text-align:center"><img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-7.png" alt></td>
<td style="text-align:center"><img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-8.png" alt></td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center"><img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-9.png" alt></td>
<td style="text-align:center"><img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-10.png" alt></td>
<td style="text-align:center"><img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-11.png" alt></td>
</tr>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center"><img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-12.png" alt></td>
<td style="text-align:center"><img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-13.png" alt></td>
<td style="text-align:center"><img src="/images/InAction-MachineLearning-XGS-sklearn-SVM-14.png" alt></td>
</tr>
</tbody>
</table>
<h2 id="4-sklearn包中朴素贝叶斯库的使用"><a href="#4-sklearn包中朴素贝叶斯库的使用" class="headerlink" title="4.sklearn包中朴素贝叶斯库的使用"></a>4.sklearn包中朴素贝叶斯库的使用</h2><h3 id="4-1-朴素贝叶斯相关知识点回顾"><a href="#4-1-朴素贝叶斯相关知识点回顾" class="headerlink" title="4.1. 朴素贝叶斯相关知识点回顾"></a>4.1. 朴素贝叶斯相关知识点回顾</h3><h4 id="4-1-1-什么是朴素贝叶斯分类器"><a href="#4-1-1-什么是朴素贝叶斯分类器" class="headerlink" title="4.1.1. 什么是朴素贝叶斯分类器"></a>4.1.1. 什么是朴素贝叶斯分类器</h4><p>判别式模型（discriminative models)：像决策树、BP神经网络、支持向量机等，都可以归入判别式模型，它们都是直接学习出输出Y与特征X的关系，如：</p>
<ul>
<li>决策函数 Y=f(X)</li>
<li>条件概率 P(Y|X)</li>
</ul>
<p>生成式模型 (gernerative models)：先对联合概率分布 P(X,Y) 进行建模，然后再由此获得P(Y|X) = P(X,Y)/P(X)</p>
<p>贝叶斯学派的思想：</p>
<blockquote>
<p>贝叶斯学派的思想可以概括为先验概率+数据=后验概率。也就是说我们在实际问题中需要得到的后验概率，可以通过先验概率和数据一起综合得到。数据大家好理解，被频率学派攻击的是先验概率，一般来说先验概率就是我们对于数据所在领域的历史经验，但是这个经验常常难以量化或者模型化，于是贝叶斯学派大胆的假设先验分布的模型，比如正态分布，beta分布等。这个假设一般没有特定的依据，因此一直被频率学派认为很荒谬。虽然难以从严密的数学逻辑里推出贝叶斯学派的逻辑，但是在很多实际应用中，贝叶斯理论很好用，比如垃圾邮件分类，文本分类</p>
</blockquote>
<p>如何对P(X,Y)进行建模？</p>
<blockquote>
<p>假如我们的分类模型样本是：</p>
<p>(x<sup>(1)</sup><sub>1</sub>,x<sup>(1)</sup><sub>2</sub>,…x<sup>(1)</sup><sub>n</sub>,y<sub>1</sub>), (x<sup>(2)</sup><sub>1</sub>,x<sup>(2)</sup><sub>2</sub>,…x<sup>(2)</sup><sub>n</sub>,y<sub>2</sub>),…(x<sup>(m)</sup><sub>1</sub>,x<sup>(m)</sup><sub>2</sub>, …, x<sup>(m)</sup><sub>n</sub>,y<sub>m</sub>)</p>
<p>则</p>
<p>P(X, Y) = P(Y) * P( X = (x<sub>1</sub>, x<sub>2</sub>, …, x<sub>n</sub>) | Y ) </p>
<p>其中</p>
<p>P( X = (x<sub>1</sub>, x<sub>2</sub>, …, x<sub>n</sub>) | Y )  =  P( x<sub>1</sub> | Y) * P( x<sub>2</sub> | Y, x<sub>1</sub>) * … P( x<sub>n</sub> | Y, x<sub>1</sub>, … , x<sub>n-1</sub>)</p>
</blockquote>
<p>这是一个超级复杂的有n个维度的条件分布，很难求出</p>
<p>朴素贝叶斯模型在这里做了一个大胆的假设，即X的n个维度之间相互独立，这样就可以得出：</p>
<p>P( X = (x<sub>1</sub>, x<sub>2</sub>, …, x<sub>n</sub>) | Y ) = P( x<sub>1</sub> | Y) * P( x<sub>2</sub> | Y) * … * P( x<sub>n</sub> | Y) </p>
<h4 id="4-1-2-朴素贝叶斯推断"><a href="#4-1-2-朴素贝叶斯推断" class="headerlink" title="4.1.2. 朴素贝叶斯推断"></a>4.1.2. 朴素贝叶斯推断</h4><p><img src="/images/InAction-MachineLearning-XGS-sklearn-naive-bayes-1.png" alt></p>
<h4 id="4-1-3-朴素贝叶斯学习"><a href="#4-1-3-朴素贝叶斯学习" class="headerlink" title="4.1.3. 朴素贝叶斯学习"></a>4.1.3. 朴素贝叶斯学习</h4><p>需要从训练样本中学习到以下两个参数：</p>
<ul>
<li><p><strong>先验概率</strong> P(c)</p>
<p>  P(c)表示了样本空间中各类样本所占的比例</p>
<p>  根据大数定律，当训练集中包含充足的独立同分布样本时，P(c)可根据各类样本出现的频率来估计</p>
<p>  <img src="/images/InAction-MachineLearning-XGS-sklearn-naive-bayes-2.png" alt></p>
</li>
<li><p><strong>类条件概率</strong>（又称为似然） P(x<sub>i</sub> | c)</p>
<p>  （1）如果 x<sub>i</sub> 是离散的，可以假设 x<sub>i</sub>符合多项式分布，这样得到 P(x<sub>i</sub> | c) 是在样本类别 c 中，特征 x<sub>i</sub> 出现的频率</p>
<p>  <img src="/images/InAction-MachineLearning-XGS-sklearn-naive-bayes-3.png" alt></p>
<p>  （2）如果 x<sub>i</sub> 是连续属性，可以假设 P(x<sub>i</sub> | c) ~ N( μ<sub>c,i</sub> , σ<sup>2</sup><sub>c,i</sub> )</p>
<p>  <img src="/images/InAction-MachineLearning-XGS-sklearn-naive-bayes-4.png" alt></p>
</li>
</ul>
<h3 id="4-2-sklearn中朴素贝叶斯类库的简介"><a href="#4-2-sklearn中朴素贝叶斯类库的简介" class="headerlink" title="4.2. sklearn中朴素贝叶斯类库的简介"></a>4.2. sklearn中朴素贝叶斯类库的简介</h3><p>在scikit-learn中，一共有3个朴素贝叶斯的分类算法类</p>
<blockquote>
<ul>
<li><p>GaussianNB：先验为高斯分布的朴素贝叶斯</p>
</li>
<li><p>MultinomialNB：先验为多项式分布的朴素贝叶斯</p>
</li>
<li><p>BernoulliNB：先验为伯努利分布的朴素贝叶斯</p>
</li>
</ul>
</blockquote>
<p>这三个类适用的分类场景各不相同</p>
<blockquote>
<p>一般来说，如果样本特征的分布大部分是<strong>连续值</strong>，使用GaussianNB会比较好</p>
<p>如果如果样本特征的分大部分是<strong>多元离散值</strong>，使用MultinomialNB比较合适</p>
<p>如果样本特征是<strong>二元离散值</strong>或者<strong>很稀疏的多元离散值</strong>，应该使用BernoulliNB。</p>
</blockquote>
<h4 id="4-2-1-GaussianNB类"><a href="#4-2-1-GaussianNB类" class="headerlink" title="4.2.1. GaussianNB类"></a>4.2.1. GaussianNB类</h4><p>GaussianNB类的主要参数仅有一个，即先验概率priors</p>
<p>这个值默认不给出，如果不给出此时P(Y=c)= m<sub>c</sub> / m，如果给出的话就以priors 为准</p>
<p>在使用GaussianNB的fit方法拟合数据后，我们可以进行预测。此时预测有三种方法</p>
<blockquote>
<ul>
<li><p>predict方法：就是我们最常用的预测方法，直接给出测试集的预测类别输出；</p>
</li>
<li><p>predict_proba方法：给出测试集样本在各个类别上预测的概率；</p>
</li>
<li><p>predict_log_proba方法：和predict_proba类似，它会给出测试集样本在各个类别上预测的概率的一个对数转化；</p>
</li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.naive_bayes import GaussianNB</span><br><span class="line"></span><br><span class="line">clf = GaussianNB()</span><br><span class="line">#拟合数据</span><br><span class="line">clf.fit(X, Y)</span><br><span class="line"></span><br><span class="line">#进行预测</span><br><span class="line">clf.predict([[-0.8, -1]])</span><br></pre></td></tr></table></figure>
<h4 id="4-2-2-MultinomialNB类"><a href="#4-2-2-MultinomialNB类" class="headerlink" title="4.2.2. MultinomialNB类"></a>4.2.2. MultinomialNB类</h4><p>MultinomialNB假设特征的先验概率为多项式分布，即如下式：</p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-naive-bayes-5.png" alt></p>
<p>MultinomialNB参数比GaussianNB多，但是一共也只有仅仅3个</p>
<ul>
<li><p><strong>参数alpha</strong>：为上面的常数λ。如果你没有特别的需要，用默认的1即可。如果发现拟合的不好，需要调优时，可以选择稍大于1或者稍小于1的数</p>
</li>
<li><p><strong>参数fit_prior</strong>：是否要考虑先验概率，如果是false，则所有的样本类别输出都有相同的类别先验概率；否则可以自己用第三个参数class_prior输入先验概率，或者不输入第三个参数class_prior让MultinomialNB自己从训练集样本来计算先验概率</p>
</li>
<li><p><strong>参数class_prior</strong>：输入先验概率，若不输入第三个参数class_prior让MultinomialNB自己从训练集样本来计算先验概率</p>
</li>
</ul>
<h4 id="4-2-3-BernoulliNB类"><a href="#4-2-3-BernoulliNB类" class="headerlink" title="4.2.3. BernoulliNB类"></a>4.2.3. BernoulliNB类</h4><p><img src="/images/InAction-MachineLearning-XGS-sklearn-naive-bayes-6.png" alt></p>
<p>其中，x <sub>i</sub> 只能取0或1</p>
<p>BernoulliNB一共有4个参数，其中3个参数的名字和意义和MultinomialNB完全相同</p>
<p>唯一增加的一个参数是binarize，这个参数主要是用来帮BernoulliNB处理二项分布的。如果不输入，则BernoulliNB认为每个数据特征都已经是二元的。否则的话，小于binarize的会归为一类，大于binarize的会归为另外一类</p>
<h2 id="5-手写数字识别：sklearn包中神经网络库的使用"><a href="#5-手写数字识别：sklearn包中神经网络库的使用" class="headerlink" title="5. 手写数字识别：sklearn包中神经网络库的使用"></a>5. 手写数字识别：sklearn包中神经网络库的使用</h2><h3 id="5-1-神经网络相关知识点回顾"><a href="#5-1-神经网络相关知识点回顾" class="headerlink" title="5.1. 神经网络相关知识点回顾"></a>5.1. 神经网络相关知识点回顾</h3><h4 id="5-1-1-感知机：神经网络的最小单元"><a href="#5-1-1-感知机：神经网络的最小单元" class="headerlink" title="5.1.1. 感知机：神经网络的最小单元"></a>5.1.1. 感知机：神经网络的最小单元</h4><p>神经网络中最基本的成分是神经元（neuron）模型，最常见的模型是M-P神经元模型，也可以称为感知机（Perceptron）</p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-1.png" alt></p>
<p>若对生物学有一点基础的了解，就可以知道上面构造的简化的M-P神经元模型几乎完全吻合生物学上的定义：</p>
<blockquote>
<p>当前神经元通过与上游神经元之间的突触结构，接受来自上游的神经元1到n传递过来的信号x<sub>1</sub>,x<sub>2</sub>…x<sub>n</sub>，由于不同的上游信号传递过来它们的影响程度是不一样的，因此对它们进行加权求和，即∑w<sub>i</sub>x<sub>i</sub>，则得到总的上游信号</p>
<p>另外当前神经元还有一个激活阈值θ，若上游信号的汇总信号强度大于θ，即则∑w<sub>i</sub>x<sub>i</sub>-θ &gt; 0，则当前神经元被激活，处于兴奋状态；若若上游信号的汇总信号强度小于θ，即则∑w<sub>i</sub>x<sub>i</sub>-θ &lt; 0，则当前神经元未被激活，处于抑制状态</p>
</blockquote>
<p>对于生物学上的神经元来说，它只有两个状态：</p>
<ul>
<li>“1”：对应神经元兴奋；</li>
<li>“0”：对应神经元抑制；</li>
</ul>
<p>则它接受的信号为上游神经元的输出为 x<sub>i</sub>∈{0,1}，它的输出信号为 y∈{0,1}</p>
<p>则它的理想激活函数是阶跃函数，即<code>f(x)=sgn(x)</code></p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-2.png" alt></p>
<p>然而，阶跃函数具有不连续、不光滑等不太好的性质，因此实际上常用sigmoid函数作为激活函数</p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-3.png" alt></p>
<p>则此时激活函数为f(x)=sigmoid(∑w<sub>i</sub>x<sub>i</sub>-θ），很明显若选择sigmod函数作为感知机的激活函数，则此时感知机等价于一个logistic回归分类器</p>
<p>类比logistic回归分类器</p>
<blockquote>
<ul>
<li><p>对于线性可分的分类任务，感知机完全等价于logistic回归分类器</p>
</li>
<li><p>对于线性不可分问题，logistic回归可以构造额外的高阶多项式：</p>
<p>  <img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-4.png" alt></p>
<p>  <img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-5.png" alt></p>
<p>  本质上是将原始特征空间中线性不可分的样本向高维特征空间进行映射，使得它们在新的高维特征空间中线性可分</p>
<p>  而感知器无法进行高维空间的映射，只能基于原始特征空间寻找线性判别边界，因此感知机在面对简单的非线性可分问题是，往往无法得到理想的判别面</p>
</li>
</ul>
</blockquote>
<p>感知机的学习规则：</p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-6.png" alt></p>
<h4 id="5-1-2-神经网络的学习：BackPropagation算法"><a href="#5-1-2-神经网络的学习：BackPropagation算法" class="headerlink" title="5.1.2. 神经网络的学习：BackPropagation算法"></a>5.1.2. 神经网络的学习：BackPropagation算法</h4><p>神经网络的学习算法本质上是数学上常用的梯度下降（Gradient Descend）算法</p>
<p>依据在一轮神经网络的训练过程中所用到的训练样本的数量的不同，可分为以下两类：</p>
<blockquote>
<ul>
<li><p>标准BP算法：每次网络训练只针对单个训练样本，一次训练就输入一个训练样本，更新一次网络参数；</p>
</li>
<li><p>累积BP算法：每次网络训练只针对所有训练样本，一次将所有训练样本输入，更新一次网络参数；</p>
</li>
</ul>
</blockquote>
<h4 id="5-1-2-1-标准BP算法"><a href="#5-1-2-1-标准BP算法" class="headerlink" title="5.1.2.1. 标准BP算法"></a>5.1.2.1. 标准BP算法</h4><p><img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-7.jpg" alt></p>
<p>以w<sub>h,j</sub>为例进行推导</p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-8.png" alt></p>
<p>在标准BP算法中，由于只对输入的一个训练样本进行网络参数的训练，因此它的目标函数为当前样本k的均方误差E<sub>k</sub></p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-9.png" alt></p>
<p>此时关键在于怎么推出△w<sub>hj</sub>的表达式？</p>
<p>根据链式求导法则，W<sub>hj</sub>的影响链条为：</p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-10.png" alt></p>
<p>因此△w<sub>hj</sub>可以写成：</p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-11.png" alt></p>
<p>中间推导过程省略，最后得到的△w<sub>hj</sub>的表达式为：</p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-12.png" alt></p>
<h4 id="5-1-2-2-累积BP算法"><a href="#5-1-2-2-累积BP算法" class="headerlink" title="5.1.2.2. 累积BP算法"></a>5.1.2.2. 累积BP算法</h4><p>累积BP算法的目标是最小化训练集的累积误差：</p>
<p><img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-13.png" alt></p>
<p>这是累积BP算法与标准BP算法的最大的也是最本质的差别</p>
<p>标准BP算法存在的问题：</p>
<blockquote>
<p>每次更新只针对单个训练样本，参数更新得非常频繁，而且不同样本训练的效果可能出现相互“抵消”的现象</p>
<p>因此为了达到与累积误差相同的极小点，标准BP算法往往需要进行更多次的迭代</p>
</blockquote>
<p>累积BP算法的优缺点：</p>
<blockquote>
<ul>
<li><p>优点：直接对累积误差最小化，它在读取整个训练样本集D后才对参数进行一次更新，其参数更新的频率低得多</p>
</li>
<li><p>缺点：累积误差下降到一点程度之后，进一步下降会非常缓慢</p>
<p>  此时标准BP算法往往会更快得到较好的解，因为其解的震荡性给它带来了一个好处——容易跳出局部最优</p>
</li>
</ul>
</blockquote>
<h3 id="5-2-sklearn中神经网络库的简介"><a href="#5-2-sklearn中神经网络库的简介" class="headerlink" title="5.2. sklearn中神经网络库的简介"></a>5.2. sklearn中神经网络库的简介</h3><p>sklearn中神经网络的实现不适用于大规模数据应用。特别是 scikit-learn 不支持 GPU</p>
<h4 id="5-2-1-MLPClassifier"><a href="#5-2-1-MLPClassifier" class="headerlink" title="5.2.1. MLPClassifier"></a>5.2.1. MLPClassifier</h4><p>神经网络又称为多层感知机（Multi-layer Perceptron，MLP）</p>
<p>sklearn中用MLP进行分类的类为MLPClassifier</p>
<p>主要参数说明：</p>
<table>
<thead>
<tr>
<th style="text-align:left">参数</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">hidden_​​layer_sizes</td>
<td style="text-align:left">tuple，length = n_layers - 2，默认值（100，）第i个元素表示第i个隐藏层中的神经元数量。</td>
</tr>
<tr>
<td style="text-align:left">activation</td>
<td style="text-align:left">{‘identity’，‘logistic’，‘tanh’，‘relu’}，默认’relu’ 隐藏层的激活函数：‘identity’，无操作激活，对实现线性瓶颈很有用，返回f（x）= x；‘logistic’，logistic sigmoid函数，返回f（x）= 1 /（1 + exp（-x））；‘tanh’，双曲tan函数，返回f（x）= tanh（x）；‘relu’，整流后的线性单位函数，返回f（x）= max（0，x）</td>
</tr>
<tr>
<td style="text-align:left">slover</td>
<td style="text-align:left">{‘lbfgs’，‘sgd’，‘adam’}，默认’adam’。权重优化的求解器：’lbfgs’是准牛顿方法族的优化器；’sgd’指的是随机梯度下降。’adam’是指由Kingma，Diederik和Jimmy Ba提出的基于随机梯度的优化器。注意：默认解算器“adam”在相对较大的数据集（包含数千个训练样本或更多）方面在训练时间和验证分数方面都能很好地工作。但是，对于小型数据集，“lbfgs”可以更快地收敛并且表现更好。</td>
</tr>
<tr>
<td style="text-align:left">alpha</td>
<td style="text-align:left">float，可选，默认为0.0001。L2惩罚（正则化项）参数。</td>
</tr>
<tr>
<td style="text-align:left">batch_size</td>
<td style="text-align:left">int，optional，默认’auto’。用于随机优化器的minibatch的大小。如果slover是’lbfgs’，则分类器将不使用minibatch。设置为“auto”时，batch_size = min（200，n_samples）</td>
</tr>
<tr>
<td style="text-align:left">learning_rate</td>
<td style="text-align:left">{‘常数’，‘invscaling’，‘自适应’}，默认’常数”。 用于权重更新。仅在solver =’sgd’时使用。’constant’是’learning_rate_init’给出的恒定学习率；’invscaling’使用’power_t’的逆缩放指数在每个时间步’t’逐渐降低学习速率learning_rate_， effective_learning_rate = learning_rate_init / pow（t，power_t）；只要训练损失不断减少，“adaptive”将学习速率保持为“learning_rate_init”。每当两个连续的时期未能将训练损失减少至少tol，或者如果’early_stopping’开启则未能将验证分数增加至少tol，则将当前学习速率除以5。</td>
</tr>
</tbody>
</table>
<p>MLPClassifier的训练使用BP算法，其使用<strong>交叉熵损失函数</strong>（Cross-Entropy loss function）</p>
<p>MLP的训练需要准备</p>
<blockquote>
<ul>
<li><p>X<sub>m*n</sub>：m个训练样本的n维特征向量构成的特征矩阵</p>
</li>
<li><p>Y<sub>m</sub>：m训练样本的目标值组成的向量</p>
</li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.neural_network import MLPClassifier</span><br><span class="line"></span><br><span class="line">X = [[0., 0.], [1., 1.]]</span><br><span class="line">y = [0, 1]</span><br><span class="line"></span><br><span class="line"># 模型训练</span><br><span class="line">clf = MLPClassifier(solver=&apos;lbfgs&apos;, alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line"></span><br><span class="line"># 模型预测</span><br><span class="line">clf.predict([[2., 2.], [-1., -2.]])</span><br></pre></td></tr></table></figure>
<p>以上训练好的MLP模型保存在clf对象中，该对象有以下两个子变量：</p>
<blockquote>
<ul>
<li><p><code>clf.coefs_</code>：模型的一系列权重矩阵</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; 	# 用以下命令查看每一层的权重矩阵的维度</span><br><span class="line">&gt; 	&gt;&gt;[coef.shape for coef in clf.coefs_]</span><br><span class="line">&gt; 	[(2, 5), (5, 2), (2, 1)]</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</blockquote>
<blockquote>
<pre><code>其中下标为 i 的权重矩阵表示第 i 层和第 i+1 层之间的权重
</code></pre><ul>
<li><p><code>intercepts_</code>：一系列偏置向量</p>
<p>  其中的下标为 i 的向量表示添加到第 i+1 层的偏置值</p>
</li>
</ul>
</blockquote>
<ul>
<li><p><strong>多分类任务</strong></p>
<p>  MLPClassifier 通过应用 Softmax 作为输出函数来支持多分类</p>
<p>  softmax回归本质上是将原先常见的二元分类任务神经网络的输出层采用的sigmoid激活函数换成softmax回归</p>
<p>  例如要利用神经网络进行k类的分类，则神经网络结构如下：</p>
<p>  <img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-14.png" alt></p>
<p>  那么最后一层即网络的输出层所采用的激活函数——softmax回归到底长什么样呢？</p>
<p>  <img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-15.png" alt></p>
<p>  因此该网络的输出层又被称为softmax layer</p>
<p>  因此在提供训练集时，若总共有k的类别，当某一个训练样本的类别为第i类时，它的目标值应该为<code>[0, 0, ..., 1, ..., 0]</code>，只在向量的第i个位置标为1，其他位置都为0</p>
</li>
<li><p><strong>多标签分类任务</strong></p>
<p>  一个样本可能属于多个类别。 对于每个类，原始输出经过 logistic 函数变换后，大于或等于 0.5 的值将进为 1，否则为 0。 对于样本的预测输出，值为 1 的索引位置表示该样本的分类类别</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; X = [[0., 0.], [1., 1.]]</span><br><span class="line">&gt;&gt;&gt; y = [[0, 1], [1, 1]]</span><br><span class="line">&gt;&gt;&gt; clf = MLPClassifier(solver=&apos;lbfgs&apos;, alpha=1e-5,</span><br><span class="line">...                     hidden_layer_sizes=(15,), random_state=1)</span><br><span class="line">...</span><br><span class="line">&gt;&gt;&gt; clf.fit(X, y) </span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; clf.predict([[1., 2.]])</span><br><span class="line">array([[1, 1]])</span><br><span class="line">&gt;&gt;&gt; clf.predict([[0., 0.]])</span><br><span class="line">array([[0, 1]])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="5-3-编程实现"><a href="#5-3-编程实现" class="headerlink" title="5.3. 编程实现"></a>5.3. 编程实现</h3><p>对于神经网络，要说它的Hello world，莫过于识别手写数字了</p>
<p>首先要获取实验数据，下载地址：<a href="http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz" target="_blank" rel="noopener">http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz</a></p>
<p>数组中的每一行都表示一个灰度图像。每个元素都表示像素所对应的灰度值。</p>
<p>数据集中将数据分成3类：训练集，验证集，测试集。一般情况下会将训练数据分为训练集和测试集</p>
<p>为什么要将训练数据分为训练集和测试集？</p>
<blockquote>
<p>将原始数据分开，保证用于测试的数据是训练模型时从未遇到的数据可以使测试更客观。否则就像学习教课书的知识，又只考教课书的知识，就算不理解记下了就能得高分但遇到新问题就傻眼了。</p>
<p>好一点的做法就是用训练集当课本给他上课，先找出把课本知识掌握好的人，再参加由新题组成的月考即测试集，若是还是得分高，那就是真懂不是死记硬背了。</p>
<p>但这样选出来的模型实际是还是用训练集和测试集共同得到的，再进一步，用训练集和验证集反复训练和检测，得到最好的模型，再用测试集来一局定输赢即期末考试，这样选出来的就更好了。</p>
</blockquote>
<p>本实验中为了方便将训练集与验证集合并</p>
<ol>
<li><p>先载入数据，并简单查看一下数据</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.neural_network import MLPClassifier</span><br><span class="line">import numpy as np</span><br><span class="line">import pickle</span><br><span class="line">import gzip</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"># 加载数据</span><br><span class="line">with gzip.open(r&quot;mnist.pkl.gz&quot;) as fp:</span><br><span class="line">    training_data, valid_data, test_data = pickle.load(fp)</span><br><span class="line"></span><br><span class="line">X_training_data, y_training_data = training_data</span><br><span class="line">X_valid_data, y_valid_data = valid_data</span><br><span class="line">X_test_data, y_test_data = test_data</span><br><span class="line"></span><br><span class="line"># 合并训练集,验证集</span><br><span class="line">X_training = np.vstack((X_training_data, X_valid_data))</span><br><span class="line">   y_training = np.append(y_training_data, y_valid_data)</span><br><span class="line"></span><br><span class="line">def show_data_struct():</span><br><span class="line">    print X_training_data.shape, y_training_data.shape</span><br><span class="line">    print X_valid_data.shape, y_valid_data.shape</span><br><span class="line">    print X_test_data.shape, y_test_data.shape</span><br><span class="line">    print X_training_data[0]</span><br><span class="line">    print y_training_data[0]</span><br><span class="line"></span><br><span class="line">show_data_struct()</span><br></pre></td></tr></table></figure>
</li>
<li><p>随便看几张训练图片</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def show_image():</span><br><span class="line">    plt.figure(1)</span><br><span class="line">    for i in range(10):</span><br><span class="line">        image = X_training[i]	# 得到包含第i张图的像素向量，为1*768</span><br><span class="line">        pixels = image.reshape((28, 28)) # 将原始像素向量转换为28*28的像素矩阵</span><br><span class="line">        plt.subplot(5,2,i+1)</span><br><span class="line">        plt.imshow(pixels, cmap=&apos;gray&apos;)</span><br><span class="line">        plt.title(y_training[i])</span><br><span class="line">        plt.axis(&apos;off&apos;)</span><br><span class="line">    plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.45,</span><br><span class="line">                        wspace=0.85)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">show_image()</span><br></pre></td></tr></table></figure>
<p> <img src="/images/InAction-MachineLearning-XGS-sklearn-neural-network-16.png" alt></p>
</li>
<li><p>训练模型</p>
<p> 为了获得比默认参数更佳的模型，我们采用网格搜索法搜索更优的训练超参数，使用gridSearchCV实现，上文<a href="#sklearn-svm-parameters-setting-for-guassian-kernal-how-to-set-parameters">3.2.2.2. 调参方法：网格搜索</a>中有较为详细的说明</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import GridSearchCV</span><br><span class="line"></span><br><span class="line">mlp = MLPClassifier()</span><br><span class="line">mlp_clf__tuned_parameters = &#123;&quot;hidden_layer_sizes&quot;: [(100,), (100, 30)],</span><br><span class="line">                                 &quot;solver&quot;: [&apos;adam&apos;, &apos;sgd&apos;, &apos;lbfgs&apos;],</span><br><span class="line">                                 &quot;max_iter&quot;: [20],</span><br><span class="line">                                 &quot;verbose&quot;: [True]</span><br><span class="line">                                 &#125;</span><br><span class="line">estimator = GridSearchCV(mlp, mlp_clf__tuned_parameters, n_jobs=6)</span><br><span class="line">estimator.fit(X_training, y_training)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<hr>
<p>参考资料：</p>
<p>(1) <a href="https://github.com/MLjian/TextClassificationImplement" target="_blank" rel="noopener">【GitHub】MLjian/TextClassificationImplement</a></p>
<p>(2) <a href="https://www.cnblogs.com/pinard/p/6035872.html" target="_blank" rel="noopener">刘建平Pinard《scikit-learn 逻辑回归类库使用小结》</a></p>
<p>(3) <a href="https://blog.csdn.net/loveliuzz/article/details/78708359" target="_blank" rel="noopener">loveliuzz《机器学习sklearn19.0——Logistic回归算法》</a></p>
<p>(4) <a href="https://www.cnblogs.com/pinard/p/6056319.html" target="_blank" rel="noopener">刘建平Pinard《scikit-learn决策树算法类库使用小结》</a></p>
<p>(5) <a href="https://blog.csdn.net/loveliuzz/article/details/78739438" target="_blank" rel="noopener">loveliuzz《机器学习sklearn19.0——决策树算法》</a></p>
<p>(6) <a href="https://www.cnblogs.com/pinard/p/6117515.html" target="_blank" rel="noopener">刘建平Pinard《scikit-learn 支持向量机算法库使用小结》</a></p>
<p>(7) <a href="https://www.cnblogs.com/pinard/p/6126077.html" target="_blank" rel="noopener">刘建平Pinard《支持向量机高斯核调参小结》</a></p>
<p>(8) <a href="https://blog.csdn.net/loveliuzz/article/details/78768063" target="_blank" rel="noopener">loveliuzz《机器学习sklearn19.0——SVM算法》</a></p>
<p>(9) 周志华《机器学习：第7章 贝叶斯分类器》</p>
<p>(10) <a href="https://www.cnblogs.com/pinard/p/6069267.html" target="_blank" rel="noopener">刘建平Pinard《朴素贝叶斯算法原理小结》</a></p>
<p>(11) <a href="https://www.cnblogs.com/pinard/p/6074222.html" target="_blank" rel="noopener">刘建平Pinard《scikit-learn 朴素贝叶斯类库使用小结》</a></p>
<p>(12) 周志华《机器学习：第5章 神经网络》</p>
<p>(13) 吴恩达《deeplearning.ai：改善深层神经网络》</p>
<p>(14) <a href="http://sklearn.apachecn.org/#/docs/18" target="_blank" rel="noopener">scikit-learn中文网《神经网络模型（有监督）》</a></p>
<p>(15) <a href="https://blog.csdn.net/weixin_38278334/article/details/83023958" target="_blank" rel="noopener">啊噗不是阿婆主《sklearn 神经网络MLPclassifier参数详解》</a></p>
<p>(16) <a href="https://www.jianshu.com/p/d4fd9c52a915" target="_blank" rel="noopener">多问Why.简书《SkLearn之MLP》</a></p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/sklearn/" rel="tag"># sklearn</a>
          
            <a href="/tags/logistic回归/" rel="tag"># logistic回归</a>
          
            <a href="/tags/决策树/" rel="tag"># 决策树</a>
          
            <a href="/tags/朴素贝叶斯方法/" rel="tag"># 朴素贝叶斯方法</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/02/09/Helpdoc-for-different-languages/" rel="next" title="在Perl、Shell和Python中传参与输出帮助文档">
                <i class="fa fa-chevron-left"></i> 在Perl、Shell和Python中传参与输出帮助文档
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/02/09/Load-and-resolve-MarkdownDoc/" rel="prev" title="前端小技巧：加载并解析Markdown文档">
                前端小技巧：加载并解析Markdown文档 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Lianm">
            
              <p class="site-author-name" itemprop="name">Lianm</p>
              <p class="site-description motion-element" itemprop="description">中国科学院北京基因组研究所研究生</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">15</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">5</span>
                    <span class="site-state-item-name">分类</span>
                  
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">24</span>
                    <span class="site-state-item-name">标签</span>
                  
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/Ming-Lian" title="GitHub &rarr; https://github.com/Ming-Lian" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:lianming17m@big.ac.cn" title="E-Mail &rarr; mailto:lianming17m@big.ac.cn" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-“达观杯”文本智能处理挑战赛：sklearn包中logistic算法的使用"><span class="nav-number">1.</span> <span class="nav-text">1. “达观杯”文本智能处理挑战赛：sklearn包中logistic算法的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-任务描述"><span class="nav-number">1.1.</span> <span class="nav-text">1.1. 任务描述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-编程实现"><span class="nav-number">1.2.</span> <span class="nav-text">1.2. 编程实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-sklearn包中logistic算法的使用"><span class="nav-number">1.3.</span> <span class="nav-text">1.3. sklearn包中logistic算法的使用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-鸢尾花数据分类：sklearn包中决策树算法类库的使用"><span class="nav-number">2.</span> <span class="nav-text">2. 鸢尾花数据分类：sklearn包中决策树算法类库的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-DecisionTreeClassifier实例"><span class="nav-number">2.1.</span> <span class="nav-text">2.1. DecisionTreeClassifier实例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-可视化决策树"><span class="nav-number">2.2.</span> <span class="nav-text">2.2. 可视化决策树</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-sklearn中测试数据：sklearn包中SVM算法库的使用"><span class="nav-number">3.</span> <span class="nav-text">3. sklearn中测试数据：sklearn包中SVM算法库的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-SVM相关知识点回顾"><span class="nav-number">3.1.</span> <span class="nav-text">3.1. SVM相关知识点回顾</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-1-SVM与SVR"><span class="nav-number">3.1.1.</span> <span class="nav-text">3.1.1. SVM与SVR</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-2-核函数"><span class="nav-number">3.1.2.</span> <span class="nav-text">3.1.2. 核函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-sklearn中SVM相关库的简介"><span class="nav-number">3.2.</span> <span class="nav-text">3.2. sklearn中SVM相关库的简介</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-1-分类库与回归库"><span class="nav-number">3.2.1.</span> <span class="nav-text">3.2.1. 分类库与回归库</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-2-高斯核调参"><span class="nav-number">3.2.2.</span> <span class="nav-text">3.2.2. 高斯核调参</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-2-1-需要调节的参数"><span class="nav-number">3.2.2.1.</span> <span class="nav-text">3.2.2.1. 需要调节的参数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-2-2-调参方法：网格搜索"><span class="nav-number">3.2.2.2.</span> <span class="nav-text">3.2.2.2. 调参方法：网格搜索</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-编程实现"><span class="nav-number">3.3.</span> <span class="nav-text">3.3. 编程实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-sklearn包中朴素贝叶斯库的使用"><span class="nav-number">4.</span> <span class="nav-text">4.sklearn包中朴素贝叶斯库的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-朴素贝叶斯相关知识点回顾"><span class="nav-number">4.1.</span> <span class="nav-text">4.1. 朴素贝叶斯相关知识点回顾</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-1-什么是朴素贝叶斯分类器"><span class="nav-number">4.1.1.</span> <span class="nav-text">4.1.1. 什么是朴素贝叶斯分类器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-2-朴素贝叶斯推断"><span class="nav-number">4.1.2.</span> <span class="nav-text">4.1.2. 朴素贝叶斯推断</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-3-朴素贝叶斯学习"><span class="nav-number">4.1.3.</span> <span class="nav-text">4.1.3. 朴素贝叶斯学习</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-sklearn中朴素贝叶斯类库的简介"><span class="nav-number">4.2.</span> <span class="nav-text">4.2. sklearn中朴素贝叶斯类库的简介</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-1-GaussianNB类"><span class="nav-number">4.2.1.</span> <span class="nav-text">4.2.1. GaussianNB类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-2-MultinomialNB类"><span class="nav-number">4.2.2.</span> <span class="nav-text">4.2.2. MultinomialNB类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-3-BernoulliNB类"><span class="nav-number">4.2.3.</span> <span class="nav-text">4.2.3. BernoulliNB类</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-手写数字识别：sklearn包中神经网络库的使用"><span class="nav-number">5.</span> <span class="nav-text">5. 手写数字识别：sklearn包中神经网络库的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-神经网络相关知识点回顾"><span class="nav-number">5.1.</span> <span class="nav-text">5.1. 神经网络相关知识点回顾</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-1-感知机：神经网络的最小单元"><span class="nav-number">5.1.1.</span> <span class="nav-text">5.1.1. 感知机：神经网络的最小单元</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-2-神经网络的学习：BackPropagation算法"><span class="nav-number">5.1.2.</span> <span class="nav-text">5.1.2. 神经网络的学习：BackPropagation算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-2-1-标准BP算法"><span class="nav-number">5.1.3.</span> <span class="nav-text">5.1.2.1. 标准BP算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-2-2-累积BP算法"><span class="nav-number">5.1.4.</span> <span class="nav-text">5.1.2.2. 累积BP算法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-sklearn中神经网络库的简介"><span class="nav-number">5.2.</span> <span class="nav-text">5.2. sklearn中神经网络库的简介</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-1-MLPClassifier"><span class="nav-number">5.2.1.</span> <span class="nav-text">5.2.1. MLPClassifier</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-编程实现"><span class="nav-number">5.3.</span> <span class="nav-text">5.3. 编程实现</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lianm</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.0.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=7.0.0"></script>

  <script src="/js/src/motion.js?v=7.0.0"></script>



  
  


  <script src="/js/src/schemes/muse.js?v=7.0.0"></script>




  
  <script src="/js/src/scrollspy.js?v=7.0.0"></script>
<script src="/js/src/post-details.js?v=7.0.0"></script>



  


  <script src="/js/src/bootstrap.js?v=7.0.0"></script>



  


  


  




  

  

  

  

  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js'; 
      }
      else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>


  

  

  

  

  

  

  

  

</body>
</html>
